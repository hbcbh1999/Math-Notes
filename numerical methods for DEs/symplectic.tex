\documentclass[a4paper, 11pt]{article}   	
\usepackage{geometry}       
\geometry{a4paper}
\geometry{margin=1in}	
\usepackage{paralist}
  \let\itemize\compactitem
  \let\enditemize\endcompactitem
  \let\enumerate\compactenum
  \let\endenumerate\endcompactenum
  \let\description\compactdesc
  \let\enddescription\endcompactdesc
  \pltopsep=\medskipamount
  \plitemsep=1pt
  \plparsep=5pt
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{bbm}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{booktabs}
\usepackage{empheq}
\pagestyle{headings}
\newcommand{\boxwidth}{430pt}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Numerical Methods for Differential Equations, 2017 Spring.}
\rhead{}

\title{\textbf{Lecture 5}}
\author{Zed}

\begin{document}
\maketitle
\section{Newton's Method for Solving Nonlinear Equations}
We want to find roots for nonlinear equation $g(x)=0$. 
\begin{itemize}
	\item[\textit{Algo.}] \textbf{Newton's Method}
	\begin{itemize}
		\item[$\cdot$] Initialize a starting point $x_0$. 
		\item[$\cdot$] Update $x$ by $x_{n+1} \leftarrow x_n - \frac{g(x_n)}{g'(x_n)}$, suppose $g'(x_n)\ne 0$.
		\item[$\cdot$] Stop when $\left\|g(x_n)\right\| \leq thres$ (small), $\left\|x_{n+1} - x_n\right\| \leq thres$ (small) or $n\geq K$ (fail to converge). 
	\end{itemize}
	The Newton's method only converges in a local sense. Suppose the real zero is $\alpha$, s.t. $g(\alpha)=0$, then using taylor expansion at $x_n$:
	\begin{equation}
		 \begin{split}
		 	0 = g(\alpha) &= g(x_n) + g'(x_n)(\alpha - x_n) + \frac{1}{2}g''(\xi_n)(\alpha - x_n)^2 \\
		 	\Rightarrow 0 &= \left(\frac{g(x_n)}{g'(x_n)} - x_n\right) + \alpha + \frac{g''(\xi_n)}{2g'(x_n)}(\alpha - x_n)^2 \\
		 \end{split}
	\end{equation}
	Hence exist bound $M$ such that 
	$$
	|\alpha - x_{n+1}| = \frac{g''(\xi_n)}{2g'(x_n)}|\alpha - x_n|^2 \leq M|\alpha - x_n|^2
	$$
	That is, if the error at $n$-th iteration $|\alpha - x_n|$ is already small, the next error at ($n+1$)-th iteration will be the square of it. Which implies a (locally) quadratic convergence rate.
\end{itemize}



\section{Implicit Methods}
\subsection{Implicit Runge-Kutta Method}
We consider the Runge-Kutta method with table:
\begin{center}
	\begin{tabular}{c|ccccc}
	$c_1$ & $a_{11}$ & $a_{12}$ & $\hdots$ & $a_{1,s-1}$ & $a_{1s}$\\
	$c_2$ & $a_{21}$ & $a_{22}$ & $\hdots$ & $a_{2,s-1}$ & $a_{2s}$\\
	$c_3$ & $a_{31}$ & $a_{32}$ & $\hdots$ & $a_{3,s-1}$ & $a_{3s}$\\
	$\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ & $\vdots$ \\
	$c_s$ & $a_{s1}$ & $a_{s2} $ & $\hdots$ & $a_{s,s-1}$ & $a_{ss}$\\
	\hline
	 & $b_1$ & $b_2$ & $\hdots$ & $b_{s-1}$ & $b_s$
	\end{tabular} = 
	\begin{tabular}{c|ccc}
	1/2 & 1/2\\
	\hline
	 & 1 
	\end{tabular}
\end{center}
A table of this kind with a full matrix of entries $\bm{A}$ (instead of entries only in the lowertriangular area) suggests that this is an \emph{implicit} RK method. 

~\\
For the listed example, we have
\begin{equation}
	\begin{cases}
	y_{n+1} = y_n + hb_1 k_1 \\
	k_1 = f(t_n + c_1 h, y_n + ha_{11}k_1)
	\end{cases}
\end{equation}
And insert the values of coefficients:
$$
y_{n+1} = y_n + h\left(f(t_n+\tfrac{1}{2}h, \tfrac{y_{n+1}}{2}+\tfrac{y_n}{2})\right)
$$
Which is usually referred to as the \emph{Implicit Midpoint Method}. This method is the simplest implicit RK method, the simplest \emph{Gauss-Legendre} method. And a \emph{Symplectic method}, which is energy-preserving.

~\\
The Midpoint method is of order 2 (see the first question in HW1).
\begin{itemize}
	\item[\textit{Ex.}] (\emph{Energy Preserving}) Consider the Hamilton system: $p'=-q, q'=p$; with $p(0)=q(0)=1$. We can find a Hamilton function $H$ such that $p'=-\frac{\partial H}{\partial q}$, $q'=\frac{\partial H}{\partial p}$. And hence $\frac{\partial H}{\partial t}=0$. However, when we use the explicit RK method to solve the system (for example, the \texttt{ode45} in \texttt{Matlab}), we will find that the method does not preserve energy, i.e. $\frac{\partial H}{\partial t}\ne 0$.
\end{itemize}



\section{Symplectic Methods}
Our motivation to derive such a family of method is that it is natural to look into these discrete systems which preserves as much as possible the intrinsic properties of the continuous system. (Feng Kang, 1985; Ruth, 1983). We use the term \emph{Symplectic} to refer to such systems, objects and methods.

\begin{itemize}
	\item[\textit{Ex.}] We consider a simple system: $p'=-q, q'=p$, i.e. $H(p,q)=\frac{1}{2}p^2+\frac{1}{2}q^2 = const$. And there is a numerical method to solve this ode system. At $n$-th step we are at $(p_n, q_n)$, and the numerical method will map this value to $(p_{n+1}, q_{n+1})$ in the next step. 

	~\\
	We consider all the $(p_n, q_n)$'s within an infinitesimal area $dV_n$, and in next step it is mapped to $dV_{n+1}$. Furthermore, for all initial values $(p_0, q_0)$ in the infinitesimal area $dV_0$, after $n$ steps of numerial iteration, it will go to $dV_n$. We want a kind of \emph{stability} such that initial values that are close to each other will yield numerical solutions that are also close to each other. In the setting of this problem we want the area of $dV_n$ $\approx$ $dV_0$. Intuitively, this suggests the same thing in the sense of stability. We investigate some methods.
	\begin{itemize}
		\item[1.] \emph{Explicit Euler}
		\begin{equation}
			\begin{cases}
			p_m = p_{m-1} - hq_{m-1} \\
			q_m = q_{m-1} + hp_{m-1}
			\end{cases}
		\end{equation}
		The Jacobian 
		$$
		\bm{J}_{ex} = \frac{\partial (p_m, q_m)}{\partial (p_{m-1}, q_{m-1})} = \begin{vmatrix}
			\frac{\partial p_m}{\partial p_{m-1}} & \frac{\partial p_m}{\partial q_{m-1}} \\
			\frac{\partial q_m}{\partial p_{m-1}} & \frac{\partial q_m}{\partial q_{m-1}}\\
		\end{vmatrix} = 
		\begin{vmatrix}
			1 & -h \\
			h & 1\\
		\end{vmatrix} = 1+h^2
		$$
		So we will see $dV_m = (1+h^2)^{m} dV_0$. Any two initial values will evolve to be infinitely farwary.

		\item[2.] \emph{Implicit Euler}
		\begin{equation}
			\begin{cases}
			p_m = p_{m-1} - hq_{m} \\
			q_m = q_{m-1} + hp_{m}
			\end{cases} \Rightarrow
			\begin{cases}
			(1+h^2)p_m = p_{m-1} - hq_{m-1} \\
			(1+h^2)q_m = q_{m-1} + hp_{m-1}
			\end{cases}
		\end{equation}
		The Jacobian 
		$$
		\bm{J}_{im} = \frac{\partial (p_m, q_m)}{\partial (p_{m-1}, q_{m-1})} = 
		\begin{vmatrix}
			\frac{1}{1+h^2} & \frac{-h}{1+h^2} \\
			\frac{h}{1+h^2} & \frac{1}{1+h^2}\\
		\end{vmatrix} = \frac{1}{1+h^2}
		$$
		Therefore we will see $dV_m = (1+h^2)^{-m}dV_0$, the solution will eventually degenerate to one point.

		\item[3.] \emph{Symplectic Euler}
		\begin{equation}
			\begin{cases}
			p_m = p_{m-1} - hq_{m-1} \\
			q_m = q_{m-1} + hp_{m}
			\end{cases} \Rightarrow
			\begin{cases}
			p_m = p_{m-1} - hq_{m-1} \\
			q_m = q_{m-1} + h(p_{m-1} - hq_{m-1})
			\end{cases}
		\end{equation}
		$$
		\bm{J}_{symp} = \frac{\partial (p_m, q_m)}{\partial (p_{m-1}, q_{m-1})} = 
		\begin{vmatrix}
			1 & -h \\
			h & 1-h^2\\
		\end{vmatrix} = 1
		$$

		\item[4.] \emph{Implicit Midpoint}
		\begin{equation}
			\begin{cases}
			p_m = p_{m-1} - \tfrac{h}{2}q_{m-1} - \tfrac{h}{2}q_{m} \\
			q_m = q_{m-1} + \tfrac{h}{2}p_{m-1} + \tfrac{h}{2}p_{m}
			\end{cases} \Rightarrow
			\begin{cases}
			(1+\tfrac{h^2}{4})p_m = (1-\tfrac{h^2}{4})p_{m-1} - hq_{m-1} \\
			(1+\tfrac{h^2}{4})q_m = (1-\tfrac{h^2}{4})q_{m-1} + hp_{m-1}
			\end{cases}
		\end{equation}
		$$
		\bm{J}_{mid} = \frac{\partial (p_m, q_m)}{\partial (p_{m-1}, q_{m-1})} = 
		\begin{vmatrix}
			\frac{4-h^2}{4+h^2} & \frac{-4h}{4+h^2} \\
			\frac{4h}{4+h^2} & \frac{4-h^2}{4+h^2}\\
		\end{vmatrix} =  \frac{(4-h^2)^2 + 16h^2}{(4+h^2)^2} = 1
		$$
	\end{itemize}

	\item[\textit{Def.}] \textbf{Hamiltonian System} We investigate Hamiltonian system in higher space dimensions: generalized coordinate $\bm{q}=(q_1, ..., q_d)^{\top}$ and generalized momentum: $\bm{p}=(p_1, ..., p_d)^{\top}$,
	\begin{equation}
		\begin{cases}
			\dot{\bm{p}} = - \frac{\partial H}{\partial \bm{q}} \\
			\dot{\bm{q}} = \frac{\partial H}{\partial \bm{p}}
		\end{cases}
	\end{equation}
	It is easy to check that the \emph{Hamiltonian Energy} $H = const$ over time.
	$$
	\frac{d}{dt} H(\bm{p}, \bm{q}) = \left(\frac{\partial H}{\partial \bm{p}}\right)^{\top} \dot{\bm{p}} +  \left(\frac{\partial H}{\partial \bm{q}}\right)^{\top} \dot{\bm{q}} = 0
	$$
	We can write in another form, let $\bm{y}=(\bm{p}, \bm{q})^{\top}=(p_1, ..., p_d; q_1, ..., q_d)^{\top}$, $\bm{J}:=\begin{pmatrix}
		\bm{O} & \bm{I}_d \\
		-\bm{I}_d & \bm{O}
	\end{pmatrix}$, one can easily check that $\bm{J}^{-1} = -\bm{J}$. Then we can write the system as $\dot{\bm{y}} = \bm{J}^{-1} \nabla \bm{H}(\bm{y})~~(*)$.\\

	\textit{Proof.~~} $\bm{J}^{-1} = -\bm{J}$ because:
	$$\bm{JJ} = \begin{pmatrix}
		\bm{O} & \bm{I}_d \\
		-\bm{I}_d & \bm{O}
	\end{pmatrix}\begin{pmatrix}
		\bm{O} & \bm{I}_d \\
		-\bm{I}_d & \bm{O}
	\end{pmatrix} = \begin{pmatrix}
		-\bm{I}_d & \bm{O} \\
		\bm{O} & -\bm{I}_d
	\end{pmatrix} = -\bm{I}_{2d}$$
	And 
	$$
	RHS(*) = \begin{pmatrix}
		\bm{O} & -\bm{I}_d \\
		\bm{I}_d & \bm{O}
	\end{pmatrix} \begin{pmatrix}
		\nabla_{\bm{p}}H \\
		\nabla_{\bm{q}}H
	\end{pmatrix} = \begin{pmatrix}
		-\nabla_{\bm{q}}H\\
		\nabla_{\bm{p}}H
	\end{pmatrix} = \begin{pmatrix}
		-\frac{\partial H}{\partial \bm{q}}\\
		\frac{\partial H}{\partial \bm{p}}
	\end{pmatrix} = (\dot{\bm{p}}, \dot{\bm{q}})^{\top} = LHS
	$$
	\item[\textit{Def.}] \textbf{Flow Map}: Given an autonomous differential equation $\bm{y}'=\bm{f}(\bm{y})$, the \emph{flow map} $\bm{\varphi}_t(\cdot): \mathbb{R}^d \to \mathbb{R}^d$, such that $\bm{\varphi}_t(\bm{y}_0) = \bm{y}(t)$ maps the initial value to the solution at $t$. The definition can be extended to the flow map of \emph{measurable sets} $\Omega \subseteq \mathbb{R}^d$: 
	\begin{equation}
		\begin{split}
			&\bm{\varphi}_t: \mathcal{B}(\mathbb{R}^d) \to \mathcal{B}(\mathbb{R}^d) \\
			&\bm{\varphi}_t(\Omega) = \{\bm{y}(t): \bm{y}_0 \in \Omega\}
		\end{split}
	\end{equation}
	where $\mathcal{B}(\mathbb{R}^d)$ is the Borel sigma-algebra on $\mathbb{R}^d$ (the collection of all measurable sets).
	\item[\textit{Def.}] \textbf{Symplecticity}: For the Hamiltonian system $(\bm{p}, \bm{q}) \in \mathbb{R}^d \times \mathbb{R}^d$:
	\begin{itemize}
		\item[1.] If $p,q$ has only one dimension, i.e. $d=1$, the system is \emph{symplectic} if the area is preserved in phase space $\mathbb{R} \times \mathbb{R}$ under the flow map $\bm{\varphi}_t$.
		\item[2.] $d\geq 2$, $(\bm{p}, \bm{q}) = (p_1, ..., p_d; q_1, ..., q_d)$ the system is symplectic if the quantity
		$$
		w^2 = \sum_{i=1}^d dp_i \wedge dq_i
		$$
		is preserved under the flow map $\bm{\varphi}_t$. Where $\wedge$ is the wedge product.
	\end{itemize}

	\item[\textit{Prop.}] The equivalent definition of symplecticity: a dynamic system is symplectic $\iff$
	$$
	\bm{J} = \bm{\Phi}_t^{\top} \bm{J} \bm{\Phi}_t
	$$
	where $\bm{J}=\begin{pmatrix}
		\bm{O} & \bm{I}_d \\
		-\bm{I}_d & \bm{O}
	\end{pmatrix}$, $\bm{\Phi}_t = \bm{\Phi}_t(\bm{y}) = \partial \bm{\varphi}_t(\bm{y})/\partial \bm{y}$ is the Jacobian of the flow map. \\
	\textit{Proof.$^*$~~}
	\begin{equation}
		\begin{split}
			w^2 &= \sum_{i=1}^d dp_i \wedge dq_i = \frac{1}{2}\left(\sum_{i=1}^d dp_i \wedge dq_i-\sum_{i=1}^d dq_i \wedge dp_i\right) \\
			&= \frac{1}{2}\bm{J}^{-1} d \bm{\varphi}_t \wedge d \bm{\varphi}_t = \frac{1}{2}\bm{J}^{-1} \bm{\Phi}_t d \bm{y} \wedge \bm{\Phi}_t d \bm{y} \\
			&= \frac{1}{2}\bm{\Phi}_t^{\top} \bm{J}^{-1} \bm{\Phi}_t d \bm{y} \wedge d \bm{y}
		\end{split}
	\end{equation}
	And we require $w^2(t)=w^2(0)$, knowing $\bm{\Phi}_t(0) = \bm{I}$ $\Rightarrow$ $\frac{1}{2}\bm{\Phi}_t^{\top} \bm{J}^{-1} \bm{\Phi}_t d \bm{y} \wedge d \bm{y} = \frac{1}{2} \bm{J}^{-1}  d \bm{y} \wedge d \bm{y}$. So we have $\bm{\Phi}_t^{\top} \bm{J} \bm{\Phi}_t = \bm{J}$. $\square$

	\item[\textit{Thm.}] (\textbf{Poincar\'e}): Hamiltonian system $\iff$ Symplectic system. \\
	\textit{Proof.~~} We only show ($\Rightarrow$), i.e. showing $\bm{\Phi}_t^{\top} \bm{J} \bm{\Phi}_t = \bm{J}$ from Hamiltonian. \\
	We use $\dot{\bm{y}} = \bm{J}^{-1} \nabla H(\bm{y})$, $\bm{\varphi}_t(\bm{y})$ is flow map. Since $\bm{\varphi}_t(\bm{y}) = \bm{y}(t)$, it must satisfy the ode when we \emph{regard $t$ as its varable} (in the time dimension). That is
	$$
	\frac{d \bm{\varphi}_t(\bm{y})}{dt} = \bm{J}^{-1} \nabla H(\bm{\varphi}_t(\bm{y}))
	$$
	$$
	\frac{d \bm{\Phi}_t(\bm{y})}{dt} = \frac{\partial}{\partial \bm{y}}\frac{d \bm{\varphi}_t(\bm{y})}{dt} =  \bm{J}^{-1} \nabla^2 H(\bm{\varphi}_t(\bm{y})) \frac{\partial \bm{\varphi}_t(\bm{y})}{\partial \bm{y}} = \bm{J}^{-1} \nabla^2 H(\bm{\varphi}_t(\bm{y})) \bm{\Phi}_t(\bm{y})
	$$
	where $\nabla^2H$ is the hessian. Therefore
	\begin{equation}
		\begin{split}
			\frac{d}{dt}\left(\bm{\Phi}_t^{\top}\bm{J} \bm{\Phi}_t\right) &= \left(\frac{d \bm{\Phi}_t}{dt}\right)^{\top} \bm{J} \bm{\Phi}_t + \bm{\Phi}_t^{\top}\bm{J} \frac{d \bm{\Phi}_t}{dt} \\
			&= (\bm{J}^{-1} \nabla^2 H(\bm{\varphi}_t(\bm{y}) \bm{\Phi}_t)^{\top}\bm{J}\bm{\Phi}_t + \bm{\Phi}_t^{\top}\bm{J}\bm{J}^{-1} \nabla^2 H(\bm{\varphi}_t(\bm{y})) \bm{\Phi}_t \\
			&= \bm{\Phi}_t^{\top}\nabla^2 H(\bm{\varphi}_t(\bm{y}))\bm{J}^{-\top} \bm{J}\bm{\Phi}_t + \bm{\Phi}_t^{\top}\bm{J}\bm{J}^{-1} \nabla^2 H(\bm{\varphi}_t(\bm{y})) \bm{\Phi}_t \\
			& = -\bm{\Phi}_t^{\top}\nabla^2 H(\bm{\varphi}_t(\bm{y})) \bm{\Phi}_t + \bm{\Phi}_t^{\top}\nabla^2 H(\bm{\varphi}_t(\bm{y})) \bm{\Phi}_t \\
			&= 0
		\end{split}
	\end{equation}
	Using the fact that $\bm{J}^{-\top}=(-\bm{J})^{\top} = \begin{pmatrix}
		\bm{O} & -\bm{I}_d \\
		\bm{I}_d & \bm{O}
	\end{pmatrix}^{\top} = \bm{J}$, and $\bm{JJ}=- \bm{I}$.
	Therefore $\bm{\Phi}_t^{\top} \bm{J}\bm{\Phi}_t = \bm{\Phi}_t^{\top} \bm{J}\bm{\Phi}_t\mid_{t=0}$. Note that $\bm{\Phi}_t\mid_{t=0} = \det(\bm{I})$. Hence we have $\bm{\Phi}_t^{\top} \bm{J}\bm{\Phi}_t = \bm{J}$. $\square$
\end{itemize}
\end{document}
