\documentclass[a4paper, 11pt]{article}   	
\usepackage{geometry}       
\geometry{a4paper}
\geometry{margin=1in}	
\usepackage{paralist}
  \let\itemize\compactitem
  \let\enditemize\endcompactitem
  \let\enumerate\compactenum
  \let\endenumerate\endcompactenum
  \let\description\compactdesc
  \let\enddescription\endcompactdesc
  \pltopsep=\medskipamount
  \plitemsep=1pt
  \plparsep=5pt
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{bbm}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{booktabs}
\usepackage{empheq}
\pagestyle{headings}
\newcommand{\boxwidth}{430pt}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Numerical Methods for Differential Equations, 2017 Spring.}
\rhead{}

\title{\textbf{Lecture 2}}
\author{Zed}{}

\begin{document}
\maketitle

\section{Lagrange Interpolation}
\emph{Motivation}: for the problem
$$
y(t_{k+1}) = y(t_k) + \int_{t_k}^{t_{k+1}} f(t, y(t))dt
$$
we want to construct a method of \emph{any} order $p$. When using the theta method with whatever linear combination of only two points (one-step method), we can only achieve order 2. Therefore it is necessary to use more than two points.

\begin{itemize}
	\item[\textit{Def.}] \textbf{Lagrange Interpolation}: We want to appoximate $g(t)$ which we know $n+1$ values at $t_0<t_1<t_2<...<t_n$. Define
	$$
	p(t) := \sum_{k=0}^n g(t_k)L_k(t),~~\text{where}~~L_k(t):=\prod_{j=0,j\ne k}^n\frac{t_j - t}{t_j - t_k}
	$$
	$L_k$ is called the Lagrange interp polynomial. $p(t)$ is called the Lagrange interp \emph{of order n}, using $\{L_k\}_0^n$ and the $n+1$ values of $g$.

	\item[\textit{Thm.}] If $g\in C^{\infty}$, $p(t)$ is Lagrange interp of order $n$ then
	$$
	g(t) - p(t) = \frac{1}{(n+1)!} g^{(n+1)}(\xi) \prod_{k=0}^n (t-t_k),~~t_0\leq\xi \leq t_n
	$$
	Further, if assuming $t_i-t_{i-1}=h$, then $\prod_{k=0}^n \leq (n+1)! h^{n+1}$, hence
	$$
	g(t) - p(t) \leq \max\limits_{t_0\leq \xi \leq t_n}|g^{(n+1)}(\xi)|\cdot h^{n+1} \sim O(h^{n+1})
	$$
\end{itemize}

\section{Multistep Methods}
Consider the same IVP, $y(t)$ is its solution. 
Suppose we know the value of $y(t)$ at $s$ points: $t_n, t_{n+1}, ..., t_{n+s-1}$ (so we also know the value of $y'(t)$ at these points), and we want to approximate $y(t_{n+s})$. We begin with the integral formula from $t_{n+s-1}$ to $t_{n+s}$:
\begin{equation}
	\begin{split}
		y(t_{n+s}) &= y(t_{n+s-1}) + \int_{t_{n+s-1}}^{t_{n+s}} y'(t) dt \\
		&= y(t_{n+s-1}) + \int_{t_{n+s-1}}^{t_{n+s}} p(t)dt + O(h^{s+1}) \\
		&= y(t_{n+s-1}) + \int_{t_{n+s-1}}^{t_{n+s}} \sum_{k=n}^{n+s-1}y'(t_k)L_k(t)dt + O(h^{s+1}) \\
		&= y(t_{n+s-1}) + \sum_{k=n}^{n+s-1}y'(t_k)\int_{t_{n+s-1}}^{t_{n+s}} L_k(t)dt + O(h^{s+1})~~~(\dag)\\
	\end{split}
\end{equation}

We hope that $\int_{t_{n+s-1}}^{t_{n+s}} L_k(t)dt$ is a constant times $h$. Actually we have
$$
\int_{t_{n+s-1}}^{t_{n+s}} L_k(t)dt = \int_{t_{s-1}}^{t_{s}} L_{k'}(t)dt
$$
where $k=n,n+1,...,n+s-1; k'=0,1,...,s-1$. We find this by translating the lattice $\{t_{k}\}$ to the left by $nh$. So we just use $k$ in the following text with $k=0,1,...,s-1$.
And we denote
$$
h\left(\frac{1}{h}\int_{t_{s-1}}^{t_{s}} L_{k}(t)dt\right) = hc_k
$$
where $c_k$ is indeed a constant. Therefore
$$
(\dag): y(t_{n+s}) = y(t_{n+s-1}) + h\sum_{k=0}^{s-1} c_kf(t_{k+n}, y(t_{k+n})) + O(h^{s+1})
$$
\begin{itemize}
	\item[\textit{Def.}] \textbf{Multistep Method}: we define the numerical method using $(\dag)$, by substituting $y(t_k)$ with discrete approx $y_k$, i.e.
	\begin{equation}
		y_{n+s} = y_{n+s-1} + h\sum_{k=0}^{s-1} c_k \cdot f(t_{k+n}, y_{k+n})
	\end{equation}
	\begin{equation}
		c_k ( = c_k^{[s]}) = \frac{1}{h} \int_{t_{s-1}}^{t_s} L_k^{[s]}(t)dt = \frac{1}{h} \int_{t_{s-1}}^{t_s} \prod_{j=0,j\ne k}^{s-1}\frac{t_j - t}{t_j - t_k} dt
	\end{equation}
	The iterative formula (2) is called the multistep method with order $s$. By definition, $(\dag)$ gives the truncation error of this method, which is $R_k \sim O(h^s)$ when there are $s$ known values of $y(t)$, and the second term is an $(s-1)^{th}$ ordered Lagrange interpolation. \\
	Note that parameter $\{c_k\}$ is a function of $k$ and $s$ (the order of the method). For the methods of different order, $c_k$'s have different values. We say $\{c_k^{[s]}\}_{k=0}^{s-1}$ in the following text to avoid ambiguity.

	\item[\textit{Ex.}] $1^{st}$ order multistep:
	$$
	c_0^{[1]} = \int_0^1 L_0^{[1]}(t)dt = \int_0^1 1dt = 1
	$$
	$$
	y_{n+1} = y_{n} + h f(t_n, y_n)
	$$
	Degenerates to the Euler method.

	\item[\textit{Ex.}] $2^{nd}$ order multistep:
	$$
	c_0^{[2]} = \int_1^2 L_0^{[2]}(t)dt = \int_1^2 \frac{1-t}{1-0}dt = -\frac{1}{2}
	$$
	$$
	c_1^{[2]} = \int_1^2 L_1^{[2]}(t)dt = \int_1^2 \frac{0-t}{0-1}dt = \frac{3}{2}
	$$
	$$
	y_{n+2} = y_{n+1} + h \left[-\frac{1}{2}f(t_n, y_n)+\frac{3}{2}f(t_{n+1}, y_{n+1})\right]
	$$
	
	\item[\textit{Ex.}] $3^{rd}$ order multistep:
	$$
	c_0^{[3]} = \int_2^3 L_0^{[3]}(t)dt = \int_2^3 \frac{1-t}{1-0}\cdot \frac{2-t}{2-0}dt = \frac{5}{12}
	$$
	$$
	c_1^{[3]} = \int_2^3 L_1^{[3]}(t)dt = \int_2^3 \frac{0-t}{0-1}\cdot \frac{2-t}{2-1}dt = -\frac{3}{4}
	$$
	$$
	c_2^{[3]} = \int_2^3 L_2^{[3]}(t)dt = \int_2^3 \frac{0-t}{0-2}\cdot \frac{1-t}{1-2}dt = \frac{23}{12}
	$$
	$$
	y_{n+3} = y_{n+2} + h \left[\frac{5}{12}f(t_n, y_n)-\frac{4}{3}f(t_{n+1}, y_{n+1})+\frac{23}{12}f(t_{n+2}, y_{n+2})\right]
	$$
	The complexity of computing parameters $\{c_k^{[s]}\}$ increases with the order, but we can hardcode the values in the program anyway. The running time of the methods will not involve the computation of $\{c_k^{[s]}\}$.
\end{itemize}

\section{General Formulation of Multistep Methods}
We propose a general formulation instead of turning to Lagrange interpolation:
\begin{equation}
	\sum_{k=0}^s a_k y_{n+k} = h\sum_{k=0}^s b_k f(t_{n+k}, y_{n+k})
\end{equation}
where $\{a_k\}_{k=0}^s$, $\{b_k\}_{k=0}^s$ are unsolved constants (parameters), independent wrt $h, n$ or the ODE. Let $a_s=1$, we can obtain an explicit method iff $b_s=0$:
$$
y_{n+s} = -\sum_{k=0}^{s-1} a_k y_{n+k} + h\sum_{k=0}^{s-1} b_k f(t_{n+k}, y_{n+k})
$$
Define
\begin{equation}
	\psi(n,y):= \sum_{k=0}^s a_k y(t_{n+k}) - h\sum_{k=0}^s b_k f(t_{n+k}, y(t_{n+k}))
\end{equation}
By defintion we want $\psi(n,y)\sim O(h^{p+1})$, then the method is of order $p$, $1\leq p\leq s$. With Taylor expansion of $y(t_{n+k})$ and $y'(t_{n+k})$ at $t_n$, we have
\begin{equation}
	\begin{split}
		\psi(n,y) &= \sum_{k=0}^s a_k \sum_{m=0}^{\infty}\left(y^{(m)}(t_{n})\cdot\frac{(kh)^m}{m!}\right) - h\sum_{k=0}^s b_k \sum_{m=0}^{\infty}\left(y^{(m+1)}(t_{n})\cdot\frac{(kh)^m}{m!}\right) \\
		&= \sum_{m=0}^{\infty}\frac{h^m y^{(m)}(t_n)}{m!} \sum_{k=0}^s a_k k^{m} - \sum_{m=1}^{\infty}\frac{h^m y^{(m)}(t_n)}{(m-1)!} \sum_{k=0}^s b_k k^{m-1}\\
		&= y(t_n)\sum_{k=0}^{s}a_k + \sum_{m=1}^{\infty}\frac{h^m y^{(m)}(t_n)}{(m-1)!}\sum_{k=0}^s (a_k k^m - mb_k k^{m-1})\\
	\end{split}
\end{equation}
It is clear that the method is of order $p$ if the $m=0, 1, ..., p$ order of $h^m$ shrink, i.e. The method (4) is of order $p$ $\iff$
\begin{equation}
	\begin{split}
		(a)~~~& \sum_{k=0}^s a_k =0\\
		(b)~~~& \sum_{k=0}^s (a_k k^m - mb_k k^{m-1}) =0~~\text{for } m=1,2,...,p\\
		(c)~~~& \sum_{k=0}^s (a_k k^m - mb_k k^{m-1}) \ne 0~~\text{for } m=p+1~\text{(or higher)}\\
	\end{split}
\end{equation}
Now we consider these parameters, define
$$
c_0 := \sum_{k=0}^s a_k,~~~c_m := \frac{1}{m!}\sum_{k=0}^s (a_k k^m - mb_k k^{m-1})
$$
The \textbf{generating polynomial} of $\{c_m\}$ is:
$$
P(z) := \sum_{m=0}^{\infty} c_m z^m
$$
\begin{equation}
	\begin{split}
		\sum_{m=0}^{\infty} c_m z^m &=  \sum_{k=0}^s a_k z^0 + \sum_{m=1}^{\infty}\frac{z^m}{m!}\sum_{k=0}^s (a_k k^m - mb_k k^{m-1})  \\
		&= \sum_{k=0}^s a_k \left(1+\sum_{m=1}^{\infty}\frac{(kz)^m}{m!}\right) + \sum_{k=0}^s b_k z\sum_{m=1}^{\infty}\frac{(kz)^{m-1}}{(m-1)!}\\
		&= \sum_{k=0}^s a_k \sum_{m=0}^{\infty}\frac{(kz)^m}{m!}+ \sum_{k=0}^s b_k z\sum_{m=0}^{\infty}\frac{(kz)^{m}}{m!} \\
		&= \sum_{k=0}^s a_k e^{kz}+ \sum_{k=0}^s b_k z e^{kz}
	\end{split}
\end{equation}
Which gives us
$$
P(z) = \sum_{m=0}^{\infty} c_m z^m = \sum_{k=0}^s a_k e^{kz}+ \sum_{k=0}^s b_k z e^{kz}
$$
Since $z$ in $P(z)$ has exactly the role of $h$ in $\psi(n,y)$. So the method is of order $p$ $\iff \psi(n,y) = O(h^{p+1})$ $\iff$ $P(z) = cz^{p+1} + h.o.t.$ (higher order terms) $(*)$.

~\\
Now let $\omega=e^z$. We have $\log \omega = \log(\omega -1+1) \sim \omega -1 $ as $\omega\to1$ as $z\to 0$. So $(*)$ $\iff$:
$$
P(z) = \sum_{k=0}^s a_k \omega^k - \log\omega \sum_{k=0}^s b_k \omega^k = c (\log \omega)^{p+1} + h.o.t. = c(\omega - 1)^{p+1} + h.o.t.
$$
By using the generating polynomial, we obtain a shortcut to see the order of the (truncation error) of the multistep methods.

\begin{itemize}
	\item[\textit{Def.}] \textbf{Characteristic Polynomial}: Define 
	$$
	P(\omega) := \sum_{k=0}^s a_k \omega^k - \log\omega \sum_{k=0}^s b_k \omega^k
	$$
	And further define $\rho(\omega):=\sum_{k=0}^s a_k \omega^k$ as the \emph{first characteristic polynomial}, $\sigma(\omega):=\sum_{k=0}^s b_k \omega^k$ as the \emph{second characteristic polynomial}. Then
	$$
	P(\omega) = \rho(\omega) - \sigma(\omega)\log \omega 
	$$
	\item[\textit{Thm.}] The multistep method is of order $p\geq 1$ $\iff$ $\exists c\ne 0$, such that as $\omega \to 1$,
	$$
	\rho(\omega) - \sigma(\omega) \log \omega = O\left((\omega-1)^{p-1}\right)
	$$

	\item[\textit{Ex.}] Check the $2^{nd}$ order multistep method:
	$$
	y_{n+2} = y_{n+1} + h \left[-\frac{1}{2}f(t_n, y_n)+\frac{3}{2}f(t_{n+1}, y_{n+1})\right]
	$$
	We have
	$$
	\rho(\omega) = \omega^2 - \omega;~~~\sigma(\omega) = \frac{3}{2}\omega - \frac{1}{2}
	$$
	So let $v:=\omega-1$
	\begin{equation}
		\begin{split}
			\rho(\omega) - \sigma(\omega) \log \omega &= \omega^2 - \omega - \log \omega \left(\frac{3}{2}\omega - \frac{1}{2}\right) \\
			&= v(v+1) - \left(v-\frac{v^2}{2}+\frac{v^3}{3}+O(v^4)\right) \left(\frac{3}{2}v + \frac{1}{2}\right) \\
			&= v^2 + v -\frac{3}{2}v^2 - v + \frac{3}{4}v^3 + \frac{1}{2}v^2-\frac{1}{3}v^3 + O(v^4) \\
			& = O(v^3)
		\end{split}
	\end{equation}
	So by theorem, it is of order 2 indeed.

	\item[\textit{Ex.}] Check the multistep method:
	$$
	y_{n+2} - 3y_{n+1} + 2y_n = h \left[\frac{13}{12}f(t_{n+2}, y_{n+2})-\frac{5}{3}f(t_{n+1}, y_{n+1})-\frac{5}{12}f(t_n, y_n)\right]
	$$
	We have
	$$
	\rho(\omega) = \omega^2 - 3\omega + 2;~~~\sigma(\omega) = \frac{13}{12}\omega^2 - \frac{5}{3}\omega - \frac{5}{12}
	$$
	By letting $v:=\omega-1$,
	$$
	\rho(\omega) - \sigma(\omega) \log \omega = O(v^3)
	$$
	So this method is also of order 2.
\end{itemize}

\section{Convergence of Multistep Methods}
\begin{itemize}
	\item[\textit{Def.}] \textbf{Consistency}: If a multistep method is at least of order 1, we call it \emph{consistent}. I.e. $\rho(\omega) - \sigma(\omega) \log \omega = O(v^p)$, $p\geq 2$.

	\item[\textit{Cor.}] A method is consistant $\iff$ $c_0=0, c_1=0$, (recall that $c_0 := \sum_{k=0}^s a_k$, $c_m := \frac{1}{m!}\sum_{k=0}^s (a_k k^m - mb_k k^{m-1})$) $\iff$
	\begin{equation}
		\begin{split}
			(a)~~~&\sum_{k=0}^s a_k = 0\\
			(b)~~~&\sum_{k=0}^s (ka_k - b_k) = 0
		\end{split}
	\end{equation}
	$\iff$ $\rho(1)=0$ and $\rho'(1)=\sigma(1)$. \\

	\item[\textit{Ex.}] Consistency, by itself, does not guarantee convergence (a global characteristic). Consider using method
	$$
	y_{n+2} - 3y_{n+1} + 2y_n = h \left[\frac{13}{12}f(t_{n+2}, y_{n+2})-\frac{5}{3}f(t_{n+1}, y_{n+1})-\frac{5}{12}f(t_n, y_n)\right]
	$$
	to solve a naive IV problem: $y'=0, y(0)=1$. From our discussion above we know this method is of order 2, hence consistent. And the true solution of this ODE is just $y\equiv1$. 

	~\\
	The method degenerates to recursion $y_{n+2} - 3y_{n+1} + 2y_n=0$. If we start with $y_0=1$, then we get the correct solution. However, if we add a small disturbance to the initial data, i.e. $y_0=1+\epsilon$, $\epsilon$ is small, and may be caused by round-off error in computing (which is ubiquitous). We will have, by solving the linear recursion:
	$$
	y_n = C_1 + C_2 \cdot 2^n = 1-\epsilon + 2^n \epsilon
	$$
	which eventually blows up. And clearly this method does not converge for all ODEs.

	\item[\textit{Def.}] \textbf{Stability}: A linear multistep ($s$-steps) method is said to be \emph{zero-stable}, if there exists constant $K$, s.t. for any two sequences $\{y_n\}$ and $\{\hat{y}_n\}$, generated with same method, but with different initial data: $\{y_0, y_1, ..., y_{s-1}\}$ and $\{\hat{y}_0, \hat{y}_1, ..., \hat{y}_{s-1}\}$, we have
	$$
	|y_n - \hat{y}_n| \leq K\cdot \max\limits_{0\leq j \leq s-1} \left(|y_j - \hat{y}_j|\right)
	$$
	for $t_n \leq T$ and $h\searrow 0$. Ususally $K$ depends on $T$ and not depends on $h$.

	\item[\textit{Thm.}] A linear multistep method is zero-stable for any ODE $y'=f(y,t)$ where $f$ satisfies Lipschitz condition $\iff$ The first characteristic polynomial of this method $\rho(z)=\sum_{k=0}^s a_k z^k$ has all its zeros (roots) inside the closed unit disc (including the boundary). Moreover, if the zero lie \emph{on} the unit circle, it must be a simple root (has multiplicity 1). 

	\textit{Proof.~~} $(\Rightarrow)$ We consider the naive IVP $y'=0$. The method degenerates to linear recursion
	$$
	\sum_{k=0}^s a_k y_{n+k} = 0
	$$
	which has general solution $y_n =\sum_{r} P_r(n)z_r^n$, where $z_r$ is root of $\rho(z)=0$, $P_r(n)$ is a polynomial whose degree equials $m(z_r)+1$, $m(z_r)$ is the multiplicity of $z_r$. It is clear that this problem has solution $y\equiv const$. Hence $y_n$ cannot be unbounded.

	~\\
	Under the other 2 scenarios (1) $|z_r|>1$ or (2) $|z_r|=1, P_r(n)$ is not constant; clearly $y_n$ unbounded. So the only possibilty is otherwise: $|z_r|<1$ or $|z_r|=1, P_r(n)\equiv c$ (has degree 0). $\blacksquare$ \\
	$(\Leftarrow)$ is long, not so relevant to the numerical computing aspect. We just skip that.

	\item[\textit{Thm.}] (\textbf{Convergence}) A multistep method is convergent $\iff$ It is consistent and zero-stable. Moreover, if the method is of order $p$, the global error is also of order $p$.
\end{itemize}
\end{document}
