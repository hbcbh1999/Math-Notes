\documentclass[a4paper,12pt,twoside]{book}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{geometry}       
\geometry{a4paper}
\geometry{margin=1.0in}

\usepackage{amsmath}
\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\arcsech}{arcsech}

\usepackage{bbm}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}

\usepackage{hyperref}
\hypersetup{colorlinks=true,allcolors=black}
\usepackage{hypcap}

\pagestyle{headings}
\newcommand{\boxwidth}{430pt}
 

\title{Advanced Probability Theory: Notes}
\author{YANG, Ze}

\begin{document}

\maketitle
\tableofcontents

%------------------------------------------------------------------------
%////////////////////////////////////////////////////////////////////////
%////////////////////////////////////////////////////////////////////////
%------------------------------------------------------------------------
\chapter{Measure Space, Prob Space}

%////////////////////////////////////////////////////////////////////////
\section{Algebraic Structures on Prob Space}

%------------------------------------------------------------------------
\subsection{Sigma Field}
Default setting: Let $S$ be a set.
\begin{itemize}
	\item[\textit{Def.}] \textbf{Algebra}: A family of $A \subseteq S$, $\Sigma_0$ is an algebra if
	\begin{itemize}
		\item[$\cdot$] $S \in \Sigma_0$.
		\item[$\cdot$] $A^c \in \Sigma_0$. (close to \textbf{Complement})
		\item[$\cdot$] $n$ finite, $\bigcup_{i=1}^n A_i \in \Sigma_0$. (close to \textbf{Finite Union})
	\end{itemize}

	\item[\textit{Rm.}] 1,2,3 implies
	\begin{itemize}
		\item[$\cdot$] $\emptyset \in \Sigma_0$.
		\item[$\cdot$] $A \cap B, A\cup B, (A\setminus B), (A\triangle B) \in \Sigma_0$.
		\item[$\cdot$] $\bigcap_{i=1}^n A_i \in \Sigma_0$.
	\end{itemize}

	\item[\textit{Def.}] \textbf{Sigma-Field}: A family of $A \subseteq S$, $\Sigma$ is a sigma-field if 1,2 (algebra) and
	\begin{itemize}
		\item[$\cdot$] $\bigcup_{j=1}^{\infty} A_j \in \Sigma$. (close to \textbf{Countable Union})
	\end{itemize}

	\item[\textit{Def.}] \textbf{Generated Sigma-Field}: $C \subseteq S$, $\sigma(C)$ is generated sigma-field from $C$ if
	\begin{itemize}
		\item[$\cdot$] $\sigma(C)$ is a sigma field.
		\item[$\cdot$] $C \subseteq \sigma(C)$.
		\item[$\cdot$] If $C \subseteq \Sigma'\ne \sigma(C)$, $\Sigma'$ is another sigma field, then $\sigma(C) \subseteq \Sigma'$.
	\end{itemize}
	i.e. $\sigma(C)$ is the smallest sigma field that is a supset of $C$. Also written as,
	\begin{equation}
		\sigma(C)=\bigcap_{\Sigma \text{: sigma field}}\{\Sigma: C \subseteq \Sigma\}
	\end{equation}

	\item[\textit{Prop.}] Several Propositions.
	\begin{itemize}
		\item[$\cdot$] Intersection of sigma field is still sigma field. (\textbf{No for Union}.)
		\item[$\cdot$] To obtain a sigma field from union of sigma fields, define:
		\begin{equation}
			\bigvee_{\alpha \in I}\Sigma_{\alpha}:=\sigma(\bigcup_{\alpha \in I}\Sigma_{\alpha})
		\end{equation}
		\item[$\cdot$] $\sigma(\sigma(C))=\sigma(C)$.
		\item[$\cdot$] $A \subseteq B \Rightarrow\sigma(A)\subseteq\sigma(B)$.
	\end{itemize}

	\item[\textit{Def.}] \textbf{Borel Sigma Field}: $S$ is topological space (where open sets can be defined).
	\begin{equation}
	 	\mathscr{B}(S):=\sigma(\{O \subseteq S: O~\text{is open}\})
	\end{equation}
	\item[\textit{Rm.}] \textbf{Borel Sigma Field on Real Line} By construction of open sets, $\forall$ open set $O \subseteq \mathbb{R}$, $O$ can be written as: $O=\bigcup_{k=1}^{n}(a_k, b_k)$. Therefore, Borel sigma field on real line is actually: 
	\begin{equation}
		\mathscr{B}(\mathbb{R})=\sigma(\{(a,b): a,b \in \mathbb{R}, a<b\})
	\end{equation}

	\item[\textit{Def.}] \textbf{Measurable Space}: Set $S$ equipped with sigma field $\Sigma$, i.e pair $(S, \Sigma)$ is a measurable space. $A\in \Sigma$ is $\Sigma$-measurable subset of $S$.
\end{itemize}

%------------------------------------------------------------------------
\subsection{Pi System and Dynkin's D System}
\begin{itemize}
	\item[\textit{Def.}] \textbf{Pi System}: A family of $A \subseteq S$, $\mathcal{I}$ is a $\pi$-system if
	\begin{itemize}
		\item[$\cdot$] $I_1, I_2 \in \mathcal{I} \Rightarrow I_1 \cap I_2 \in \mathcal{I}$. (closed to \textbf{Finite Intersection})
	\end{itemize}

	\item[\textit{Rm.}] $\pi$ systems are easier then sigma field. For example, $\mathbb{R}$ generated $\pi$ (one notion) is family of all intervals of form $(-\infty,x]$.
	\begin{equation}
		\pi(\mathbb{R})=\{(-\infty,x]: x\in \mathbb{R}\}
	\end{equation}

	\item[\textit{Def.}] \textbf{D System}: A family of $A \subseteq S$, $\mathcal{D}$ is Dykin's $d$-system if
	\begin{itemize}
		\item[$\cdot$] $S \in \mathcal{D}$.
		\item[$\cdot$] $A,B \in \mathcal{D}$, $A \subseteq B \Rightarrow B \setminus A \in \mathcal{D}$.
		\item[$\cdot$] $A_n \in \mathcal{D}, n \geq 1, A_n \nearrow A \Rightarrow \lim_{n \to \infty}A_n = A \in \mathcal{D}$. (closed to \textbf{limit from below})
	\end{itemize}

	\item[\textit{Prop.}] $\Sigma$ is a $\sigma$-algebra $\iff$ $\Sigma$ is both $\pi$-system and $d$-system. 

	\item[\textit{Proof.}] $\Rightarrow$ is obvious.\newline
	$\Leftarrow$: Check against 3 defining properties. (1) by 1-d. (2) by 2-d, pick $B=S\in \Sigma$, $A^c=B\setminus A\in \Sigma$. (3) Consider 
	\begin{equation}
		U_n:=\bigcup_{n\geq 1}B_n=(\bigcap_{n\geq 1}B_n^c)^c \in \Sigma
	\end{equation}
	This is ensured by 1-pi and 2-d. And $U_n\nearrow \bigcup_{n\geq 1}B_n=:U$; by 3-d, $U\in\Sigma$. $\blacksquare$
\end{itemize}

%------------------------------------------------------------------------
\subsection{Dynkin's Lemma}
\begin{itemize}
	\item[\textit{Thm.}] (\textbf{Dynkin}) If $\mathcal{I}$ is a $\pi$-system on $S$, $\mathcal{D}$ is a $d$-system on $S$; $\mathcal{I} \subseteq \mathcal{D}$. \textbf{Then} $\sigma(\mathcal{I}) \subseteq \mathcal{D}$.
	
	\item[\textit{Proof.}] $d(\mathcal{I}):=$d system generated by $\mathcal{I}$.\footnote{$d(\mathcal{I})$ is d system, $d(\mathcal{I})$ subset $\mathcal{I}$ no other d system subset $d(\mathcal{I})$ supset $\mathcal{I}$}. Define
	\begin{equation}
		\mathcal{D}_1:=\{B\in d(\mathcal{I}): A\cap B\in d(\mathcal{I}), \forall A\in \mathcal{I}\}
	\end{equation}
	By definition $\mathcal{D}_1 \subseteq d(\mathcal{I})$. Clearly $\mathcal{I}\subseteq \mathcal{D}_1$, so if $\mathcal{D}_1$ is d-system, we will have $d(\mathcal{I})=\mathcal{D}_1$. Consider any $A\in \mathcal{I}$:
	\begin{itemize}
		\item[$\cdot$] $S\cap A=A$. $\Rightarrow S \in \mathcal{D}_1$.
		\item[$\cdot$] $(B_1\setminus B_2)\cap A=(B_1 \cap A)\setminus(B_2)=:D$. Both sides of setminus $\in d(\mathcal{I})$. Since $d(\mathcal{I})$ is d-system, $D\in d(\mathcal{I})$.
		\item[$\cdot$] $B_n\nearrow U:=\bigcup_{n\geq 1}B_n$. $A\cap B_n \nearrow A\cap U\in d(\mathcal{I})$. So $U\in \mathcal{D}_1$. \textit{Check.}
	\end{itemize}
	Define
	\begin{equation}
		\mathcal{D}_2 := \{C\in d(\mathcal{I}): B\cap C\in d(\mathcal{I}), \forall B\in d(\mathcal{I})\}
	\end{equation}
	$\mathcal{I}\subseteq \mathcal{D}_2$. Similarly, we check that $\mathcal{D}_2$ is indeed a d-system. So $\mathcal{D}_2 = d(\mathcal{I})$. Now we check $\mathcal{D}_2$ is a pi-system. Consider any $B\in d(\mathcal{I})$:
	\begin{itemize}
		\item[$\cdot$] $(C_1 \cap C_2) \cap B=C_1 \cap (C_2\cap B)=:C_1 \cap B'$. By definition of $\mathcal{D}_2$, $B'\in d(\mathcal{I})$; $C_1 \cap B'\in d(\mathcal{I})$. \textit{Check}.
	\end{itemize}
	Now that $d(\mathcal{I})=:\Sigma$ is both pi and d, it is a sigma field. \newline
	Since $\mathcal{I}\in \Sigma$, $\sigma(\mathcal{I})\subseteq \Sigma$.\newline
	For any other d-system $\mathcal{D}'\supseteq \mathcal{I}$. Therefore 
	\begin{equation}
		\mathcal{I}\subseteq\sigma(\mathcal{I})\subseteq\Sigma:=d(\mathcal{I})\subseteq \mathcal{D}'
	\end{equation}
	For any d-system $\mathcal{D}'\supseteq \mathcal{I}$. $\blacksquare$

	\item[\textit{Rm.}] We claim without proof that $\sigma(E)\supseteq d(E)$ for any set $E$. Generated sigma field is always more complex then generated d. Dynkin's suggests that, if $E=\mathcal{I}$ is pi system, then
	\begin{equation}
		d(\mathcal{I})=\sigma(\mathcal{I})~~\mathcal{I}\text{ - pi system.}
	\end{equation}
\end{itemize}


%////////////////////////////////////////////////////////////////////////
\section{Measure}

%////////////////////////////////////////////////////////////////////////
\section{Events}

%------------------------------------------------------------------------
\subsection{Events as Sets}
\begin{itemize}
	\item[\textit{Def.}] \textbf{Events}: In prob space $(\Omega, \mathcal{F}, \mathbb{P})$, set $E\in \mathcal{F}$ is an event.
	\begin{itemize}
		\item[$\cdot$] If $\mathbb{P}(E)=1$, say $E$ happens \textbf{Almost Surely}. If $\mathbb{P}(E)=0$, say $E$ happens \textbf{Almost Nowhere}.
	\end{itemize}
\end{itemize}

%------------------------------------------------------------------------
\subsection{IO and EV}
\begin{itemize}
	\item[\textit{Def.}] \textbf{$\mathbf{E}_n$ Infinitely Often}: Sequence of events $\{E_n\}\in \mathcal{F}$, define:
	\begin{equation}
		\{E_n~i.o.\}=\limsup\limits_{n\rightarrow\infty}E_n:=\bigcap_{n\geq 1}\bigcup_{m\geq n}E_m
	\end{equation}
	Clearly $U_n=\bigcup_{m\geq n}E_m\searrow\{E_n~i.o.\}$. Because it is a union of less and less sets. Therefore by continuity from above:
	\begin{equation}
		\{E_n~i.o.\}=\lim\limits_{n\rightarrow\infty}U_n
	\end{equation}


	\item[\textit{Def.}] \textbf{$\mathbf{E}_n$ Eventually Always}: Sequence of events $\{E_n\}\in \mathcal{F}$, define:
	\begin{equation}
		\{E_n~e.v.\}=\liminf\limits_{n\rightarrow\infty}E_n:=\bigcup_{n\geq 1}\bigcap_{m\geq n}E_m
	\end{equation}
	Clearly $A_n=\bigcap_{m\geq n}E_m\nearrow\{E_n~i.o.\}$. Because it is an intersection of less and less sets. Therefore by continuity from below:
	\begin{equation}
		\{E_n~e.v.\}=\lim\limits_{n\rightarrow\infty}A_n
	\end{equation}


	\item[\textit{Prop.}] \textbf{Properties about limit events}
	\begin{itemize}
		\item[] \textbf{Basic}:
		\item[1.] $\liminf\limits_{n\rightarrow\infty}E_n \subseteq \limsup\limits_{n\rightarrow\infty}E_n$ 
		\item[2.] $(\limsup\limits_{n\rightarrow\infty}E_n)^c=\liminf\limits_{n\rightarrow\infty}E_n^c$
		\item[3.] $\lim\limits_{n\rightarrow\infty}E_n=E \iff \limsup\limits_{n\rightarrow\infty}E_n=\liminf\limits_{n\rightarrow\infty}E_n=E$

		\item[] \textbf{Cap/Cup}:
		\item[4.] $(\limsup\limits_{n\rightarrow\infty}A_n)\cup(\limsup\limits_{n\rightarrow\infty}B_n)=\limsup\limits_{n\rightarrow\infty}(A_n\cup B_n)$
		\item[5.] $(\limsup\limits_{n\rightarrow\infty}A_n)\cap(\limsup\limits_{n\rightarrow\infty}B_n)\supseteq\limsup\limits_{n\rightarrow\infty}(A_n\cap B_n)$
		\item[6.] $(\liminf\limits_{n\rightarrow\infty}A_n)\cap(\liminf\limits_{n\rightarrow\infty}B_n)=\liminf\limits_{n\rightarrow\infty}(A_n\cap B_n)$
		\item[7.] $(\liminf\limits_{n\rightarrow\infty}A_n)\cup(\liminf\limits_{n\rightarrow\infty}B_n)\subseteq\liminf\limits_{n\rightarrow\infty}(A_n\cup B_n)$

		\item[] \textbf{Setminus}:
		\item[8.] $(\limsup\limits_{n\rightarrow\infty}E_n)\setminus(\liminf\limits_{n\rightarrow\infty}E_n)=\limsup\limits_{n\rightarrow\infty}(E_n\setminus E_{n+1})$

		\item[]	\textbf{With Measure}:
		\item[9.] $\mathbb{P}(\liminf\limits_{n\rightarrow\infty}E_n)\leq \liminf\limits_{n\rightarrow\infty}\mathbb{P}(E_n)\leq \limsup\limits_{n\rightarrow\infty}\mathbb{P}(E_n)\leq \mathbb{P}(\limsup\limits_{n\rightarrow\infty}E_n)$\footnote{First $\leq$ is Fatou's lemma, third is reverse-Fatou's lemma.}
		\item[10.] If $\lim\limits_{n\rightarrow\infty}E_n=E$, then $\mathbb{P}(\lim\limits_{n\rightarrow\infty}E_n)=\mathbb{P}(E)$.
	\end{itemize}

	\item[\textit{Proofs.}] for some of above.
	\begin{itemize}
		\item[8.]
		\item[9.] i.e.
		\begin{equation}
		\begin{split}
			\mathbb{P}(\{E_n~e.v\}) &\leq \liminf\limits_{n\rightarrow\infty}\mathbb{P}(E_n)\\
			&\leq \limsup\limits_{n\rightarrow\infty}\mathbb{P}(E_n) \leq \mathbb{P}(\{E_n~i.o\})
		\end{split}
		\end{equation}
		$\mathbb{P}(\{E_n~i.o\})=\mathbb{P}(\lim\limits_{n\rightarrow\infty}U_n)$\newline
		Cont from above (need finiteness of $\mathbb{P}$ !): $\mathbb{P}(\lim\limits_{n\rightarrow\infty}U_n)=\lim\limits_{n\rightarrow\infty}\mathbb{P}(U_n)$. \newline
		Clearly $\mathbb{P}(U_n)\geq \sup\limits_{n\geq m}\mathbb{P}(E_n)$.\footnote{LHS is union, RHS is picking maximum from $E_n$.} Take limit both side: \newline
		$\mathbb{P}(\{E_n~i.o\})=\lim\limits_{n\rightarrow\infty}\mathbb{P}(U_n)\geq \lim\limits_{n\rightarrow\infty}\sup\limits_{n\geq m}\mathbb{P}(E_n)=:\limsup\limits_{n\rightarrow\infty}\mathbb{P}(E_n)$. $\blacksquare$
	\end{itemize}
\end{itemize}

%------------------------------------------------------------------------
\subsection{Fatou's Lemma}
\begin{itemize}
	\item[\textit{Lemma}] (\textbf{Reverse FATOU} - Need Finiteness of $\mathbb{P}$)
	\begin{equation}
		\limsup\limits_{n\rightarrow\infty}\mathbb{P}(E_n) \leq \mathbb{P}(\{E_n~i.o\})
	\end{equation}

	\item[\textit{Lemma}] (\textbf{FATOU} - Apply for General Measure)
	\begin{equation}
		\mathbb{P}(\{E_n~e.v\}) \leq \liminf\limits_{n\rightarrow\infty}\mathbb{P}(E_n)
	\end{equation}
\end{itemize}

%------------------------------------------------------------------------
\subsection{Borel-Cantelli 1st Lemma}
\begin{itemize}
	\item[\textit{Thm.}] (\textbf{BC 1}) In $(\Omega, \mathcal{F}, \mathbb{P})$, $\{E_n\}\subseteq \mathcal{F}$:
	\begin{equation}
		\sum_{n\geq 1}\mathbb{P}(E_n)<\infty \Rightarrow \mathbb{P}(\{E_n~i.o\})=0
	\end{equation}

	\item[\textit{Proof.}] Since $U_n\searrow \{E_n~i.o\}$ $\Rightarrow$ $U_{n}\subseteq U_{n-1}\subseteq ... \subseteq U_1$.
	\begin{equation}
	\begin{split}
		\mathbb{P}(\{E_n~i.o\})&=\lim\limits_{n\rightarrow\infty}\mathbb{P}(U_n)\\
		&\leq \mathbb{P}(U_1)\\
		&\leq \sum_{m\geq 1}\mathbb{P}(E_m)=0~~\blacksquare
	\end{split}
	\end{equation}
\end{itemize}


%------------------------------------------------------------------------
%////////////////////////////////////////////////////////////////////////
%////////////////////////////////////////////////////////////////////////
%------------------------------------------------------------------------
\chapter{Mapping, RV}

%////////////////////////////////////////////////////////////////////////
\section{Measurable Function}

%////////////////////////////////////////////////////////////////////////
\section{Random Variable}

%////////////////////////////////////////////////////////////////////////
\section{Law, Distribution Function}

%////////////////////////////////////////////////////////////////////////
\section{Convergence of RV}

%------------------------------------------------------------------------
%////////////////////////////////////////////////////////////////////////
%////////////////////////////////////////////////////////////////////////
%------------------------------------------------------------------------
\chapter{Independence}

%////////////////////////////////////////////////////////////////////////
\section{Independence: Sets}

%------------------------------------------------------------------------
\subsection{Indep Events}
\begin{itemize}
	\item[\textit{Def.}] \textbf{Mutually Independent Events}: Events in $\{E_n\}$ sequence are mutually indep. $\iff$ whatever $k\geq1$, index-subsequence $\{n_1, n_2, ... n_k\}$: 
	\begin{equation}
		\mathbb{P}(E_{n_1}\cap E_{n_2}\cap ... \cap E_{n_k})=\prod_{j=1}^k \mathbb{P}(E_{n_j})
	\end{equation}

	\item[\textit{Rm.}] 
	\begin{itemize}
		\item[$\cdot$] $A\perp B\iff A^c \perp B \iff A^c\perp B^c$
		\item[$\cdot$] If $\mathbb{P}\left(A\right)=1$ or $0$ $\Rightarrow$ $A\perp~\forall B\in \mathcal{F}$.
	\end{itemize}

	\item[\textit{Def.}] \textbf{Pairwise Indep}: $\{E_n\}$ sequence are pairwise indep if $\mathbb{P}\left(E_i\cap E_j\right)=\mathbb{P}\left(E_i\right)\mathbb{P}\left(E_j\right)$, $\forall i\ne j$.
\end{itemize}

%------------------------------------------------------------------------
\subsection{Indep Sigma Field}
\begin{itemize}
	\item[\textit{Def.}] \textbf{Independent Sigma Field}: Sequence (Not necessarily finite) of sub sigma-fields $\mathcal{G}_1, \mathcal{G}_2 ... $ of $\mathcal{F}$ are indep. if, for any $k$, any subsequence of $k$ distinct members: $\{\mathcal{G}_{n_1}, \mathcal{G}_{n_2}, ... ,\mathcal{G}_{n_k}\}$ ($\{n_k\}$ distinct), any choice of set ${G}_{i}\in \mathcal{G}_{i}$: 
	\begin{equation}
		\mathbb{P}(G_{n_1}\cap G_{n_2}\cap ... \cap G_{n_k})=\prod_{j=1}^k \mathbb{P}(G_{n_j})
	\end{equation}

	\item[\textit{Def.}] \textbf{Indep of Events - Redefine}: Events $\{E_n\}$ are indep if sigma field $\{\mathcal{E}_n\}$ are indep, where
	\begin{equation}
		\mathcal{E}_i = \{\emptyset,~E_i,~\Omega \setminus E_i,~\Omega\}
	\end{equation}
\end{itemize}

%------------------------------------------------------------------------
\subsection{Pi System Lemma}
\begin{itemize}
	\item[\textit{Thm.}] (\textit{Study indep via generator pi systems}) $\mathcal{G}, \mathcal{H}$ are sub-sigma field of $\mathcal{F}$. $\mathcal{I}, \mathcal{J}$ are pi systems, where $\sigma(\mathcal{I})=\mathcal{G}$, $\sigma(\mathcal{J})=\mathcal{H}$. \textit{Then}
	$$\mathcal{G}\perp \mathcal{H}\iff \mathcal{I}\perp \mathcal{J}$$
	i.e. $\forall~I\in \mathcal{I}$, $J\in \mathcal{J}$: 
	\begin{equation}
		\mathbb{P}\left(I\cap J\right)=\mathbb{P}\left(I\right)\mathbb{P}\left(J\right)
	\end{equation}

	\item[\textit{Proof.}]
\end{itemize}

%------------------------------------------------------------------------
\subsection{Borel-Cantelli 2nd Lemma}
\begin{itemize}
	\item[\textit{Thm.}] (\textbf{BC 2}) $\{E_n\}$ is a seq of \textbf{INDEPENDENT} events, \textit{then}
	\begin{equation}
		\sum_{n\geq 1}\mathbb{P}\left(E_n\right)=\infty \Rightarrow \mathbb{P}\left(\{E_n~i.o\}\right)=1
	\end{equation}

	\item[\textit{Proof.}] Do the complement, i.e. $\mathbb{P}\left(\{E_n^c~e.v\}\right)=0$.
	\begin{equation}
		\begin{split}
			\{E_n^c~e.v\}&=\liminf\limits_{n\rightarrow\infty}E^c_n=\bigcup_{n\geq 1}\bigcap_{m\geq n}E_m^c=\bigcup_{n\geq 1}A_n
		\end{split}
	\end{equation}
	\begin{equation}
		\begin{split}
			\mathbb{P}\left(A_n\right)&=\mathbb{P}\left(\bigcap_{m\geq n}E_m^c\right)=\prod_{n\geq m}(1- \mathbb{P}\left(E_n\right))\\
			&\leq \exp\left[-\sum_{m\geq n}\mathbb{P}\left(E_n\right)\right]=0
		\end{split}
	\end{equation}
	So $\mathbb{P}\left(\{E_n^c~e.v\}\right)\leq \sum_{n\geq 1}\mathbb{P}\left(A_n\right)=0$. $\blacksquare$
\end{itemize}

%------------------------------------------------------------------------
\subsection{Tail Sigma Field, Kolmogorov 0/1}
\begin{itemize}
	\item[\textit{Def.}] \textbf{Tail Sigma Field} associated with a sequence of events:
	\begin{equation}
		\mathcal{T}:=\bigcap_{n\geq 1}\sigma(\{E_m\}: m\geq n)=\bigcap_{n\geq 1}\sigma(E_n, E_{n+1}, E_{n+2}, ...)
	\end{equation}

	\item[\textit{Thm.}] (\textbf{Kolmogorov 0/1}) If $\{E_n\}$ is \textbf{Indep} sequence, $\mathcal{T}$ is tail associated with $\{E_n\}$. \textit{Then}, $\mathbb{P}\left(A\right)=0$ or $1$ $\forall A\in \mathcal{T}$.
\end{itemize}


%////////////////////////////////////////////////////////////////////////
\section{Independence: RV}

%------------------------------------------------------------------------
\subsection{With Expectations}
\textbf{Note}: This section is introduced after chapter 4.
\begin{itemize}
	\item[\textit{Lemma.}] $X,Y$ are indep RV, $X\in \mathcal{L}^1$, then $\forall B \subseteq \mathscr{B}(\mathbb{R})$,
	\begin{equation}
		\mathbb{E}\left[X; Y\in B\right]= \mathbb{E}\left[X\right] \cdot \mathbb{P}\left(Y\in B\right)~~~~\#
	\end{equation}

	\item[\textit{Proof.}] If $X=\mathbbm{1}_{A}$ indicator, \# is clearly true. \newline
	By linearity, \# holds for $X\in SF^+$. \newline
	By (MON), \# holds for $X\in m \mathcal{F}^+$. \newline
	Since $X\in \mathcal{L}^1$, so do $X^{\pm}$. All integrals involved in \# are finite, linearlity $\Rightarrow$ \# holds for any $X\in m \mathcal{F}$. $\blacksquare$

	\item[\textit{Thm.}] (\textit{Indep: product in expectation is expectation of product.}) If $X, Y$ indep, $X,Y \in \mathcal{L}^1$; \textit{then} $XY\in \mathcal{L}$ and $\mathbb{E}\left[XY\right]=\mathbb{E}\left[X\right]\mathbb{E}\left[Y\right]$.

	\item[\textit{Proof.}] Assume $Y= \mathbbm{1}_{A}$. By lemma \#, for all $X\in m \mathcal{F}$:
	\begin{equation}
		\mathbb{E}\left[XY\right] = \mathbb{E}\left[X;A\right]=\mathbb{E}\left[X\right] \mathbb{P}\left(A\right) = \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right]
	\end{equation}
	Implies thm holds for $Y$ indicator. By linearity, holds for simples. \newline
	(MON) $\Rightarrow$ holds for non-negative. \newline
	Since $X, Y \in \mathcal{L}^1$, holds for $X^{\pm}, Y^{\pm}$. All integrals involved in equation are finite. linearity $\Rightarrow$ holds for all $Y\in m \mathcal{F}$. $\blacksquare$

	\item[\textit{Cor.}] (\textit{Composition with Borel function}) $X,Y$ indep (does not require integrability in $X,Y$ themselves), and $f,g$ are Borel functions, $f(X)\in \mathcal{L}^1$, $g(Y) \in \mathcal{L}^1$; \textit{then} 
	\begin{equation}
		\mathbb{E}\left[f(X)g(Y)\right]=\mathbb{E}\left[f(X)\right]\mathbb{E}\left[g(Y)\right]
	\end{equation}

	\item[\textit{Proof.}] Apply thm above. Note that $f(X), g(Y)$ are indep RVs. Since $f(X)\in m \sigma(X)$, $g(Y) \in m \sigma(Y)$, $X\perp Y$. $\blacksquare$

	\item[\textit{Cor.}] (\textit{Covariance}): If $X,Y$ are serially uncorrelated, \textit{then} $\mathrm{Cov}\left[X,Y\right]=0$. Moreover, if process $\{X_n\}\in \mathcal{L}^2$, \textit{then} define $S_n:=\sum_1^n X_j$ as partial sum, we have $\mathrm{Var}\left[S_n\right]=\sum_1^n \mathrm{Var}\left[X_j\right]$
\end{itemize}

%------------------------------------------------------------------------
%////////////////////////////////////////////////////////////////////////
%////////////////////////////////////////////////////////////////////////
%------------------------------------------------------------------------
\chapter{Integration, Expectation}

%////////////////////////////////////////////////////////////////////////
\section{Integration}

%------------------------------------------------------------------------
\subsection{Integrability, L1 Space}
Default setting: in general (abstract) measure space $(S, \Sigma, \mu)$.
\begin{itemize}
	\item[\textit{Def.}] \textbf{Integrable}: $f\in m\Sigma$ is $\mu$-integrable, denote $f\in \mathcal{L}^1(S, \Sigma, \mu)$ if both $\mu(f^+)$ and $\mu(f^-)$ are finite. \newline
	$\iff \mu(|f|)<\infty$

	\item[\textit{Prop.}] \textbf{Properties of $\mathcal{L}^1$ Functions}: if $f\in \mathcal{L}^1$
	\begin{itemize} 
		\item[$\cdot$] $\mu(\{f=\pm\infty\})=0$.
		\item[$\cdot$] $|\mu(f)|\leq \mu(|f|)$
		\item[$\cdot$] (\textit{linearity}) if $f,g \in \mathcal{L}^1$, $a,b \in \mathbb{R}$ then $af+bg\in \mathcal{L}^1$.
		\item[$\cdot$] (\textit{monotonicity}) if $f\leq g~a.e$, then $\mu(f)\leq \mu(g)$.
	\end{itemize}

	\item[\textit{Proof.}] for some
	\begin{itemize}
		\item[] (\textit{linearity}) First show $f+g \in \mathcal{L}^1$. In that $|f+g|\leq |f|+|g|$ everywhere $\Rightarrow$ $\mu(|f+g|)\leq \mu(|f|)+\mu(|g|)$. \newline
		Then prove linearity. Let $h:=f+g$, $h^+-h^-=f^+-f^-+g^+-g^-$. Shift to obtain plus sign: $h^++f^-+g^-=h^-+f^++g^+$. $\Rightarrow \mu(h^++f^-+g^-)=\mu(h^-+f^++g^+)$. 
		Apply linearity for $m\Sigma^+$ functions both sides. Also since $h^{\pm},f^{\pm},g^{\pm} \in \mathcal{L}^1$, we can shift things back. $\blacksquare$
	
	\end{itemize}
\end{itemize}


%////////////////////////////////////////////////////////////////////////
\section{Convergence Theorems}
Default setting: In general measure space $(S, \Sigma, \mu)$. $f_n: S\mapsto \bar{\mathbb{R}}$ (extended real line), $f: S\mapsto \bar{\mathbb{R}}$; $f_n, f\in m\Sigma$.

%------------------------------------------------------------------------
\subsection{Monotone Convergence Thm}
\begin{itemize}
	\item[\textit{Thm.}] (\textbf{MON}) If $f_n \nearrow f$, and $\mu(f_1^-)<\infty$; \textit{then} $\mu(f_n)\nearrow \mu(f)$.

	\item[\textit{Rm.}] (\textbf{MON}) still applies if $f_n\nearrow f~a.s$. This MON is also a more general version, which only requires one support from $\mu(f_1^-)$.

	\item[\textit{Cor.}] (\textbf{Nonnegative - MON}): If ${f_n}\in m\Sigma^+$, $f_n \nearrow f$, then $\mu(f_n)\nearrow \mu(f)$.

	\item[\textit{Cor.}] (\textbf{Reverse - MON}): If $f_n \searrow f$, and $\mu(f^-_1)\leq \infty$; \textit{then} $\mu(f_n)\searrow \mu(f)$.
	\item[\textit{Proof.}] Define $g_n := f_1^+ - f_n$, then $g_n\geq 0$ in that $f_1^+ \geq f_n^+ \geq f_n$. Clearly $g_n \nearrow g:=f_1^+-f$. \newline
	Apply (\textbf{MON}) to $\{g_n\}$: 
\end{itemize}

%------------------------------------------------------------------------
\subsection{Fatou's Lemma}
\begin{itemize}
	\item[\textit{Thm.}] (\textbf{FATOU}): If exists $g: S\mapsto \bar{\mathbb{R}}$, $g\in m\Sigma$, $\mu(g^-)< \infty$. And that $f_n\geq g$ uniformly $\forall n\geq 1$. \textit{Then},
	\begin{equation}
	 	\mu(\liminf\limits_{n\rightarrow\infty}f_n)\leq \liminf\limits_{n\rightarrow\infty}(\mu(f_n))
	\end{equation} 

	\item[\textit{Proof.}] Define $g_n:=\inf\limits_{m\geq n}f_m$, clearly
	\begin{equation}
		g_n \nearrow  \sup\limits_{n\geq 1}\inf\limits_{m\geq n}f_m =: \liminf\limits_{n\rightarrow\infty}f_n
	\end{equation}
	$g_n=\inf\limits_{m\geq n}f_m\geq g$\footnote{Since every $f_m$ in infimum $\geq g$ uniformly.} for $\forall n$. So $g_n^-\leq g^-$.\newline
	Thus $\{g_n\}$ are supported by $\mu(g_1^-)\leq \mu(g^-) < \infty$. Apply \textbf{(MON)}: $\mu(g_n)\nearrow \mu(\liminf\limits_{n\rightarrow\infty}f_n)$\newline
	On the other hand, $g_n + g^- = g_n^+ +(g^- - g_n^-) \in m\Sigma^+$ non-negative. By definition of $g_n$: $0\leq g_n+g^- \leq f_m +g^-\in m\Sigma^+$ $\forall m\geq n$. Use monotonicity/linearity for plus non-negatives:
	\begin{equation}
		\begin{split}
			\mu(g_n+g^-) &\leq \mu(f_m +g^-)\\
			\mu(g_n)+\mu(g^-) &\leq \mu(f_m) + \mu(g^-)\\
			\mu(g_n) &\leq \inf\limits_{m\geq n}\mu(f_m)\\
		\end{split}
	\end{equation}
	Holds for all $n\geq 1$. Let $n\to \infty$ both sides in increasingly:
	\begin{equation}
		\mu(\sup\limits_{n\geq 1}\inf\limits_{m\geq n}f_m)\nwarrow\mu(\inf\limits_{m\geq n}f_m) \leq \inf\limits_{m\geq n}\mu(f_m) \nearrow \sup\limits_{n\geq 1}\inf\limits_{m\geq n}\mu(f_n)
	\end{equation}
	where nwarrow follows (\textbf{MON}), nearrow is just taking limit directly. Anyway, we have:
	\begin{equation}
		\mu(\sup\limits_{n\geq 1}\inf\limits_{m\geq n}f_m) \leq \sup\limits_{n\geq 1}\inf\limits_{m\geq n}\mu(f_n)~~~\blacksquare
	\end{equation}

	\item[\textit{Rm.}] $\leq$ in (\textbf{FATOU}) can be strict $<$. Consider: $f_n = \mathbbm{1}_{[n, n+1]}$ is a moving hat to plus inf. Clearly $\liminf\limits_{n\rightarrow\infty}f_n=0$, because for any $x$, after $N>x$, $f_n(x)\equiv0$. But $\mu(f_n)\equiv1$. $0=\mu(\liminf\limits_{n\rightarrow\infty}f_n)<\liminf\limits_{n\rightarrow\infty}\mu(f_n)=\mu(f_n)=1$.

	\item[\textit{Thm.}] (\textbf{Reverse - FATOU}) If exists $g: S\mapsto \bar{\mathbb{R}}$, $g\in m\Sigma$, $\mu(g^+)< \infty$. And that $f_n\leq g$ uniformly $\forall n\geq 1$. \textit{Then},
	\begin{equation}
	 	\mu(\limsup\limits_{n\rightarrow\infty}f_n)\geq \limsup\limits_{n\rightarrow\infty}(\mu(f_n))
	\end{equation} 
\end{itemize}

%------------------------------------------------------------------------
\subsection{Dominated Convergence Thm}
\begin{itemize}
	\item[\textit{Thm.}] (\textbf{DOM}) $f_n \xrightarrow{a.s.} f$. For some $g\in \mathcal{L}^1$, $|f_n|\leq g$ uniformly. \textit{Then} $f_n \xrightarrow{\mathcal{L}^1} f$. \newline
	In particular $f\in \mathcal{L}^1$, $\mu(f_n)\to \mu(f)$.

	\item[\textit{Proof.}] Clearly $f_n \in \mathcal{L}^1$ for all $n$. \newline
	$|f|=\lim\limits_{n\rightarrow\infty}|f_n|\leq g$. So $f\in \mathcal{L}^1$.\newline
	By pointwise ($a.s.$) convergence, $\limsup\limits_{n\rightarrow\infty}|f_n-f|=0$. \newline
	Moreover $|f_n-f|\leq 2|g|$ uniformly. $\mu(g^+)<\infty$. Apply (\textbf{Reverse - FATOU}):
	\begin{equation}
		0=\mu(\limsup\limits_{n\rightarrow\infty}|f_n-f|)\geq \limsup\limits_{n\rightarrow\infty}(\mu(|f_n-f|))\geq 0
	\end{equation}
	So,
	\begin{equation}
		0=\limsup\limits_{n\rightarrow\infty}(\mu(|f_n-f|))\geq \liminf\limits_{n\rightarrow\infty}\mu(|f_n-f|)
	\end{equation}
	i.e. $\limsup\limits_{n\rightarrow\infty}(\mu(|f_n-f|))= \liminf\limits_{n\rightarrow\infty}\mu(|f_n-f|)=0$. Therefore $f_n \xrightarrow{\mathcal{L}^1} f$. $\blacksquare$

	\item[\textit{Rm.}] $\xrightarrow{a.s.}$ \textbf{Does NOT imply} $\xrightarrow{\mathcal{L}^1}$, so the dominant condition cannot be dropped. $\xrightarrow{\mathcal{L}^1}$ \textbf{Does NOT imply} $\xrightarrow{a.s.}$ either. 
\end{itemize}

%------------------------------------------------------------------------
\subsection{Scheffe's Lemma}
\begin{itemize}
	\item[\textit{Thm.}] (\textbf{SCHEFFE}) $f,f_n \in \mathcal{L}^1$, $f_n \xrightarrow{a.s.} f$. \textit{Then}
	\begin{equation}
		\mu(f_n)\to \mu(f) \iff f_n \xrightarrow{\mathcal{L}^1} f
	\end{equation}

	\item[\textit{Proof.}] $\Leftarrow$ is clear. Prove $\Rightarrow$. \newline
	Define $g_n:=|f_n|+|f|-|f_n-f|\geq 0$ uniformly. Just for checking, $\mu(0^+)<\infty$. Apply (\textbf{FATOU}) to $g_n$:
	\begin{equation}
		\mu(\liminf\limits_{n\rightarrow\infty}|f_n|+|f|-|f_n-f|) \leq \liminf\limits_{n\rightarrow\infty}\mu(|f_n|+|f|-|f_n-f|)
	\end{equation}
	By $a.s$ convergence $\mu(\limsup\limits_{n\rightarrow\infty}|f_n-f|)=0$.
	\begin{equation}
		\begin{split}
			LHS &= \mu(2|f|-\limsup\limits_{n\rightarrow\infty}|f_n-f|)\\
			&= 2\mu(|f|)-\mu(\limsup\limits_{n\rightarrow\infty}|f_n-f|) 
		\end{split}
	\end{equation}
	Note that $\inf$ is switched to $\sup$ when taking minus out.
	\begin{equation}
		\begin{split}
			RHS &= 2\mu(|f|)-\limsup\limits_{n\rightarrow\infty}\mu(|f_n-f|)
		\end{split}
	\end{equation}
	$f\in \mathcal{L}^1$ so it can be cancelled out. 
	\begin{equation}
		0=\mu(\limsup\limits_{n\rightarrow\infty}|f_n-f|)\geq \limsup\limits_{n\rightarrow\infty}\mu(|f_n-f|)
	\end{equation}
	$\mu(|f_n-f|)\to 0$. $\blacksquare$
\end{itemize}

%////////////////////////////////////////////////////////////////////////
\section{Radon-Nikodyn Thm}
\begin{itemize}
	\item[\textit{Def.}] \textbf{$f\mu$ measure}: $f\in (m\Sigma)^+$ \textbf{Non-negative!}, $f\mu$ is a new measure on measurable space $(S, \Sigma)$ defined for $A\in \Sigma$ as
	\begin{equation}
		f\mu(A) := \int_A fd\mu = \mu(f \mathbbm{1}_{A})
	\end{equation}
	Easy to check that this definition is indeed a measure (contable additive).

	\item[\textit{Prop.}] For $h\in (m\Sigma)^+$ (\textbf{Non-negative}): $(f\mu)(h)=\mu(fh)$ (\#).
	\item[\textit{Proof}.] Let $h=\mathbbm{1}_{A}$, $A\in \Sigma$. Then
	\begin{equation}
		(f\mu)(h):= \int_{\Omega} f\mathbbm{1}_{A} d\mu = \mu(f \mathbbm{1}_{A})=\mu(fh)
	\end{equation}
	holds for indicators.
	By linearity, (\#) holds for $h\in SF^+$.\newline
	By (\textbf{MON}), (\#) holds for $h\in (m\Sigma)^+$. $\blacksquare$

	\item[\textit{Cor.}] For $h\in m\Sigma$ (\textbf{General} fucntion now!), \textit{then},
	\begin{equation}
		h \in \mathcal{L}^1(S, \Sigma, f\mu) \iff f\cdot h\in \mathcal{L}^1(S,\Sigma, \mu)
	\end{equation}
	In particular, if this($h\in \mathcal{L}^1$) is the case, then $(f\mu)(h)=\mu(fh)$ (\#).
	\item[\textit{Proof.}] $h\in \mathcal{L}^1(S, \Sigma, \mu)$ $\iff$ $f\mu(h^+)=\mu(fh^+)<\infty$ and $f\mu(h^-)=\mu(fh^-)<\infty$. \newline
	Since $f\in m\Sigma^+$, above $\iff$ $\mu(fh^+)=\mu((fh)^+)<\infty$, $\mu(fh^-)=\mu((fh)^-)<\infty$. \newline
	$\iff$ $\mu(fh)<\infty \iff f\cdot h\in \mathcal{L}^1(S,\Sigma, \mu)$. The equality is clearly true. $\blacksquare$

	\item[\textit{Thm.}] (\textbf{Radon-Nikodyn}) If $\mu$, $\lambda$ are measures on $(S, \Sigma)$, both are $\sigma$-finite. Moreover, if $\lambda$ is absolutely continous wrt $\mu$, \footnote{i.e.(\textbf{ab.cont}) $\forall A\in \Sigma$, $\mu(A)=0$ $\Rightarrow$ $\lambda(A)=0$.} \textit{Then}, \newline
	Exists $f\in (m\Sigma)^+$, such that $\lambda = f\mu$. Define \textit{Radon-Nikodyn derivative} of $\lambda$ wrt $\mu$ as this $f$. Denote
	$$f=:\frac{d\lambda}{d\mu}$$ 

\end{itemize}

%////////////////////////////////////////////////////////////////////////
\section{Expectation}

%------------------------------------------------------------------------
\subsection{Notation}
\begin{itemize}
	\item[\textit{Def.}] \textbf{Expectation}: $(\Omega, \mathcal{F}, \mathbb{P})$, $X: \Omega \mapsto \bar{\mathbb{R}}$.
	\begin{equation}
		\mathbb{E}[X]:=\int_{\Omega} X d \mathbb{P}
	\end{equation}

	\item[\textit{Def.}] \textbf{Integrability}: $X\in \mathcal{L}^1(\Omega, \mathcal{F}, \mathbb{P})$ if $\mathbb{E}[X]< \infty$.
\end{itemize}

%------------------------------------------------------------------------
\subsection{Convergence Theorems}
Default setting: in prob space $(\Omega, \mathcal{F}, \mathbb{P})$. Sequence of RVs $X_n: \Omega \mapsto \bar{\mathbb{R}}$, $X: \Omega \mapsto \bar{\mathbb{R}}$ and $X_n, X\in m \mathcal{F}$. (Note: \textbf{NOT} imposing $X_n,X\in \mathcal{L}^1$ here in general.)
\begin{itemize}
	\item[\textit{Thm.}] (\textbf{MON}): $X_n \nearrow X$ ($\xrightarrow{a.s.}$), $\mathbb{E}[X_1^-]<\infty$; \textit{then} $\mathbb{E}[X_n]\nearrow \mathbb{E}[X]$.

	\item[\textit{Thm.}] (\textbf{FATOU}): $\mathbb{E}[X^-]<\infty$, $X_n\geq X$ for all $n\geq1$ for some $X$; \textit{then} $\mathbb{E}[\liminf\limits_{n\rightarrow\infty}X_n]\leq \liminf\limits_{n\rightarrow\infty}\mathbb{E}[X_n]$. (\textit{liminf inside $<$ liminf outside})

	\item[\textit{Thm.}] (\textbf{Revserse. FATOU}) $\mathbb{E}[X^+]<\infty$, $X_n\leq X$ for all $n\geq1$ for some $X$; \textit{then} $\mathbb{E}[\limsup\limits_{n\rightarrow\infty}X_n]\geq \limsup\limits_{n\rightarrow\infty}\mathbb{E}[X_n]$. (\textit{limsup inside $<$ limsup outside})

	\item[\textit{Thm.}] (\textbf{DOM}) $X_n \xrightarrow{a.s.} X$, $|X_n|\leq Y$ for some $Y\in \mathcal{L}^1$; \textit{then} $X_n \xrightarrow{\mathcal{L}^1} X$, i.e. $\mathbb{E}[|X_n-X|]\to 0$.

	\item[\textit{Thm.}] (\textbf{SCHEFFE}) $X_n, X\in \mathcal{L}^1$, $X_n\xrightarrow{a.s.} X$; \textit{then} $\mathbb{E}\left[X_n\right]\to \mathbb{E}\left[X\right] \iff X_n \xrightarrow{\mathcal{L}^1} X$. ($\Leftarrow$ is trivial)

	\item[\textit{Rm.}] (Strengthened version of convergence thms in Prob space) $X_n \xrightarrow{a.s.} X$ in \textbf{MON, DOM, SCHEFFE} can be replaced with $X_n \xrightarrow{i.p.} X$, same result can be obtained nevertheless.

\end{itemize}

%------------------------------------------------------------------------
\subsection{Lp Space}
\begin{itemize}
	\item[\textit{Def.}] \textbf{$\mathcal{L}^p$ Integrable, p-th Moment, $\mathcal{L}^p$ Norm}: $1\leq p<\infty$
	\begin{itemize}
		\item[$\cdot$] Define $X\in \mathcal{L}^p (\Omega, \mathcal{F}, \mathbb{P})$ if $|X|^p \in \mathcal{L}^1$, i.e. $\mathbb{E}[|X|^p]<\infty$.
		\item[$\cdot$] For $X\in \mathcal{L}^p$, define $\mathbb{E}[X^p]$ as p-th moment of $X$.
		\item[$\cdot$] Define $\mathcal{L}^p$ norm of $X$ as:
		\begin{equation}
			\|X\|_p:=\left(\mathbb{E}[|X|^p]\right)^{\frac{1}{p}}
		\end{equation}
	\end{itemize}
	\item[\textit{Prop.}] \textbf{Properties of $\mathcal{L}^p$}
		\begin{itemize}
			\item[$\cdot$] $\mathcal{L}^p$ is a vector space in $\mathbb{R}$.

			\item[$\cdot$] $\|X\|_p$ satisfies defining properties of norm: 
			\begin{itemize}
				\item[] $\|X\|_p \geq 0$.
				\item[] $\|X\|_p=0\Rightarrow X=0~a.s.$
				\item[] $\|cX\|_p = |c| \|X\|_p$, constant $c$.
				\item[] $\|X+Y\|_p \leq \|X\|_p + \|Y\|_p$ (\textit{triangle ineq.}) Equal sign achieved at: $Y=cX$, constant $c\geq 0$.
			\end{itemize}

			\item[$\cdot$] (\textbf{Minkowski ineq.}) Another name for the triangular built-in property of vector space (as $\mathcal{L}_p$ space).

			\item[$\cdot$] (\textbf{Cauchy-Schwartz ineq.}) If $X,Y \in \mathcal{L}^2$, then $XY \in \mathcal{L}^1$. And $\mathbb{E}[|XY|]\leq \|X\|_2 \|Y\|_2$. Equal sign achieved at $Y=cX$.

			\item[$\cdot$] (\textbf{Holder's ineq.}) For $1<p,q<\infty$, and $1/p+1/q=1$, $X\in \mathcal{L}^p$, $Y\in \mathcal{L}^q$; \textit{then} $XY\in \mathcal{L}^1$, and $\mathbb{E}[|XY|]\leq \|X\|_p \|Y\|_q$. This a generalized version of Cauchy-Schwartz.

			\item[$\cdot$] \textbf{Monotonicity} of $\|\cdot\|_p$. If $1\leq p<q<\infty$, $X\in \mathcal{L}^q$; \textit{then} $X\in \mathcal{L}^p$. Moreover $\|X\|_p\leq \|X\|_q$. Equal sign is achieved at $X=c$ constant $a.s.$

			\item[$\cdot$] $\mathcal{L}^p$ is \textit{Banach Space} i.e. $\mathcal{L}^p$ is complete under metric $d(X,Y)=\|X-Y\|_p$.\newline
			In particular, $\mathcal{L}^2$ is \textit{Hilbert Space}: $\forall X,Y \in \mathcal{L}^2$, inner product:
			\begin{equation}
				\langle X,Y\rangle_2 = \int_{\Omega} XY d \mathbb{P}
			\end{equation}

			\item[\textit{Def}] \textbf{Variance}: Define the second moment of quantity $X- \mathbb{E}[X]$ (centered $X$): $\mathrm{Var}[X]:=\mathbb{E}[(X- \mathbb{E}[X])^2]=\mathbb{E}[X^2]-\mathbb{E}^2[X]\geq 0$ by monotonicity: $(\mathbb{E}[X^2])^{\frac{1}{2}}\geq \mathbb{E}[X]$.
		\end{itemize}
		\item[\textit{Rm.}] Monotonicity of $\|\cdot\|_p$ can be proved by Holder's ineq. taking $Y=1$, we do need the prob space where $\mathbb{P}(\Omega)=1$.
\end{itemize}

%------------------------------------------------------------------------
\subsection{Markov's Ineq.}
\textit{Non-negative valued} mapping $g: \mathbb{R}\mapsto [0,+\infty]$ is \textit{non-decreasing} Borel function ($g\in m \mathscr{B}$). \textit{Then} for all constant $c\in \mathbb{R}$:
\begin{equation}
	\mathbb{E}[g(X)]\geq \mathbb{E}[g(X);X\geq c]=\int_{\{X\geq c\}}g(X)d \mathbb{P}\geq g(c)\mathbb{P}(X\geq c)
\end{equation}
Rearrange this ineq, we estimate the upper bound of probability $\mathbb{P}(X\geq c)$ by $\mathbb{E}$ and some pre-determined function evaluated at this constant $c$, i.e.
\begin{equation}
	\mathbb{P}(X\geq c)\leq \frac{\mathbb{E}[g(X);X\geq c]}{g(c)}\leq \frac{\mathbb{E}[g(X)]}{g(c)}
\end{equation}
gives \textbf{Markov's Ineq.}

\begin{itemize}

	\item[\textit{Rm.}] This upperbound is meaningful only if at least $g(X)\in \mathcal{L}^1$. Also, the second $\leq$ uses the fact that $g$ is non-negative valued.

	\item[\textit{EX.1}] $g(X)=X$ identity. $X\in \mathcal{L}^1$, $X\geq 0$ (non-negative) \textit{then}:
	\begin{equation}
		\mathbb{P}(X\geq c)\leq \frac{\mathbb{E}[X]}{c}
	\end{equation}

	\item[\textit{EX.2}] Take $g(X)=|X|^p \cdot \mathbbm{1}_{(0,+\infty)}$. If $X\in \mathcal{L}^p$, \textit{then}:
	\begin{equation}
		\mathbb{P}(X\geq c)= \mathbb{P}(|X|^p\geq c^p)\leq \frac{\mathbb{E}[|X|^p]}{c^p}
	\end{equation}

	\item[\textit{EX.3}] Take $g(X)=e^{a|X|} \cdot \mathbbm{1}_{(0,+\infty)}$ for some $a$. If $e^{a|X|}\in \mathcal{L}^1$, \textit{then}:
	\begin{equation}
		\mathbb{P}(X\geq c)= \mathbb{P}(e^{a|X|}\geq e^{ac})\leq \frac{\mathbb{E}[e^{a|X|}]}{e^{ac}}
	\end{equation}

	\item[\textit{Prop.}] $X_n \xrightarrow{\mathcal{L}^p} X$ $\Rightarrow$ $X_n \xrightarrow{i.p} X$.
	\begin{itemize}
		\item[\textit{Proof.}] Use Markov's ineq. $\forall \epsilon>0$
		\begin{equation}
			\mathbb{P}(|X_n-X|\geq \epsilon)=\mathbb{P}(|X_n-X|^p\geq \epsilon^p)\leq \frac{\mathbb{E}[|X_n-X|^p]}{\epsilon^p}
		\end{equation}
		$X_n \xrightarrow{\mathcal{L}^p} X \Rightarrow \mathbb{E}[|X_n-X|^p] \xrightarrow{n\to \infty}0=RHS$. $\blacksquare$
	\end{itemize}

	\item[\textit{Rm.}] 
		\begin{itemize}
			\item[$\cdot$] $X_n \xrightarrow{\mathcal{L}^p} X$ \textbf{Does not imply} $X_n \xrightarrow{a.s.} X$.
			\item[$\cdot$] $X_n \xrightarrow{a.s.} X$ \textbf{Does not imply} $X_n \xrightarrow{\mathcal{L}^p} X$ either. DOM, SHEFFE supports this arrow because they imposes extra condtions.
			\item[$\cdot$] $X_n \xrightarrow{i.p.} X$ \textbf{Does not imply} $X_n \xrightarrow{\mathcal{L}^p} X$.
			\item[$\cdot$] However $X_n \xrightarrow{i.p.} X$ plus some extra conditions can lead to $X_n \xrightarrow{\mathcal{L}^p} X$. Conditions can be: \textbf{DOM}, \textbf{SCHEFFE} (note that i.p. and a.s. are equivalent hypothesis for these two in $(\Omega, \mathcal{F}, \mathbb{P})$), or \textbf{Unifrom Integrable}.
		\end{itemize}
\end{itemize}


%------------------------------------------------------------------------
\subsection{Uniform Integrablility}
\begin{itemize}
	\item[\textit{Prop.}] (\textit{Motivation for Unif.Integrability}) 
	$$X\in \mathcal{L}^1\iff\lim\limits_{M\rightarrow\infty}\mathbb{E}[|X|;|X|>M]=0$$
	\begin{itemize}
		\item[\textit{Proof.}] $\Leftarrow$: Let $C:=\sup\limits_{M>1}\mathbb{E}[|X|;|X|\geq M]$. By hypothesis, this is bounded, i.e. $C<\infty$. And
		\begin{equation}
			\begin{split}
				\mathbb{E}[|X|] &= \mathbb{E}[|X|;|X|>M]+\mathbb{E}[X;|X|\leq M]\\
				& \leq M+C < \infty
			\end{split}
		\end{equation}
		$\Rightarrow$: Consider $X_M:=|X|\cdot \mathbbm{1}_{\{|X|\leq M\}}\nearrow |X|$. Clearly $X_1\in \mathcal{L}^1$. By \textit{(MON)}: $\mathbb{E}[|X|;|X|\leq M]=\mathbb{E}[|X_M|]\nearrow \mathbb{E}[|X|]<\infty$.
		\begin{equation}
			\begin{split}
				\lim\limits_{M\rightarrow\infty}\mathbb{E}[|X|;|X|>M] &=
				\lim\limits_{M\rightarrow\infty}\mathbb{E}[|X|]-\mathbb{E}[|X|;|X|\leq M] \\
				&= 0~~~\blacksquare
			\end{split}
		\end{equation}
	\end{itemize}

	\item[\textit{Def.}] \textbf{Unifrom Integrable}: Sequence of RV $\{X_n\}$ is \textit{U.I.} if 
	\begin{equation}
		\lim\limits_{M\rightarrow\infty}\sup\limits_{n\geq 1}\mathbb{E}[|X_n|;|X_n|>M]=0
	\end{equation}

	\item[\textit{Rm.}] U.I. says that for all $\epsilon>0$, exists $M$ large, such that $\mathbb{E}[|X_n|;|X_n|>M]<\epsilon$ \textbf{uniformly} for all $n\geq 1$.

	\item[\textit{Prop.}] (\textit{Strength of U.I. hypothesis}) 
	\begin{itemize}
		\item[$\cdot$] ${X_n}$ U.I. $\Rightarrow$ $\{|X_n|\}$ is unifromly bounded in $\mathcal{L}^1$. 
		\item[$\cdot$] $\{|X_n|\}$ is unifromly bounded in $\mathcal{L}^p$ for all $p\geq 1$ $\Rightarrow$ ${X_n}$ U.I.

		\item[\textit{Rm.}] Say $\{|X_n|\}$ is \textit{unifromly bounded} in $\mathcal{L}^p$ if: $\forall n\geq 1$, $\exists M<\infty$ is \textit{irrelevant} to $n$; such that $\mathbb{E}[|X_n|^p]<M$. \textit{OR} just: 
		\begin{equation}
			\sup\limits_{n}\mathbb{E}[|X_n|^p]<M
		\end{equation}

		\item[\textit{Proof.}] (1): By hypothesis, $\exists M$ large, $\sup\limits_{n}\mathbb{E}[|X_n|;|X_n|>M]<\epsilon$.
		\begin{equation}
			\begin{split}
				\mathbb{E}[|X_n|]&=\mathbb{E}[|X_n|;|X_n|\leq M]+\mathbb{E}[|X_n|;|X_n|>M]\\
				&\leq M+\sup\limits_{n}\mathbb{E}[|X_n|;|X_n|>M].~~\blacksquare
			\end{split}
		\end{equation}
		(2): By hypothesis, $\sup\limits_{n}\mathbb{E}\left[|X_n|^p\right]<C<\infty$.
		\begin{equation}
			\begin{split}
				\mathbb{E}[|X_n|;|X_n|>M]&\leq \mathbb{E}\left[\frac{|X_n|^{p-1}}{M^{p-1}}\cdot |X_n|; |X_n|>M\right]\\
				&= \frac{1}{M^{p-1}} \mathbb{E}\left[|X_n|;|X_n|^p>M\right]\\
				&\leq \frac{\sup\limits_{n}\mathbb{E}\left[|X_n|^p\right]}{M^{p-1}}\leq \frac{C}{M^{p-1}}\xrightarrow{M\to \infty}0.~~\blacksquare
			\end{split}
		\end{equation}
	\end{itemize}

	\item[\textit{Thm.}] (\textit{the \textbf{Exact Gap} between i.p. and $\mathcal{L}^1$ convergence})
	\begin{equation}
		X_n \xrightarrow{\mathcal{L}^1} X \iff X_n \xrightarrow{i.p} X\text{ and }\{X_n\}\text{ is U.I.} 
	\end{equation}

	\begin{itemize}
		\item[\textit{Proof.}]
	\end{itemize}

\end{itemize}

%------------------------------------------------------------------------
\subsection{Jensen's Ineq.}
\begin{itemize}
	\item[\textit{Def.}] \textbf{Convex Mapping}: $\phi: \mathbb{R}\mapsto \mathbb{R}$, if $x,y\in \mathbb{R}$, $p,q\in (0,1)$, $p+q=1$. $\phi$ is a convex function \textit{if}:
	\begin{equation}
		\phi(px+qy)\leq p\phi(x)+q\phi(y)
	\end{equation}

	\item[\textit{Prop.}] \textbf{Support Line}: If $phi$ is convex, then $\forall x \in \mathbb{R}$, $\exists$ a line $l$ crosses $(x, \phi(x))$; $l$ stays entirely below the graph of $\phi$.
\end{itemize}

$X\in \mathcal{L}^1$, $\phi: \mathbb{R}\mapsto \mathbb{R}$ is a convex mapping, $\phi\in \mathcal{L}^1$; \textit{then}:
\begin{equation}
	\phi(\mathbb{E}\left[X\right])\leq \mathbb{E}\left[\phi(X)\right]
\end{equation}
gives \textbf{Jensen's Ineq.}, average inside $\leq$ average outside.

\begin{itemize}
	\item[\textit{Proof.}] Using support line. Exists $l$ passes $(\mathbb{E}\left[X\right], \phi(\mathbb{E}\left[X\right]))$, say $y=ax+b$, that supports $\phi$; i.e. $\forall w\in \Omega$:
	\begin{equation}
		aX(w)+b\leq \phi(X(w))
	\end{equation}
	Take expectation both sides, and notice $\mathbb{E}\left[aX+b\right]=a \mathbb{E}\left[X\right]+b = \phi(\mathbb{E}\left[X\right])$:
	\begin{equation}
		\phi(\mathbb{E}\left[X\right])=\mathbb{E}\left[aX+b\right] \leq \mathbb{E}\left[\phi(X)\right].~~\blacksquare
	\end{equation}

	\item[\textit{Cor.}] \textbf{Popular Convex}: $|X|$, $X^2$, $e^{aX}$, $X^+:=\max\{X,0\}$, $X^-:=\max\{-X,0\}$ are convex, satisfy jensen.
\end{itemize}

%------------------------------------------------------------------------
\subsection{On Prob Density Function}
Recall law of $X$, $\mathcal{L}_X(B):=\mathbb{P}(X\in B)$, $B\in\mathscr{B}(\mathbb{R})$ is a new prob measure on $(\mathbb{R},\mathscr{B}(\mathbb{R}))$. For default setting in this subsection, consider $f:\mathbb{R}\mapsto \mathbb{R}$ is a \textit{Borel mapping}. And RV $X: \Omega\mapsto \mathbb{R}$. 

Also recall distribution function: $F_X(x):=\mathcal{L}_X(-\infty,x]$.

\begin{itemize}
	\item[\textit{Prop.}] (\textit{Transference of Integrability/Integral against $\mathbb{P}$ and $\mathcal{L}_X$ measure}): 
	$$f(X)=f\circ X(w)\in \mathcal{L}^1 (\Omega, \mathcal{F}, \mathbb{P}) \iff f(x) \in \mathcal{L}^1(\mathbb{R}, \mathscr{B}(\mathbb{R}), \mathcal{L}_X)$$

	In particular, if $f\circ X\in \mathcal{L}^1 (\Omega, \mathcal{F}, \mathbb{P})$:
	\begin{equation}
		\int_{\Omega}f(X(w))d \mathbb{P}=\int_{\mathbb{R}}f(x) d\mathcal{L}_X~~~(\#)
	\end{equation}


	\item[\textit{Rm.}] This thing is in fact transferring the relationship on \textit{sets} level, i.e. $\mathcal{L}_X(B)=\mathbb{P}(X\in B)$ to \textit{integral} level.

	
	\item[\textit{Proof.}] Let $f=\mathbbm{1}_{B}$. Define preimage $X^{-1}(B):=\{w\in \Omega: X(w)\in B\} \subseteq \Omega$.
	\begin{equation}
		LHS=\int_{\Omega}\mathbbm{1}_{B}(X(w))d \mathbb{P}=\int_{\Omega}\mathbbm{1}_{X^{-1}(B)}d \mathbb{P}=\mathbb{P}(X^{-1}(B))
	\end{equation}
	\begin{equation}
		RHS=\int_{\mathbb{R}}\mathbbm{1}_{B}(x) d\mathcal{L}_X=\mathcal{L}_X(B)=\mathbb{P}(X\in B)=\mathbb{P}(X^{-1}(B))
	\end{equation}
	$(\#)$ holds for indicators. \newline
	By linearity $\Rightarrow$ $(\#)$ holds for $f\in SF^+$. \newline
	By (MON) $\Rightarrow$ $(\#)$ holds for $f\in [m \mathscr{B}(\mathbb{R})]^+$. \newline
	For general $f=f^+-f^-\in \mathcal{L}^1$, $\int f^{\pm}< \infty$. By linearity, $(\#)$ holds for general $f$. $\blacksquare$

	\item[\textit{Cor.}] (\textit{$\mathbb{E}\left[X\right]$ and $\mathrm{Var}\left[X\right]$}) For $X\in \mathcal{L}^1$, $X\in \mathcal{L}^2$ respectively: 
	\begin{equation}
		\mathbb{E}\left[X\right]=\int_{\mathbb{R}} x d\mathcal{L}_X
	\end{equation}
	\begin{equation}
		\mathrm{Var}\left[X\right]=\int_{\mathbb{R}} (x-E[X])^2 d\mathcal{L}_X
	\end{equation}

	\item[\textit{Cor.}] (\textit{Law as a lower level object}) If $X,Y$ has identical Law $\mathcal{L}$, \textit{then} $\forall f\in m\mathscr{B}$:
	\begin{itemize}
		\item[$\cdot$] $f(X) \in \mathcal{L}^1$ $\Rightarrow$ $f(Y)\in \mathcal{L}^1$
		\item[$\cdot$] If $f(X)\in \mathcal{L}^1$:
		\begin{equation}
			\mathbb{E}\left[X\right]=\mathbb{E}\left[Y\right]=\int_{\mathbb{R}}f(x)d \mathcal{L}
		\end{equation}
	\end{itemize}

	\item[\textit{Notation.}] (\textit{Lebesgue-Stieltjes version of $\mathbb{E}\left[X\right]$ using dist function})
	\begin{equation}
		\mathbb{E}\left[X\right]=\int_{\mathbb{R}}f(x)d \mathcal{L}_X=\int_{\mathbb{R}}f(x)d F_X
	\end{equation}

	\item[\textit{Def.}] \textbf{Probability Density Function}: RV $X$ has p.d.f $f_X$, if
	\begin{itemize}
		\item[$\cdot$] $f_X: \mathbb{R}\mapsto [0,+\infty]$ is measurable.
		\item[$\cdot$] The \textit{Radon-Nikodyn derivative} of measure $\mathcal{L}_X$ with respect to lebesgue measure $\mu$ exists. I.e. $\mathcal{L}_X$ is \textit{absolutely continous} wrt $\mu$.
	\end{itemize}
	Use $dx$ as abbr of $d\mu_{leb}$, for $f_X$, if exists, we have: 
	\begin{equation}
		f_X = \frac{d \mathcal{L}_X}{dx}
	\end{equation}

	\item[\textit{Prop.}] (\textit{Transference of Integrability/Integral against $\mathbb{P}$ and lebesgue measure via p.d.f}): If $f_X$ exists, and $h: \mathbb{R}\mapsto \mathbb{R}$ is Borel function, we have:
	$$h(X)=h\circ X(w)\in \mathcal{L}^1 (\Omega, \mathcal{F}, \mathbb{P}) \iff hf_X \in \mathcal{L}^1(\mathbb{R}, \mathscr{B}(\mathbb{R}), \mu_{leb})$$

	In particular, if $h\circ X\in \mathcal{L}^1 (\Omega, \mathcal{F}, \mathbb{P})$:
	\begin{equation}
		\int_{\Omega}h(X(w))d \mathbb{P}=\int_{\mathbb{R}}h(x)f_X(x)dx
	\end{equation}

	\item[\textit{Rm.}] \textbf{Existance of p.d.f}
	\begin{itemize}
		\item[$\cdot$] $f_X$ exists $\Rightarrow$ $F_X$ is continous $everywhere$.
			\begin{itemize}
				\item[] Assume otherwise with discontinuity $\{x_0\}$, $F(x_0^+)-F(x_0^-)>0$. Then $\mathcal{L}_X(\{x_0\})>0$, not absolutely cont wrt $\mu_{leb}$.
			\end{itemize}

		\item[$\cdot$] $f_X$ exists $\Rightarrow$ $F_X$ is differentiable $a.e.$
		\begin{itemize}
			\item[] $F_X(y)=\int_{-\infty}^y f_X(x)dx$
			\item[] $F'_X(y)=f_X(y)~~a.e.$
		\end{itemize}

		\item[$\cdot$] $F_X$ is differentiable $a.e.$ \textbf{Does not imply} $f_X$ exists.
		\begin{itemize}
			\item[] Counter Example: $\mathcal{L}_X=\delta_0$ is Dirac Delta function. $\mathcal{L}_X$ is not absolutely cont wrt $\mu_{leb}$.
		\end{itemize}

		\item[$\cdot$] $F_X$ is differentiable $everywhere$ $\Rightarrow$ $f_X$ exists.
		\begin{itemize}
			\item[] $F'_X(y)=f_X(y)~~a.e.$
		\end{itemize}

		\item[$\cdot$] $F_X$ is continuous $everywhere$ \textbf{Does not imply} $f_X$ exists.
		\begin{itemize}
			\item[] Counter Example: $\mathcal{L}_X$ is Cantor function (fractal structured), $\mathcal{L}_X$ is not absolutely cont wrt $\mu_{leb}$.
		\end{itemize}
	\end{itemize}
\end{itemize}

%------------------------------------------------------------------------
%////////////////////////////////////////////////////////////////////////
%////////////////////////////////////////////////////////////////////////
%------------------------------------------------------------------------
\chapter{Law of Large Numbers}

%------------------------------------------------------------------------
\section{Terminology}
Given process $\{X_n\}$, define partial sum $S_n:= \sum_1^n X_j$. The \textbf{Strong/Weak Law of Large Number} is said to hold for $\{X_n\}$ in following two cases,

\begin{itemize}
	\item[$\cdot$] In \textit{Classical Setting}, say WLLN holds if 
	\begin{equation}
	 	\frac{S_n- \mathbb{E}\left[S_n\right]}{n} \xrightarrow{i.p} 0
	\end{equation} 
	Say SLLN holds if
	\begin{equation}
		\frac{S_n- \mathbb{E}\left[S_n\right]}{n} \xrightarrow{a.s.} 0
	\end{equation}

	\item[$\cdot$] In \textit{General Setting} consider $\{a_n\}\in \mathbb{R}$, $\{b_n\}>0$, $b_n \nearrow \infty$. WLLN for $\{X_n\}$ normalized by $a_n, b_n$, if
	\begin{equation}
		\frac{S_n- a_n}{b_n} \xrightarrow{i.p} 0
	\end{equation}
	SLLN if
	\begin{equation}
		\frac{S_n- a_n}{b_n} \xrightarrow{a.s} 0
	\end{equation}
\end{itemize}

We study the conditions under which WLLN and SLLN can hold. There are two types of them:
\begin{itemize}
	\item[$\cdot$] Estimates/Controls on \textbf{Moments} (i.e. \textbf{Integrability})
	\item[$\cdot$] Estimates/Controls on \textbf{Distributions}.
\end{itemize}

%////////////////////////////////////////////////////////////////////////
\section{Chebyshev (WLLN1)}
\begin{itemize}
	\item[\textit{Thm.}] (\textbf{Chebyshev}) $\{X_n\}$ is a seq of RVs, satisfying
	\begin{itemize}
		\item[$\cdot$] (\textit{Dist}) $\{X_n\}$ are uncorrelated, i.e. $\mathrm{Cov}\left[X_i, X_j\right]=0$ $\forall i\ne j$.
		\item[$\cdot$] (\textit{Moments}) $\{X_n\}$ is bounded by $\mathcal{L}^2$, i.e. $\sup\limits_{n}\mathbb{E}\left[X^2_n\right]< \infty$.
	\end{itemize}
	Then WLLN holds for $\{X_n\}$.
\end{itemize}

\begin{itemize}
	\item[\textit{Proof.}] WLOG assume $\mathbb{E}\left[X_n\right]=0$. \newline
	Otherwise we can always take $Z_n = X_n - \mathbb{E}\left[X_n\right]$ be centerred $X_n$, which has zero means. Under this, $\mathbb{E}\left[X_iX_j\right]=\mathbb{E}\left[X_i\right]\mathbb{E}\left[X_j\right]=0$. \newline
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[S_n^2\right]&=\sum_{j=1}^n \mathbb{E}\left[X_j^2\right]+\sum_{1\leq i\ne j\leq n} \mathbb{E}\left[X_iX_j\right]\\
			&\leq n\cdot \sup\limits_{n} \mathbb{E}\left[X^2_n\right]
		\end{split}
	\end{equation}
	For all $\epsilon >0$, using Markov's ineq,
	\begin{equation}
		\mathbb{P}\left(\left|\frac{S_n}{n}\right|\geq \epsilon\right)\leq \frac{\mathbb{E}\left[S_n^2\right]}{n^2\epsilon^2}\leq \frac{n \sup\limits_{n} \mathbb{E}\left[X^2_n\right]}{n^2 \epsilon^2} \xrightarrow{n\to \infty}0
	\end{equation}
	So WLLN ($\xrightarrow{i.p}$) holds. $\blacksquare$ 

	\item[\textit{Rm.}] \textbf{Chebyshev's ineq}: If $X\in \mathcal{L}^2$, then 
	\begin{equation}
		\mathbb{P}\left(|X- \mathbb{E}\left[X\right]|>c\right)\leq \frac{\mathrm{Var}\left[X\right]}{c^2}
	\end{equation}
	Says exactly same thing as Markov's. 
\end{itemize}


%////////////////////////////////////////////////////////////////////////
\section{Rajchmah (SLLN1)}
\begin{itemize}
	\item[\textit{Thm.}] (\textbf{Rajchmah}) Same hypothesis,
	\begin{itemize}
		\item[$\cdot$] (\textit{Dist}) $\{X_n\}$ are uncorrelated, i.e. $\mathrm{Cov}\left[X_i, X_j\right]=0$ $\forall i\ne j$.
		\item[$\cdot$] (\textit{Moments}) $\{X_n\}$ is bounded by $\mathcal{L}^2$, i.e. $\sup\limits_{n}\mathbb{E}\left[X^2_n\right]< \infty$.
	\end{itemize}
	In fact we have SLLN.
\end{itemize}

\begin{itemize}
	\item[\textit{Proof.}] WLOG assume $\mathbb{E}\left[X_n\right]=0$. By proof of SLLN, we already have:
	\begin{equation}
		\mathbb{P}\left(\left|\frac{S_n}{n}\right|\geq \epsilon\right)\leq \frac{M}{n \epsilon^2}
	\end{equation}
	Where $M=\sup\limits_{n} \mathbb{E}\left[X^2_n\right]$. But the whole thing (order $\frac{1}{n}$) is not summable. Consider subsequence $\{X_{n^2}\} \subseteq \{X_n\}$,

	\begin{equation}
		\mathbb{P}\left(\left|\frac{S_{n^2}}{n^2}\right|\geq \epsilon\right)\leq \frac{\mathbb{E}\left[S^2_{n^2}\right]}{n^4 \epsilon^2}\leq\frac{n^2M}{n^4\epsilon^2}=\frac{M}{n^2\epsilon^2}
	\end{equation}
	is summable, i.e. for all $\epsilon>0$,
	\begin{equation}
		\sum_{n\geq1} \mathbb{P}\left(\left|\frac{S_{n^2}}{n^2}\right|\geq \epsilon\right) < \infty
	\end{equation}
	By (BC1): $\mathbb{P}\left(\left|\frac{S_{n^2}}{n^2}\right|\geq \epsilon~~i.o.\right)=0$, which is $\iff$ $\left|\frac{S_{n^2}}{n^2}\right| \xrightarrow{a.s.} 0$. Holds for subsequence $n^2$. \newline
	Then define
	\begin{equation}
	 	D_n:=\max\limits_{n^2 \leq k < (n+1)^2} |S_k - S_{n^2}|
	\end{equation} 
	which somehow captures the worst deviation from $n^2$ subsequence.
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[D_n^2\right]&\leq \sum_{k=n^2+1}^{(n+1)^2-1} \mathbb{E}\left[(S_k-S_{n^2})^2\right]\\
			&=\sum_{k=n^2+1}^{(n+1)^2-1} \sum_{l=n^2+1}^{k} \mathbb{E}\left[X_l^2\right]\\
			&\leq 2n \cdot 2n \cdot M = \Theta(n^2)
		\end{split}
	\end{equation}
	Where $M=\sup\limits_{n} \mathbb{E}\left[X^2_n\right]$, Second equal sign follows that $\{X_n\}$ are uncorrelated. Final leq is just counting the terms. We now has the idea to estimate $D_n$ by its order $n^2$. For all $\epsilon>0$, markov:
	\begin{equation}
		\mathbb{P}\left(\frac{|D_n|}{n^2}>\epsilon\right)\leq \frac{\mathbb{E}\left[D_n^2\right]}{n^4 \epsilon^2}\leq \frac{4n^2M}{n^4 \epsilon}=\frac{4M}{n^2\epsilon^2}
	\end{equation}
	Which is summable ($\frac{1}{n^2}$).
	\begin{equation}
		\sum_{n\geq 1} \mathbb{P}\left(\frac{|D_n|}{n^2}>\epsilon\right)<\infty
	\end{equation}
	BC1: $\mathbb{P}\left(\frac{|D_n|}{n^2}>\epsilon~~i.o.\right)=0$ $\Rightarrow$ $\frac{|D_n|}{n^2} \xrightarrow{a.s.} 0$. \newline
	For every $w\in \Omega$ such that both $\frac{|D_n|}{n^2} \to 0$ and $\left|\frac{S_{n^2}}{n^2}\right| \to 0$ occurs\footnote{Since these two are a.s. convergence, $w$ is in fact also a.s.}, for every $k\geq 1$, $\exists!~n(k)$ such that $n^2(k)\leq k < (n(k)+1)^2$, and
	\begin{equation}
		\begin{split}
			\frac{|S_k|}{k}&\leq\frac{|S_k-S_{n^2(k)}|+|S_{n^2(k)}|}{n^2(k)}\\
			&\leq \frac{|D_n|}{n^2(k)} + \frac{|S_{n^2(k)}|}{n^2(k)}\xrightarrow{k\to \infty}0\\
		\end{split}
	\end{equation}
	Which holds for $a.e.$. So $\frac{|S_k|}{k}\xrightarrow{a.s.}0$. $\blacksquare$


	\item[\textit{Rm.}] (\textbf{Cantelli})
	\begin{itemize}
		\item[$\cdot$] (\textit{Dist}) $\{X_n\}$ are indep.
		\item[$\cdot$] (\textit{Moments}) $\{X_n\}$ is bounded by $\mathcal{L}^4$.
	\end{itemize}
	Supports SLLN, much weaker then the one above.
\end{itemize}



%////////////////////////////////////////////////////////////////////////
\section{Khintchine (WLLN2) and Kolmogorov-Feller (WLLN3)}

%------------------------------------------------------------------------
\subsection{Equivalence of Seqs}
\begin{itemize}
	\item[\textit{Def.}] \textbf{Equivalence}: Two sequence $\{X_n\}, \{Y_n\}$ are called equivalent, if
	\begin{equation}
		\sum_{n\geq 1}\mathbb{P}\left(X_n\ne Y_n\right)<\infty
	\end{equation}

	\item[\textit{Prop.}] If $X_n, Y_n$ are equivalent, then $\sum_{n\geq 1}(X_n-Y_n)$ converges almost everywhere. And $\forall b_n >0$, $b_n \nearrow \infty$,
	\begin{equation}
		\frac{1}{b_n}\sum_{k=1}^n(X_k - Y_k) \xrightarrow{a.s.} 0
	\end{equation}

	\item[\textit{Proof.}] BC1: $\mathbb{P}\left(X_n\ne Y_n~~i.o.\right)=0$ $\Rightarrow$ $\mathbb{P}\left(X_n=Y_n~~e.v.\right)=1$. Which means $X_n$ and $Y_n$ are eventually the same.\newline
	For almost every $w\in \Omega$, $\exists N(w)>0$, such that $\forall n>N(w)$, $X_n(w)=Y_n(w)$. So clearly
	\begin{equation}
		\sum_{n\geq 1}(X_n(w)-Y_n(w))=\sum_{k=1}^{N(w)}(X_k(w)-Y_k(w))<\infty
	\end{equation}
	i.e. $\sum_{n\geq 1}(X_n(w)-Y_n(w))$ a.s. converges. \newline
	Moreover, for a.e. $w$,
	\begin{equation}
		\frac{1}{b_n}\sum_{n\geq 1}(X_n(w)-Y_n(w))=\frac{1}{b_n}\sum_{k=1}^{N(w)}(X_k(w)-Y_k(w)) \xrightarrow{b_n\to \infty}0
	\end{equation}
	i.e. $\frac{1}{b_n}\sum_{n\geq 1}(X_n-Y_n)\xrightarrow{a.s.}0$. $\blacksquare$

\end{itemize}

%------------------------------------------------------------------------
\subsection{Big O and Small o Notations}
\begin{itemize}
	\item[\textit{Def.}] \textbf{Big O and Small o Notations}: Assume ${a_n}\subseteq \mathbb{R}$, $\{b_n\} \subseteq \mathbb{R}^+$. $b_n \nearrow +\infty$.
	\begin{itemize}
		\item[$\cdot$] We write $a_n = O(b_n)$ $\iff$ $\exists c>0$, $\exists N$ large, such that $\forall n>N$:
		\begin{equation}
			\frac{1}{c}b_n \leq a_n \leq cb_n
		\end{equation}

		\item[$\cdot$] We write $a_n = o(b_n)$ $\iff$ 
		\begin{equation}
			\lim\limits_{n\rightarrow\infty}\frac{a_n}{b_n}=0
		\end{equation}
	\end{itemize}

	\item[\textit{Lemma}] (\textit{Sum of converge-to-zero seq is o(n)}): If $\lim\limits_{n\rightarrow\infty}a_n=0$, then $\sum_{j=1}^na_i=o(n)$, i.e. 
	\begin{equation}
		\lim\limits_{n\rightarrow\infty} \frac{\sum_{j=1}^n a_j}{n}=0
	\end{equation}

	\item[\textit{Proof.}] Since $a_n$ converges to zero $\Rightarrow$ $\forall \epsilon$, $\exists N_1$, $\forall n>N_1$, $|a_n|<\frac{\epsilon}{2}$.
	\begin{equation}
		\left|\frac{1}{n}\sum_{j=1}^n a_j|\right|\leq \frac{1}{n}\sum_{j=1}^{N_1}|a_j| + \frac{1}{n} \sum_{j=N_1+1}^n |a_j|
	\end{equation}
	Clearly the second term $<\frac{\epsilon}{2}$ and the first term converges to zero, i.e. $\exists N_2>0$, $\forall n>N_2$: $\frac{1}{n}\sum_{j=1}^{N_1}|a_j|<\frac{\epsilon}{2}$. Take $N=\max\{N_1, N_2\}$, the whole thing $<\epsilon$. $\blacksquare$

\end{itemize}

%------------------------------------------------------------------------
\subsection{Khintchine's WLLN}
From now we are using the general sense of LLNs, specified in the first section.
\begin{itemize}
	\item[\textit{Thm.}] (\textbf{Khintchine}) $\{X_n\}$ be a seq of RVs satisfying
	\begin{itemize}
		\item[$\cdot$] (\textit{Dist}) $\{X_n\}$ are \textbf{pairwise indep.}, and \textbf{identically distributed}.
		\item[$\cdot$] (\textit{Moments}) $m:=\mathbb{E}\left[X_n\right]<\infty$. ($\mathcal{L}^1$).
	\end{itemize}
	Then,
	\begin{equation}
		\frac{S_n - nm}{n} \xrightarrow{i.p} 0;~~\text{i.e.}~~\frac{S_n - \mathbb{E}\left[S_n\right]}{n} \xrightarrow{i.p} 0;~~\text{i.e.}~~\frac{S_n}{n}\xrightarrow{i.p}m=\mathbb{E}\left[X_n\right]
	\end{equation}

	\item[\textit{Proof.}] Consider truncated sequence $\{Y_n\}$,
	\begin{equation*}
		Y_n = \begin{cases}
		X_n &\text{if $|X_n|\leq n$,}\\
		0 &\text{otherwise.}
		\end{cases}
	\end{equation*}
	We hope that $X_n$ and $Y_n$ are \textit{equivalent}. By definiton,
	\begin{equation}
		\sum_{n\geq 1}\mathbb{P}\left(X_n\ne Y_n\right)=\sum_{n\geq 1}\mathbb{P}\left(|X_n|>n\right)=\sum_{n\geq 1}\mathbb{P}\left(|X_1|>n\right)
	\end{equation}
	\begin{equation}
		\begin{split}
			\infty>\mathbb{E}\left[|X_1|\right]&=\int_0^{\infty}\mathbb{P}\left(|X_1|>t\right)dt\\
			&=\sum_{n\geq 1}\int_{n-1}^n \mathbb{P}\left(|X_1|>t\right)dt\geq \sum_{n\geq 1}\mathbb{P}\left(|X_1|>n\right)
		\end{split}
	\end{equation}
	Therefore they are indeed equivalent. And clearly $\{Y_n\}$ are also pairwise indep.\newline
	Define $T_n:=\sum_{j=1}^n Y_j$
	\begin{equation}
		\mathrm{Var}\left[T_n\right]=\sum_{j=1}^n \mathrm{Var}\left[Y_j\right]\leq \sum_{j=1}^n \mathbb{E}\left[Y_j^2\right]=\sum_{j=1}^n \mathbb{E}\left[X_j^2; |X_j|\leq j\right]
	\end{equation}
	Note that we can easily obtain a \textit{crude} estimate of $\mathrm{Var}\left[T_n\right]$. $RHS \leq \sum_{1}^n j\cdot \mathbb{E}\left[|X_j|\right] \leq \mathbb{E}\left[|X_1|\right]\sum_1^nj = O(n^2)$, which is not sufficient. To show any WLLN, we \textbf{always} somehow need $\mathrm{Var}\left[S_n\right]=o(n^2)$.\newline
	Finer estimate is made by following. Consider $l_n$, such that, $0<l_n \nearrow \infty$, $l_n<n$, and $l_n=o(n)$. For example, $l_n = \lfloor\sqrt{n}\rfloor$. Then,
	\begin{equation}
		\begin{split}
			\mathrm{Var}\left[T_n\right]&\leq\left(\sum_{j=1}^{l_n}+\sum_{j=l_n+1}^n\right)\mathbb{E}\left[X_j^2; |X_j|\leq j\right]\\
			&\leq \mathbb{E}\left[|X_1|\right]\sum_{j=1}^{l_n}j + \sum_{j=l_n+1}^n \left(\mathbb{E}\left[X_j^2; |X_j|\leq l_n\right]+\mathbb{E}\left[X_j^2; l_n< |X_j|\leq j\right]\right)\\
			&\leq \mathbb{E}\left[|X_1|\right]O(l_n^2)+\sum_{j=l_n+1}^n l_n\cdot \mathbb{E}\left[|X_1|\right]+\sum_{j=l_n+1}j\cdot \mathbb{E}\left[|X_1|;|X_1|>l_n\right]\\
			&= \mathbb{E}\left[|X_1|\right]\cdot O(l_n^2)+\mathbb{E}\left[|X_1|\right]\cdot O(nl_n)+\mathbb{E}\left[|X_1|;|X_1|>l_n\right]\cdot O(n^2)\\
			&=o(n^2)
		\end{split}
	\end{equation}
	For the last equal sign to $o(n^2)$, notice that the first two terms are clearly $o(n^2)$, and since $X_1\in \mathcal{L}^1$ $\Rightarrow$ $\mathbb{E}\left[|X_1|; |X_1|>l_n\right]\xrightarrow{l_n \to \infty}0$, so the third term is also $o(n^2)$ (actually zero at infinity). We have
	\begin{equation}
		\lim\limits_{n\rightarrow\infty}\frac{\mathrm{Var}\left[T_n\right]}{n^2}=0
	\end{equation}
	Apply (\textbf{Chebyshev}), for all $\epsilon>0$,
	\begin{equation}
		\mathbb{P}\left(\frac{|T_n - \mathbb{E}\left[T_n\right]|}{n}>\epsilon\right)\leq \frac{\mathrm{Var}\left[T_n\right]}{n^2 \epsilon^2} \xrightarrow{n\to \infty}0
	\end{equation}
	Implies
	\begin{equation}
		\frac{T_n - \mathbb{E}\left[T_n\right]}{n} \xrightarrow{i.p} 0
	\end{equation}
	Now we are going from $T_n$ to $S_n$.
	\begin{equation}
		\begin{split}
			\frac{|S_n- \mathbb{E}\left[S_n\right]|}{n}&\leq \frac{|S_n - T_n|}{n}+\frac{|T_n - \mathbb{E}\left[T_n\right]|}{n} + \frac{|\mathbb{E}\left[T_n\right]- \mathbb{E}\left[S_n\right]|}{n}\\
			&= Q_1 + Q_2 + Q_3
		\end{split}
	\end{equation}
	\begin{itemize}
		\item[$\cdot$] For $Q_1$, since $\{X_n\}, \{Y_n\}$ are equivalent, use the property of equivalent sequence, $Q_1 \xrightarrow{a.s.}0$.
		\item[$\cdot$] For $Q_2$, we already know $Q_2 \xrightarrow{i.p} 0$.
		\item[$\cdot$] For $Q_3$,
		\begin{equation}
			Q_3 \leq \frac{1}{n}\sum_{j=1}^n \mathbb{E}\left[|X_j|;|X_j|>j\right]=\frac{1}{n}\sum_{j=1}^n \mathbb{E}\left[|X_1|; |X_1|>j\right]
		\end{equation}
		By lemma in last subsection, sum of converge-to-zero seq is $o(n)$. Since $\lim\limits_{n\rightarrow\infty}\mathbb{E}\left[|X_1|;|X_1|>n\right]=0$, the sum is $o(n)$, implies $Q_3\to 0$ (pointwise).
	\end{itemize}
	Pick the weakest convergence of three sub-quantities, $\frac{|S_n- nm|}{n} \xrightarrow{i.p}0$. $\blacksquare$

	\item[\textit{Rm.}] To show any WLLN, we \textbf{always} somehow need $\mathrm{Var}\left[S_n\right]=o(n^2)$.
\end{itemize}

%------------------------------------------------------------------------
\subsection{Kolmogorov-Feller's WLLN}
\begin{itemize}
	\item[\textit{Thm.}] (\textbf{Kolmogorov-Feller}) $\{X_n\}$ are \textbf{pairwise indep.}, some seq of numbers $\{b_n\}$, $0<b_n\nearrow \infty$. $\{X_n\}$ satisfies, 
	\begin{equation}
		\lim\limits_{n\rightarrow\infty}\sum_{j=1}^n \mathbb{P}\left(|X_j|>b_n\right)=0
	\end{equation}
	\begin{equation}
		\lim\limits_{n\rightarrow\infty}\sum_{j=1}^n \mathbb{E}\left[\frac{|X_j|^2}{b_n^2};|X_j|\leq b_n\right]=0
	\end{equation}
	Then, if $a_n$ is defined by $a_n:=\sum_{j=1}^n \mathbb{E}\left[X_j;|X_j|\leq b_n\right]$, we have
	\begin{equation}
		\frac{S_n - a_n}{b_n} \xrightarrow{i.p} 0
	\end{equation}
\end{itemize}


%////////////////////////////////////////////////////////////////////////
\section{Kolmogorov (SLLN2)}

%------------------------------------------------------------------------
\subsection{Kronecker's Lemma}
\begin{itemize}
	\item[\textit{Lemma}] (\textbf{Kronecker}) Two sequences of numbers, $\{x_n\}\subseteq \mathbb{R}$, $\{a_n\} \subseteq \mathbb{R}^+$, $a_n \nearrow \infty$, then
	$$\sum_{n\geq 1}\frac{x_n}{a_n}~~\text{Converges to finite value } \Rightarrow \frac{1}{a_n}\sum_{j=1}^n x_j \xrightarrow{n\to \infty}0$$
	Note the reverse direction ($\Leftarrow$) is \textbf{Not} true.

	\item[\textit{Proof.}] For $1\leq n <\infty$, define
	\begin{equation}
		b_n:=\sum_{j=1}^n \frac{x_j}{a_j}
	\end{equation}
	By hypothesis, $\lim\limits_{n\rightarrow\infty}b_n = b < \infty$. Let $b_0=a_0=0$, clearly by definition $x_n = a_n(b_n-b_{n-1})$, so
	\begin{equation}
		\begin{split}
			\frac{1}{a_n}\sum_{j=1}^n x_j &= \frac{1}{a_n} \sum_{j=1}^n a_j(b_j-b_{j-1})\\
			&=(b_n-b_{n-1})+\frac{1}{a_n}\sum_{j=1}^{n-1}a_jb_j-\frac{1}{a_n}\sum_{j=1}^{n-1}a_jb_{j-1}\\
			&=(b_n-b_{n-1})+\frac{1}{a_n}\sum_{j=1}^{n-1}a_jb_j-\frac{1}{a_n}\sum_{j=0}^{n-2}a_{j+1}b_{j}\\
			&=(b_n-b_{n-1}\frac{a_n}{a_n})+\frac{1}{a_n}\sum_{j=1}^{n-1}(a_j-a_{j+1})b_j+\frac{a_{n-1}b_{n-1}}{a_n}\\
			&=b_n-\frac{1}{a_n}(a_n-a_{n-1})b_{n-1}+\frac{1}{a_n}\sum_{j=1}^{n-1}(a_j-a_{j+1})b_j\\
			&=b_n-\frac{1}{a_n}\sum_{j=1}^{n-1}(a_{j+1}-a_j)b_j~~(\#)
		\end{split}
	\end{equation}
	This bunch of thing,
	\begin{equation}
		\frac{1}{a_n} \sum_{j=1}^n a_j(b_j-b_{j-1}) = b_n-\frac{1}{a_n}\sum_{j=1}^{n-1}(a_{j+1}-a_j)b_j
	\end{equation}
	is actually \textbf{Abel Summation Formula} (discrete version of integration by parts).\newline
	Note the telescoping sum $\frac{1}{a_n}\sum_0^{n-1}(a_{j+1}-a_j)=1$, so $b_n = \frac{1}{a_n}\sum_0^{n-1}b_n(a_{j+1}-a_j)$, therefore
	\begin{equation}
		(\#)=\frac{1}{a_n}\sum_{j=1}^{n-1}(b_n-b_j)(a_{j+1}-a_j)
	\end{equation}
	Since $\lim\limits_{n\rightarrow\infty}b_n = b< \infty$, $\{b_n\}$ is Cauchy-sequence. I.e. $\forall \epsilon>0$, $\exists N$, such that $\forall n, m>N$, $|b_n-b_m|<\epsilon$. Split (\#) into two parts,
	\begin{equation}
		\begin{split}
			(\#)&=\frac{1}{a_n}\sum_{j=1}^{n-1}(b_n-b_j)(a_{j+1}-a_j)\\
			&=\frac{1}{a_n}\left(\sum_{j=1}^{N-1}+\sum_{j=N}^n\right)(b_n-b_j)(a_{j+1}-a_j)\\
			&\leq 2\epsilon
		\end{split}
	\end{equation}
	Where the first part is taken care by $\frac{1}{a_n}\to 0$, second is due to $|b_n-b_j|\to 0$. $\blacksquare$
\end{itemize}

%------------------------------------------------------------------------
\subsection{Kolmogorov's Ineq}
\begin{itemize}
	\item[\textit{Lemma.}] (\textbf{Kolmogorov's Ineq}) Seq $\{X_n\}$. Define $S_n$ as partial sum, $\{X_n\}$ satisfy
	\begin{itemize}
		\item[$\cdot$] $\{X_n\}$ is \textbf{Mutually Indep.} (Pairwise Not sufficient!)
		\item[$\cdot$] $\mathbb{E}\left[X_n\right]=0$, $\mathbb{E}\left[X^2\right]<\infty$ for all $n$.
	\end{itemize}
	Then, $\forall \epsilon>0$, 
	\begin{equation}
		\mathbb{P}\left(\max\limits_{1\leq j\leq n}|S_j|>\epsilon\right)\leq \frac{\mathbb{E}\left[S_n^2\right]}{\epsilon^2}
	\end{equation}

	\item[\textit{Proof.}] Define $A:=\{\max\limits_{1\leq j\leq n}|S_j|>\epsilon\}$ the event inside LHS. \newline
	Define $A_j := \{|S_i|\leq \epsilon, \text{for }i =1,2,...,j-1\}\cap\{|S_j|>\epsilon\}$ the larger one pops up at exactly index $j$.\newline
	$A_i \cap A_j=\emptyset$, for $i\ne j$ ($\{A_j\}$ are disjoint), clearly. \newline
	We have
	\begin{equation}
		A=\bigcup_{j=1}^{n}A_j
	\end{equation}
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[S_n^2\right]&\geq \mathbb{E}\left[S_n^2; A\right]= \sum_{j=1}^n \mathbb{E}\left[S_n^2;A_j\right]\\
			&= \sum_{j=1}^n \mathbb{E}\left[(S_j+(S_n-S_j))^2; A_j\right]\\
			&= \sum_{j=1}^n \mathbb{E}\left[S_j^2; A_j\right]+2\sum_{j=1}^n \mathbb{E}\left[S_j(S_n-S_j);A_j\right]+\sum_{j=1}^n \mathbb{E}\left[(S_n-S_j)^2; A_j\right]\\
			&= \sum_{j=1}^n \mathbb{E}\left[S_j^2; A_j\right]+\sum_{j=1}^n \mathbb{E}\left[(S_n-S_j)^2\right]\\
			&\geq \sum_{j=1}^n \mathbb{E}\left[S_j^2; A_j\right]> \epsilon^2 \sum_{j=1}^n \mathbb{P}\left(A_j\right)=\epsilon^2 \mathbb{P}\left(A\right)
		\end{split}
	\end{equation}
	Two things in this derivation, \newline
	\begin{itemize}
		\item[$\cdot$] The cross term $S_j(S_n-S_j)$ is removed because $S_n-S_j=\sum_{k=j+1}^n X_k$, indepent wrt $X_l$ for any $l\leq j$, thus indep wrt $S_j$. $\mathbb{E}\left[S_j(S_n-S_j)\right]=\mathbb{E}\left[S_j\right]\mathbb{E}\left[S_n-S_j\right]$, and clearly $\mathbb{E}\left[S_n-S_j\right]=0$.
		\item[$\cdot$] The final estimate of expectation by probability. Since $A_j$ contains constraint $|S_j|>\epsilon$, so $\mathbb{E}\left[S^2_j; A_j\right]>\mathbb{E}\left[\epsilon^2; A_j\right]=\epsilon^2 \mathbb{P}\left(A_j\right)$.
	\end{itemize}
	Therefore $\mathbb{P}\left(A\right)\leq \frac{\mathbb{E}\left[S_n^2\right]}{\epsilon^2}$. $\blacksquare$
\end{itemize}

%------------------------------------------------------------------------
\subsection{Kolmogorov's SLLN}
\begin{itemize}
	\item[\textit{Thm.}] (\textbf{Kolmogorov-Prelude}) $\{Y_n\}$ satisfy
	\begin{itemize}
		\item[$\cdot$] $\{Y_n\}$ \textbf{Mutually indep.}
		\item[$\cdot$] $\sum_{n\geq1}\mathrm{Var}\left[Y_n\right]<\infty$, (automatically have $Y_n \in \mathcal{L}^2$).
	\end{itemize}
	Then, $\sum_{n\geq 1}(Y_n - \mathbb{E}\left[Y_n\right])$ converges almost surely.

	\item[\textit{Proof.}] Denote partial sum $S_n$. Fix some $N>0$, consider $\{Y_{N+n}:n\geq 1\}$. Denote tail of summation $T_m := \sum_{j=1}^m Y_{N+j}=S_{N+m}-S_N$.\newline
	Clearly $\{T_m\}$ mutually indep, apply Kolmogorov's ineq to sequence $T_m - \mathbb{E}\left[T_m\right]$,
	\begin{equation}
		\begin{split}
			\mathbb{P}\left(\max\limits_{1\leq j\leq m}|T_j- \mathbb{E}\left[T_j\right]|>\epsilon\right)&\leq \frac{\mathrm{Var}\left[T_m\right]}{\epsilon^2}=\frac{1}{\epsilon^2}\sum_{j=N+1}^{N+m}\mathrm{Var}\left[Y_j\right]
		\end{split}
	\end{equation}
	We are allowed to take $m\to \infty$(?)
	\begin{equation}
		\begin{split}
			\mathbb{P}\left(\sup\limits_{ j\geq 1}|T_j- \mathbb{E}\left[T_j\right]|>\epsilon\right)&=\mathbb{P}\left(\bigcup_{m\geq 1}\{\max\limits_{1\leq j\leq m}|T_j- \mathbb{E}\left[T_j\right]|>\epsilon\}\right)\\
			&=\lim\limits_{m\rightarrow\infty}\mathbb{P}\left(\max\limits_{1\leq j\leq m}|T_j- \mathbb{E}\left[T_j\right]|>\epsilon\right)\\
			&\leq \frac{1}{\epsilon^2}\sum_{j=N+1}^{\infty}\mathrm{Var}\left[Y_j\right]\xrightarrow{N\to \infty}0
		\end{split}
	\end{equation}
	Convergence to 0 when $N\to \infty$ follows the hypothesis that $\sum_{n\geq1}\mathrm{Var}\left[Y_n\right]<\infty$.\newline
	Now we have an important intermediate result,
	\begin{equation}
		\lim\limits_{N\rightarrow\infty}\mathbb{P}\left(\sup\limits_{ j\geq 1}|T_j- \mathbb{E}\left[T_j\right]|>\epsilon\right)=0
	\end{equation}
	i.e. 
	\begin{equation}
		\lim\limits_{N\rightarrow\infty}\mathbb{P}\left(\sup\limits_{ j\geq 1}|(S_{N+j}-S_N)- \mathbb{E}\left[S_{N+j}-S_N\right]|>\epsilon\right)=0
	\end{equation}
	This line says that for all $\epsilon$, we can somehow control the \textbf{maximum oscillation of tail sequence}. We will see that this statement \textit{always} implies convergence a.s. Consider
	\begin{equation}
		\begin{split}
			\{S_n- \mathbb{E}\left[S_n\right]\text{ does not converge in $\mathbb{R}$}\} 
			&\subseteq \{S_n- \mathbb{E}\left[S_n\right] \text{ is not Cauchy}\}\\
			&=\bigcup_{k\geq 1}\bigcap_{N\geq 1}\{\sup\limits_{j\geq N}|(S_j- \mathbb{E}\left[S_j\right])-(S_N- \mathbb{E}\left[S_N\right])|>\frac{1}{k}\}
		\end{split}
	\end{equation}
	We hope that this has zero probability. Fix $k$
	\begin{equation}
		\begin{split}
			&\mathbb{P}\left(\bigcap_{N\geq 1}\{\sup\limits_{j\geq N}|(S_j- \mathbb{E}\left[S_j\right])-(S_N- \mathbb{E}\left[S_N\right])|>\frac{1}{k}\}\right)\\
			&=\mathbb{P}\left(\bigcap_{N\geq 1}\{\sup\limits_{j\geq N}|T_j- \mathbb{E}\left[T_j\right]|>\frac{1}{k}\}\right)\\
			&\leq\lim\limits_{N\rightarrow\infty}\mathbb{P}\left(\sup\limits_{ j\geq 1}|T_j- \mathbb{E}\left[T_j\right]|>\epsilon\right)=0
		\end{split}
	\end{equation}
	Therefore
	\begin{equation}
		\mathbb{P}\left(\{S_n- \mathbb{E}\left[S_n\right]\text{ converges in $\mathbb{R}$}\}\right)=1
	\end{equation}
	i.e. $\sum_{n\geq 1}(Y_n - \mathbb{E}\left[Y_n\right])$ converges almost surely. $\blacksquare$

	\item[\textit{Thm.}] (\textbf{Kolmogorov}) $\{X_n\}$ satisfies
	\begin{itemize}
		\item[$\cdot$] \textbf{Mutually indep}.
		\item[$\cdot$] $\sum_{n\geq 1}\mathrm{Var}\left[X_n\right]/n^2<\infty$
	\end{itemize}
	then, 
	$$\sum_{n\geq 1}\frac{X_n - \mathbb{E}\left[X_n\right]}{n}\text{  Converges almost surely.}$$
	Apply Kronecker's Lemma, we have SLLN:
	$$\frac{1}{n}\sum_{n\geq 1}(X_n - \mathbb{E}\left[X_n\right]) \xrightarrow{a.s.} 0~~\text{i.e.}~~\frac{S_n - \mathbb{E}\left[S_n\right]}{n}\xrightarrow{a.s.}0$$

	\item[\textit{Proof.}] Let $Y_n:=\frac{X_n}{n}$. Check $Y_n$ satisfying hypothesis of prelude thm, then we have desired result by prelude thm. $\blacksquare$
\end{itemize}

%////////////////////////////////////////////////////////////////////////
\section{Kolmogorov' (SLLN3)}
\begin{itemize}
	\item[\textit{Thm.}] (\textbf{Kolmogorov'}) $\{X_n\}$ is i.i.d. sequence. Following two statement holds,
	\begin{equation}
		\mathbb{E}\left[|X_1|\right]<\infty\Rightarrow\frac{S_n - \mathbb{E}\left[S_n\right]}{n} \xrightarrow{a.s.} 0~~\text{i.e.}~~\frac{S_n}{n}\xrightarrow{a.s.}\mathbb{E}\left[X_1\right]~~(\#1)
	\end{equation}
	\begin{equation}
		\mathbb{E}\left[|X_1|\right]=\infty\Rightarrow \limsup\limits_{n\rightarrow\infty}\frac{|S_n|}{n}=\infty~~\text{ a.s.}~~(\#2)
	\end{equation}

	\item[\textit{Proof.}] First do (\#2). Assume $\mathbb{E}\left[|X_1|\right]=\infty$. Fix any $A>0$, $\mathbb{E}\left[|\frac{X_1}{A}|\right]=\infty$.\newline
	Then
	\begin{equation}
		\sum_{n\geq 1}\mathbb{P}\left(\left|\frac{X_1}{A}\right|>n\right)=\infty
	\end{equation}
	(Because, in more general case,
	\begin{equation}
		\begin{split}
			\infty &= \mathbb{E}\left[|X|\right]=\int_0^{\infty}\mathbb{P}\left(|X|>t\right)dt\\
			&=\sum_{n\geq 1}\int_{n-1}^n \mathbb{P}\left(|X|>t\right)dt\leq \sum_{n\geq 1}\mathbb{P}\left(|X|>n-1\right)
		\end{split}
	\end{equation}
	In fact, $\mathbb{E}\left[|X|\right]<\infty \iff \sum_1^{\infty} \mathbb{P}\left(|X|>n\right)<\infty$)\newline
	Apply (\textbf{BC2}), $\mathbb{P}\left(|X_n|>nA~~i.o\right)=1$, i.e.
	\begin{equation}
		\mathbb{P}\left(\frac{|S_n-S_{n-1}|}{n}>A~~i.o\right)=1
	\end{equation}
	Consider,
	\begin{equation}
		\begin{split}
			\{|S_n-S_{n-1}|>nA\}&\subseteq \left\{|S_n|>\frac{n}{2}A\right\}\cup\left\{|S_{n-1}|>\frac{n}{2}A\right\}\\
			&\subseteq \left\{\frac{|S_n|}{n}>\frac{A}{2}\right\}\cup\left\{\frac{|S_{n-1}|}{n-1}>\frac{A}{2}\right\}
		\end{split}
	\end{equation}
	Two parts at RHS says same thing, so actually we have
	\begin{equation}
		\mathbb{P}\left(\frac{|S_n|}{n}>\frac{A}{2}~~i.o\right)=1
	\end{equation}
	This is true for $\forall A>0$. So take intersection over $A$, the statement still holds.
	\begin{equation}
		\begin{split}
			\bigcap_{m\geq1}\left\{\frac{|S_n|}{n}>m~~i.o\right\} &\subseteq \bigcap_{m\geq1}\left\{\limsup\limits_{n\rightarrow\infty}\frac{|S_n|}{n}>m\right\}\\
			&=\left\{\limsup\limits_{n\rightarrow\infty}\frac{|S_n|}{n}=\infty\right\}~~~\blacksquare
		\end{split}
	\end{equation}
	Now show (\#1), assume $\mathbb{E}\left[|X_1|\right]<\infty$, truncate $X_n$,
	\begin{equation*}
		Y_n := \begin{cases}
		X_n &\text{if $|X_n|\leq n$,}\\
		0 &\text{otherwise.}
		\end{cases}
	\end{equation*}
	By same argument in WLLN(Khintchine), we can come to $\sum \mathbb{P}\left(Y_n\ne X_n\right)<\infty$, i.e. $X_n, Y_n$ are equivalent. Clearly $\{Y_n\}$ is also indep. \newline
	We want to refer to (SLLN2), i.e. $\sum\frac{1}{n^2}\mathrm{Var}\left[Y_n\right]<\infty$, consider this quantity
	\begin{equation}
		\begin{split}
			\sum_{n\geq 1} \frac{\mathrm{Var}\left[Y_n\right]}{n^2}&\leq \sum_{n\geq 1}\frac{\mathbb{E}\left[Y_n^2\right]}{n^2}=\sum_{n\geq 1}\frac{\mathbb{E}\left[X_n^2; |X_n|<n\right]}{n^2}\\
			&= \sum_{n\geq 1}\frac{\mathbb{E}\left[X_1^2; |X_1|<n\right]}{n^2}\\
			&= \sum_{n\geq 1}\left(\frac{1}{n^2}\sum_{j=1}^n\mathbb{E}\left[X_1^2; j-1\leq|X_1|\leq j\right]\right)\\
			&= \sum_{j=1}^n\left(\mathbb{E}\left[X_1^2; j-1\leq|X_1|\leq j\right]\sum_{n\geq j}\frac{1}{n^2}\right)\\
			&=\sum_{j=1}^n\mathbb{E}\left[X_1^2; j-1\leq|X_1|\leq j\right]\cdot O\left(\frac{1}{j}\right)\\
			&\leq C\sum_{j=1}^n\frac{1}{j}\cdot j \cdot \mathbb{E}\left[|X_1|; j-1\leq|X_1|\leq j\right]=C \mathbb{E}\left[|X_1|\right]<\infty
		\end{split}
	\end{equation}
	In which we swich the order of summation at the forth equal sign, noticing that $\sum_{n\geq j}1/n^2=O(1/j)$, and apply definition of $O$ notation at the end, $0<C<\infty$ is constant.\newline
	Apply (SLLN2) for $\{Y_n\}$, we have
	\begin{equation}
		\frac{\sum_{j=1}^n|Y_j- \mathbb{E}\left[Y_j\right]|}{n} \xrightarrow{a.s.} 0
	\end{equation}
	Split target quantity in similar fashion as WLLN2:
	\begin{equation}
		\begin{split}
			\frac{|S_n- \mathbb{E}\left[S_n\right]|}{n}
			&\leq\frac{|\sum_1^n X_j-Y_j|}{n}+\frac{|\sum_1^n Y_j- \mathbb{E}\left[Y_j\right]|}{n}+\frac{|\sum_1^n \mathbb{E}\left[Y_j\right]- \mathbb{E}\left[X_j\right]|}{n}\\
			&=Q_1+Q_2+Q_3
		\end{split}
	\end{equation}
	We proved $Q_2 \xrightarrow{a.s.}0$. \newline
	By property of equivalent seqs $Q_1 \xrightarrow{a.s.} 0$.\newline
	\begin{equation}
		Q_3 = \frac{\sum_1^n \mathbb{E}\left[X_j:|X_j|>j\right]}{n}=\frac{1}{n}\sum_1^n \mathbb{E}\left[X_1; |X_1|>j\right]
	\end{equation}
	By lemma, $a_n \to 0$ $\Rightarrow$ $\sum a_n = o(n)$. We have $Q_3 \to 0$ pointwise. Therefore $Q_1+Q_2+Q_3 \xrightarrow{a.s.} 0$ as desired. $\blacksquare$
\end{itemize}

\section{(SLLN4)}
\begin{itemize}
	\item[\textit{Thm.}] (\textbf{SLLN4}) Let $\{X_n: n\geq 1\}$ be sequence of $\mathcal{L}^1$, indep RVs; $S_n$ be partial sum. Let $\phi: \mathbb{R}\to \mathbb{R}$ be positive and continuous even function such that $\frac{\phi(x)}{|x|}$ is non-decreasing in $x$ and $\frac{\phi(x)}{x^2}$ is non-increasing in $x$. Assume for some sequence $\{b_n: n\geq 1\}$ of positive real numbers with $b_n \nearrow \infty$,
	\begin{equation}
		\sum_{n\geq 1}\frac{\mathbb{E}\left[\phi(X_n)\right]}{\phi(b_n)}<\infty
	\end{equation}
	Show that $\sum_{n\geq 1}\frac{X_n - \mathbb{E}\left[X_n\right]}{b_n}$ converges a.s., hence
	\begin{equation}
		\frac{S_n - \mathbb{E}\left[S_n\right]}{b_n} \xrightarrow{a.s.} 0
	\end{equation}

	\item[$Proof$.] See problem 8-4-3.
\end{itemize}

%////////////////////////////////////////////////////////////////////////
\section{Levy's Equivalence Thm}
\begin{itemize}
	\item[\textit{Thm.}] (\textbf{Levy}) $\{X_n\}$ indep. $S_n$ is partial sum, then
	\begin{equation}
		S_n \xrightarrow{i.p} S \iff S_n \xrightarrow{a.s.} S
	\end{equation}
	In fact (won't prove)
	\begin{equation}
		S_n \xrightarrow{dist} S \iff S_n \xrightarrow{a.s.} S
	\end{equation}

	\item[\textit{Rm.}] Intuition is that, in general, it is so \textit{hard} for sum of independent RV to converge that as long as it converges, it converges \textit{in all sense}.

	\item[\textit{Proof.}] Only for the in.prob part. $\Rightarrow$:\newline
	By i.p; $\forall \epsilon>0$, $\exists N$, for all $m,n>N$, 
	\begin{equation}
		\mathbb{P}\left(|S_m-S_n|>\epsilon\right)\leq \mathbb{P}\left(|S_n-S|>\frac{\epsilon}{2}\right) + \mathbb{P}\left(|S_m-S|>\frac{\epsilon}{2}\right)\leq \epsilon
	\end{equation}
	\begin{equation}
		\begin{split}
			\epsilon
			&\geq \mathbb{P}\left(|S_m-S_n|>\epsilon\right)\\
			&\geq \mathbb{P}\left(|S_m-S_n|>\epsilon~~\&~~\max\limits_{n+1\leq k\leq m}|S_k-S_n|>2\epsilon\right)\\
			&= \sum_{k=n+1}^m\mathbb{P}\left(|S_m-S_n|>\epsilon~~\&~~|S_j-S_n|\leq2\epsilon, \forall j=n+1,...,k-1~~\&~~|S_k-S_n|>2\epsilon\right)\\
			&\geq \sum_{k=n+1}^m\mathbb{P}\left(|S_m-S_k|\leq\epsilon~~\&~~|S_j-S_n|\leq2\epsilon, \forall j=n+1,...,k-1~~\&~~|S_k-S_n|>2\epsilon\right)
		\end{split}
	\end{equation}
	Notice that 
	\begin{equation}
		\begin{split}
			&\left\{|S_m-S_k|\leq\epsilon\right\} \in \sigma(X_{k+1}, ... ,X_m)\\
			&\left\{|S_j-S_n|\leq2\epsilon, \forall j=n+1,...,k-1~~\&~~|S_k-S_n|>2\epsilon\right\} \in \sigma(X_{n+1}, ... ,X_k)
		\end{split}
	\end{equation}
	Are independent, so
	\begin{equation}
		\begin{split}
			\epsilon&\geq\sum_{k=n+1}^m\mathbb{P}\left(|S_m-S_k|\leq\epsilon~~\&~~|S_j-S_n|\leq2\epsilon, \forall j=n+1,...,k-1~~\&~~|S_k-S_n|>2\epsilon\right)\\
			&=\sum_{k=n+1}^m\mathbb{P}\left(|S_m-S_k|\leq\epsilon\right)\cdot\mathbb{P}\left(|S_j-S_n|\leq2\epsilon, \forall j=n+1,...,k-1~~\&~~|S_k-S_n|>2\epsilon\right)\\
			&\geq(1-\epsilon)\sum_{k=n+1}^m\mathbb{P}\left(|S_j-S_n|\leq2\epsilon, \forall j=n+1,...,k-1~~\&~~|S_k-S_n|>2\epsilon\right)\\
			&=(1-\epsilon)\cdot\mathbb{P}\left(\max\limits_{n+1\leq k\leq m}|S_k-S_n|>2\epsilon\right)
		\end{split}
	\end{equation}
	we have, $\forall \epsilon>0$, for all $m,n >N$,
	\begin{equation}
		\mathbb{P}\left(\max\limits_{n+1\leq k\leq m}|S_k-S_n|>2\epsilon\right) \leq \frac{\epsilon}{1-\epsilon}
	\end{equation}
	Let $m\to \infty$,
	\begin{equation}
		\mathbb{P}\left(\sup\limits_{k\geq n+1}|S_k-S_n|>2\epsilon\right) \leq \frac{\epsilon}{1-\epsilon}
	\end{equation}
	Therefore
	\begin{equation}
		\lim\limits_{n\rightarrow\infty}\mathbb{P}\left(\sup\limits_{k\geq n+1}|S_k-S_n|>2\epsilon\right)=0
	\end{equation}
	Which implies
	\begin{equation}
		\mathbb{P}\left(\lim\limits_{n\rightarrow\infty}S_n\text{ exists in $\mathbb{R}$ ($<\infty$)}\right)=1~~\text{i.e.}~~S_n \xrightarrow{a.s.} S~~~\blacksquare
	\end{equation}

\end{itemize}

%------------------------------------------------------------------------
%////////////////////////////////////////////////////////////////////////
%////////////////////////////////////////////////////////////////////////
%------------------------------------------------------------------------
\chapter{Product Space}

%////////////////////////////////////////////////////////////////////////
\section{Basic Structure}

\begin{itemize}
	\item[\textit{Def}] \textbf{Product Space}: Let $(S_1, \Sigma_1), (S_2, \Sigma_2)$ be two measurable spaces. Define
	$$S:=S_1 \times S_2$$
	$$\Sigma := \sigma(\{B_1 \times B_2; B_i \in \Sigma_i \text{ (rectangles ), i=1,2}\})$$
	And coordinate maps $\rho_i: S \to S_i$, $\rho_i(s)=s_i$ for $\forall s \in S$. $(S, \Sigma)$ is called product space by $(S_1, \Sigma_1)\times (S_2, \Sigma_2)$. 

	\item[\textit{Rm.}] 
	\begin{itemize}
		\item[$\cdot$] In fact $\Sigma=\sigma(\rho_1, \rho_2)$, i.e. preimage of $\rho_i \in \Sigma$, which is clearly the case, for example, pick any $B_1\in \Sigma_1$, $\rho_1^{-1}(B_1)=B_1 \times S_2 \in \Sigma$.
		\item[$\cdot$] The generator set in $\sigma(\cdot)$, collection of rectangles, is a $\pi$ system.
	\end{itemize}

	\item[\textit{Lemma}] (\textit{Measurability on prod space implies that at each, fix another coordinate.}) $(S, \Sigma)=(S_1, \Sigma_1)\times(S_2, \Sigma_2)$. Consider $m\Sigma\ni f:S\to \mathbb{R}$, then
	\begin{itemize}
		\item[$\cdot$] Fix $\bar{s_1}\in S_1$ \textit{then} $m\Sigma_2 \ni f(\bar{s_1}, \cdot): S_2 \to \mathbb{R}, s_2 \mapsto f(\bar{s_1}, s_2)$.
		\item[$\cdot$] Fix $\bar{s_2}\in S_2$ \textit{then} $m\Sigma_1 \ni f(\cdot, \bar{s_2}): S_1 \to \mathbb{R}, s_1 \mapsto f(s_1, \bar{s_2})$.
	\end{itemize}

	\item[\textit{Proof. }] We use \textbf{Monotone Class Thm}. Let $\mathcal{H}$ be the class of real-valued functions, such that results in lemma holds. It suffices to show $ m\Sigma \subseteq \mathcal{H}$, i.e. $\forall f \in m\Sigma$, $f\in \mathcal{H}$, lemma holds.\newline
	One can easily show $\mathcal{H}$ is a vector space\footnote{Since linearity preserves measurability.}, and $1\in \mathcal{H}$.\newline
	Consider $\{f_n\} \subseteq \mathcal{H}$, $f_n \nearrow f$, $f_n >0$. Then, for all $s \in S$, $f(s) =\lim\limits_{n\rightarrow\infty} f_n(s)$,\footnote{Since limiting preserves measurability.} $f\in m\Sigma$. Hence $\mathcal{H}$ is monotone class. \newline
	$\pi$ system $\mathcal{I}=\{B_1 \times B_2, B_i \in \Sigma_i, i=1,2\}$, $\sigma(\mathcal{I})=\Sigma$, for all $A \in \mathcal{I}$, 
	\begin{equation}
		\mathbbm{1}_{A}(s)=\mathbbm{1}_{B_1 \times B_2}((s_1, s_2))=\mathbbm{1}_{B_1}(s_1) \cdot \mathbbm{1}_{B_2}(s_2)
	\end{equation}
	Clearly, $\mathbbm{1}_{A}$ is $\Sigma_i$ measurable fixing the other coordinate, i.e. $\mathbbm{1}_{A}\in \mathcal{H}$. By monotone class thm, $m(\sigma(\mathcal{I}))\in \mathcal{H}$. $\blacksquare$
\end{itemize}

%////////////////////////////////////////////////////////////////////////
\section{Product Measure, Fubini's Thm}
\textbf{Motivation}: We want to define measure on product space $(S, \Sigma)$.

\begin{itemize}
	\item[\textit{Def.}] \textbf{Slice Integral}: Assume $\mu_i$ is finite measure on $(S_i, \Sigma_i)$. For pointwise mapping $f: S\to \mathbb{R}$ for either $f \in b\Sigma$ or $(m\Sigma)^+$, for all $s_1\in S_1$ and $s_2 \in S_2$, define two slice integrals of $f$:
	\begin{equation}
		I_1^f(s_1):=\int_{S_2}f(s_1, s_2)\mu_2(ds_2)
	\end{equation}
	\begin{equation}
		I_2^f(s_2):=\int_{S_1}f(s_1, s_2)\mu_1(ds_1)
	\end{equation}

	\item[\textit{Lemma}] \textit{(Integrate slice against another coordinate)} Assume $f\in b\Sigma$, then $I^f_i \in b\Sigma_i$, $i=1,2$ And
	\begin{equation}
		\int_{S_1} I_1^f(s_1)\mu_1(ds_1)=\int_{S_2} I_2^f(s_2)\mu_2(ds_2)~~(\dagger)
	\end{equation}
	i.e.
	\begin{equation}
		\int_{S_1}\int_{S_2}f(s_1, s_2)d\mu_1 d\mu_2 = \int_{S_2}\int_{S_1}f(s_1, s_2)d\mu_2 d\mu_1
	\end{equation}

	\item[\textit{Proof.}] Let $\mathcal{H}$ be class of bounded functions s.t. lemma holds. Verify that $\mathcal{H}$ is a monotone class (1,2 omitted here, for 3, $f_n \nearrow f$, $\dagger$ holds on $f$ by (\textbf{DOM})) \newline
	Choose same $\pi$ system $\mathcal{I}$ ($B_1 \times B_2$), indicator $\mathbbm{1}_{A}$:
	\begin{equation}
		I_1^{\mathbbm{1}_{A}}(s_1)=\int_{S_2} \mathbbm{1}_{A}(s_1, s_2) d\mu_2=\int_{S_2} \mathbbm{1}_{B_1}(s_1)\cdot \mathbbm{1}_{B_2}(s_2) d\mu_2= \mathbbm{1}_{B_2}(s_2)\mu_2(B_2)
	\end{equation}
	Similarly
	\begin{equation}
		I_2^{\mathbbm{1}_{A}}(s_2)=\mathbbm{1}_{B_1}(s_1)\mu_1(B_1)
	\end{equation}
	($\dagger$) integrate out remaining coordinate, both are $\mu_1(B_1)\mu_2(B_1)$. Therefore $\mathbbm{1}_{A}\in \mathcal{H}$. By monotone class thm, $\sigma(\mathcal{I})=b\Sigma \subseteq \mathcal{H}$. $\blacksquare$

	\item[\textit{Cor.}] (\textbf{Tonelli}) $f\in (m\Sigma)^{+}$, then $\dagger$ holds for $I_i^{f}\in (m\Sigma)^+$.

	\item[\textit{Proof. }] For each $k> 0$, define $f_k:=f\wedge k:=f\cdot \mathbbm{1}_{\{f\leq k\}}$. Clearly $f_k \in b\Sigma$, moreover $f_k \nearrow f$. \newline
	Apply lemma for $f_k$, we have $I_i^{f_k} \in b\Sigma_i$.\newline
	Since $f=\lim\limits_{k\rightarrow\infty}f_k$, by (\textbf{MON}) $\Rightarrow$ $I_i^f = \lim\limits_{k\rightarrow\infty}I_i^{f_k}$, $i=1,2$. So $I_i^{f}\in (m\Sigma)^+$. $\blacksquare$


	\item[\textit{Thm.}] (\textbf{Fubini}) Measure space $(S_i, \Sigma_i, \mu_i)$, $i=1,2$, $\mu_i$ are finite measure. Define $(S,\Sigma)$ same as section 1, define $\mu: S\to \mathbb{R}$, s.t. for all $A\in \Sigma$,
	\begin{equation}
		\mu(A):=\int_{S_1} I^{\mathbbm{1}_{A}}_1 d\mu_1 = \int_{S_2} I^{\mathbbm{1}_{A}}_2 d\mu_2
	\end{equation}
	Denote $(S,\Sigma, \mu)=(S_1,\Sigma_1, \mu_1)\times(S_2,\Sigma_2, \mu_2)$, denote $\mu = \mu_1 \times \mu_2$, \newline
	\textit{Then}
	\begin{itemize}
		\item[$\cdot$] $\mu$ is a measure on $(S, \Sigma)$ (contable additive).
		\item[$\cdot$] $\mu$ is the unique measure on $(S, \Sigma)$, such that $\mu(B_1 \times B_2)=\mu_1(B_1)\cdot \mu_2(B_2)$.
		\item[$\cdot$] If $f\in (m\Sigma)^+$, then
		\begin{equation}
			\int_S fd\mu=\int_{S_1} I_1^fd\mu_1=\int_{S_2} I_2^fd\mu_2~~(\#)
		\end{equation}
		\item[$\cdot$] If $f\in \mathcal{L}^1(S,\Sigma,\mu)$, then $I_i^f\in \mathcal{L}^1(S_i, \Sigma_i, \mu_i)$, and (\#) holds
	\end{itemize}

	\item[\textit{Proof.}] $\bullet$ Part-1, $\mu$ is measure. \newline
	Pick $A,B\in \Sigma$, disjoint $\Rightarrow$ $\mathbbm{1}_{A\cup B}=\mathbbm{1}_{A}+\mathbbm{1}_{B}$. By definition
	\begin{equation}
		\mu(A\cup B):=\int_{S_1} (I^{\mathbbm{1}_{A}}_1+I^{\mathbbm{1}_{B}}_1) d\mu_1=:\mu(A)+\mu(B)
	\end{equation}
	So we have finite additivity. Now consider $\mu(U), U:=\bigcup_{n\geq 1}E_n, U_n:=\bigcup_{j=1}^n E_j, E_n\in \Sigma$. We have $\mathbbm{1}_{U_n}\nearrow \mathbbm{1}_{U}$. 
	By (\textbf{MON}): $I_1^{\mathbbm{1}_{U_n}}\nearrow I^{\mathbbm{1}_{U}}_1$.\newline
	By (\textbf{MON}) again: $\int I_1^{\mathbbm{1}_{U_n}} \to \int I_1^{\mathbbm{1}_{U}}$. \newline
	Therefore
	\begin{equation}
		\mu(U):=\int_{S_1}I_1^{\mathbbm{1}_{U}} = \lim\limits_{n\rightarrow\infty}\int_{S_1}I_1^{\mathbbm{1}_{U_n}}=\int_{S_1}\lim\limits_{n\rightarrow\infty}I_1^{\mathbbm{1}_{U_n}}=\int_{S_1}\sum_{n\geq 1}I_1^{\mathbbm{1}_{E_n}}=\sum_{n\geq 1}\mu(E_n)~~\blacksquare
	\end{equation}

	$\bullet$ Part-2, $\mu$ is unique. \newline
	If $\mu'$ is another measure satisfies hypothesis ($\mu(B_1 \times B_2)=\mu_1(B_1)\cdot \mu_2(B_2)$). Clearly $\mu=\mu'$ on $\mathcal{I}$, rectangles. $\mathcal{I}$ is $\pi$ system. By $\pi$-system thm, $\mu=\mu'$ on $\Sigma$. $\blacksquare$ \newline

	$\bullet$ Part-3, (\#) eq for $f\in (m\Sigma)^+$. \newline
	The second equal sign is clear, (\textbf{Tonelli}), show the first one. \newline
	For $f=\mathbbm{1}_{A}$, 
	\begin{equation}
		\int_S f d\mu=\int_S \mathbbm{1}_{A}d\mu=\mu(A):=\int_{S_1} I^{\mathbbm{1}_{A}}_1 d\mu_1
	\end{equation}
	Holds just by definition of $\mu$.\newline
	For $f\in SF^+$, by linearity, \# holds. \newline
	For $f\in (m\Sigma)^+$ by (\textbf{MON}), \# holds. $\blacksquare$ \newline

	$\bullet$ Part-4, (\#) eq for $f\in \mathcal{L}^1$. \newline
	$f=f^+-f^-$, $f\in \mathcal{L}^1 \Rightarrow f^{\pm}<\infty~a.s$. So \# holds for $f^{\pm}$. All relevant integrals are finite, we can rearrange terms by linearity. So \# holds for $f$. $\blacksquare$


	\item[\textit{Rm.}] Remarks on (\textbf{Fubini})
	\begin{itemize}
		\item[$1.$] The condition in statement says $\mu_i$ are finite. We actually have Fubini for $\mu_i$ that are $\sigma$-finite.
		\item[$2.$] Since we can extend product of two to product of finitely many, Fubini holds for $n<\infty$ product space, i.e. $\prod_{k=1}^n (S_k, \Sigma_k, \mu_k)$. 
		\item[$3.$] Lemma in section 1 says measurability on product space implies that at each factor space. But other direction is not true. i.e. $f(\bar{s_1}, \cdot)\in m\Sigma_1,f(\cdot, \bar{s_2})\in m\Sigma_2$ \textbf{Does Not Imply} $f\in m\Sigma$.
		\item[$4.$] Fubini says integrability on product space implies that at each factor space. But other direction is not true. i.e. $I^f_i\in \mathcal{L}^1(S_i, \Sigma_i, \mu_i)$ \textbf{Does Not Imply} $f\in \mathcal{L}^1(S, \Sigma,\mu)$.
	\end{itemize}
	Two examples of 3 and 4:

\end{itemize}

%////////////////////////////////////////////////////////////////////////
\section{Joint Distribution, Joint Law}

\begin{itemize}
	\item[\textit{Def.}] \textbf{Joint Distribution}: Prob space $(\Omega, \mathcal{F}, \mathbb{P})$, real valued RV $X,Y$, define joint distribution function as
	\begin{equation}
		F_{(X,Y)}(x,y):=\mathbb{P}\left(X\leq x, Y\leq y\right)
	\end{equation}

	\item[\textit{Def.}] \textbf{Joint Law}: Define $\mathcal{L}_{(X,Y)}$ as joint law of $(X,Y)$. $\mathcal{L}_{(X,Y)}$ is then a prob measure on product image space $(\mathbb{R}^2, \mathscr{B}(\mathbb{R}^2))$, s.t. for all $A\in \mathscr{B}(\mathbb{R}^2)$,
	\begin{equation}
		\mathcal{L}_{(X,Y)}(A):=\mathbb{P}\left((X,Y)\in A\right)
	\end{equation}

	\item[\textit{Def.}] \textbf{Joint PDF}: If $\mathcal{L}_{(X,Y)}$ is absolutely continuous with respect to lebesgue measure on $\mathbb{R}^2$ (denote as $dxdy$), then the joint pdf of $(X,Y)$ exists, denote $f_{(X,Y)}$, $f_{(X,Y)}\in m\mathscr{B}(\mathbb{R}^2)$, and is defined as Radon-Nikodym derivative of joint law wrt lebesgue measure on product image space,
	\begin{equation}
		f_{(X,Y)}:= \frac{d \mathcal{L}_{(X,Y)}}{dxdy}
	\end{equation}

	\item[\textit{Prop.}] If $f_{(X,Y)}$ is joint pdf, then by (\textbf{Fubini}), then
	$$f_X(x):=\int_{\mathbb{R}}f_{(X,Y)}(x,y)dy~~\text{ is pdf of $X$.}$$
	$$f_Y(y):=\int_{\mathbb{R}}f_{(X,Y)}(x,y)dx~~\text{ is pdf of $Y$.}$$

\end{itemize}
%------------------------------------------------------------------------
\subsection{Joint * of Indep RVs}
\begin{itemize}
	\item[\textit{Prop.}] $X,Y$ are RV with respective cdf and law $\mathcal{L}_X, \mathcal{L}_Y$; $F_X, F_Y$. \textit{Then TFAE}\footnote{Jargon: The followings are equivalent ($\iff$).}:
	\begin{itemize}
		\item[$\cdot$] $X,Y$ are independent.
		\item[$\cdot$] $\mathcal{L}_{(X,Y)}=\mathcal{L}_X \times \mathcal{L}_Y$.
		\item[$\cdot$] $F_{X,Y}(x,y)=F_X(x)\cdot F_Y(y)$ for all $(x,y)\in \mathbb{R}^2$.
		\item[$\cdot$] (If respective pdf $f_X, f_Y$ exists) $f_{X,Y}(x,y)=f_X(x) \cdot f_Y(y)$ for a.e. $(x,y)\in \mathbb{R}^2$.
	\end{itemize}
	Statement four is special, in that respective pdf may not exist. And there is allowance for a.e. form every $(x,y)$, because integration eliminates aberrant null sets.
	\item[\textit{Proof.}] Proof is straightforward, noticing all four statements $\iff$ $\mathbb{P}\left(X\leq x, Y\leq y\right)=\mathbb{P}\left(X\leq x\right)\cdot \mathbb{P}\left(Y\leq y\right)$. $\blacksquare$

	\item[\textit{Prop.}] $X,Y$ indep, $X+Y$ is a new RV. Then Law of $X+Y$ is given by
	$$\mathcal{L}_{X+Y}(c)=\int_{\mathbb{R}}\mathcal{L}_Y([-\infty, c-x])\mathcal{L}_X(dx)=\int_{\mathbb{R}}\mathcal{L}_X([-\infty, c-y])\mathcal{L}_Y(dy)$$

	\item[\textit{Proof.}] 
	\begin{equation}
		\begin{split}
			\mathcal{L}_{X+Y}(c)=\mathbb{P}\left(X+Y\leq c\right)&=\iint_{\{(x,y):x+y\leq c\}}d \mathcal{L}_{(X,Y)}\\
			&=\iint_{\{(x,y):x+y\leq c\}}d (\mathcal{L}_{X}\times \mathcal{L}_Y)\\
			&=\int_{\mathbb{R}}\left[\int_{\mathbb{R}}\mathbbm{1}_{(-\infty,c-x]}(y)\mathcal{L}_Y(dy)\right]\mathcal{L}_X(dx)\\
			&=\int_{\mathbb{R}}\mathcal{L}_Y([-\infty, c-x])\mathcal{L}_X(dx)~~\blacksquare
		\end{split}
	\end{equation}
\end{itemize}

%------------------------------------------------------------------------
\subsection{Convolutions}
\begin{itemize}
	\item[\textit{Def.}] \textbf{Convolution of Function}: for $f\in \mathcal{L}^1$, $g$ is bounded, define
	$$(f*g)(x):=\int_{\mathbb{R}}f(x-y)g(y)dy$$

	\item[\textit{Def.}] \textbf{Convolution of Measure}: Given two finite measures $\mu,\nu$ on $(S, \Sigma)$, $\mu * \nu = \nu * \mu$ is a measure, for all $A\in (S, \Sigma)$, given by
	$$(\mu*\nu)(A):=\int_S \mu(A-s)\nu(ds)=\int_S \nu(A-s)\mu(ds)=:(\nu*\mu)(A)$$
	Where $A-s$ is $s$ translation of $A$, i.e. $A-s=\{t\in S, t+s \in A\}$.

	\item[\textit{Rm.}] By prop in last section, we actually have: (when $X,Y$ indep)
	$$\mathcal{L}_{X+Y}=\mathcal{L}_X * \mathcal{L}_Y$$
	If $f_X, f_Y$ exists, we have
	$$f_{X+Y}=f_X * f_Y$$
\end{itemize}

%////////////////////////////////////////////////////////////////////////
\section{Product of Countably Many Spaces}

%------------------------------------------------------------------------
\subsection{Product Measure}
We are now considering product of countably many spaces. i.e. $\prod_{n\geq 1}(\Omega_n, \mathcal{F}_n, \mathbb{P}_n)$.\newline
Define 
$$\Omega:=\prod_{n\geq 1}\Omega_n$$
representative element $w = (w_1, w_2,...), w_i \in \Omega_i$.\newline
Consider \textbf{Cylinder Sets} $E$, defined by
\begin{equation}
	E:=\prod_{n\geq 1}F_n= \prod_{k=1}^N F_{n_k} \times \prod_{j\notin\{n_k\}_1^N}\Omega_j
\end{equation}
Where $F_{n_k}\subseteq \Omega_{n_k}$, other $F_j=\Omega_j$ for $j\notin\{n_k\}$. This is saying that \textit{All but finitely many factors of $E$ are $\Omega$s}. \newline
Define
\begin{equation}
	\Sigma_0:=\left\{\bigcup_{k\geq 1}^K E^{[k]}: E^{[k]} \text{ are disjoint cylinder sets}\right\}
\end{equation}
It can be shown (omitted) that $\Sigma_0$ is an algebra. Let $\mathcal{F}:=\sigma(\Sigma_0)$. \newline
Define set function $\mathbb{P}: \Sigma_0\to [0,1]$, such that for all $A=\bigcup_{k\geq 1}^K E^{[k]}\in \Sigma_0$,
\begin{equation}
	\mathbb{P}\left(A\right):=\sum_{k=1}^K \left[\prod_{j\geq 1} \mathbb{P}_j\left(F_j\right)\right]
\end{equation}
Where $\mathbb{P}_j$ is measure on factor space. Then one can prove (omitted) that $\mathbb{P}$ is well-defined, $\mathbb{P}$ is a measure (countable additive).\newline
Thus by Caratheodory extension thm, $\mathbb{P}$ can be uniquely extended to $\mathcal{F}=\sigma(\Sigma_0)$. \newline
So we define $(\Omega, \mathcal{F}, \mathbb{P}):=\prod_{n\geq 1}(\Omega_n, \mathcal{F}_n, \mathbb{P}_n)$.

%------------------------------------------------------------------------
\subsection{Kolmogorov Extension Thm}
\begin{itemize}
	\item[\textit{Thm.}] (\textbf{Prelude}) Let $\{\mu_n: n\geq 1\}$ be a countable sequence of prob measures on $(\mathbb{R}, \mathscr{B}(\mathbb{R}))$. Then there exists $(\Omega, \mathcal{F}, \mathbb{P})$ and a seq of \textbf{indep} RVs $\{X_n\}$, such that $\mathcal{L}_{X_n}=\mu_n$ for all $n\geq 1$.

	\item[\textit{Proof.}] Previous result, for all $n\geq1$, exists $(\Omega_n, \mathcal{F}_n, \mathbb{P}_n)$, RV $Y_n:\Omega_n \to \mathbb{R}$, s.t $Y_n=\mu_n$. \newline
	Construct such spaces and $Y_n$ for all $n$, and together, define 
	\begin{equation}
		(\Omega, \mathcal{F}, \mathbb{P}):=\prod_{n\geq1}(\Omega_n, \mathcal{F}_n, \mathbb{P}_n)
	\end{equation}
	Define in product space $X_n: \Omega\to \mathbb{R}$, $w\in \Omega\mapsto Y_n(w_n)\in \mathbb{R}$, i.e. $X_n(w)=Y(w_n)$. Now for all $B\in \mathscr{B}(\mathbb{R})$, by definition of product measure in countably product space,
	\begin{equation}
		\begin{split}
			\mathbb{P}\left(X_n \in B\right)&=\prod_{j=1}^{n-1}\mathbb{P}_j\left(\Omega_j\right) \cdot \mathbb{P}_n\left(Y_n\in B\right)\cdot \prod_{j=n+1}^{\infty}\mathbb{P}_j\left(\Omega_j\right)\\
			&=\mathbb{P}_n\left(Y_n \in B\right)=\mu_n(B)
		\end{split}
	\end{equation}
	Then show $\{X_n\}$ indep, i.e. $\forall L\geq 1$, $n_1, n_2, ..., n_L$ disjoint,
	\begin{equation}
		\begin{split}
			\mathbb{P}\left(X_{n_1}\in B_1, X_{n_2}\in B_2 ,..., X_{n_L} \in B_L \right)&=\mathbb{P}\left(\prod_{j\notin{n_k}_1^L}\Omega_j \times \prod_{l=1}^L\{Y_{n_l}\in B_l\}\right)\\
			&=\prod_{l=1}^L \mathbb{P}\left(Y_{n_l}\in B_l\right)\\
			&=\prod_{l=1}^L \mathbb{P}\left(X_{n_l}\in B_l\right)~~(indep)~~\blacksquare
		\end{split}	
	\end{equation}

	\item[\textit{Thm.}] (\textbf{Kolmogorov's Extension}) For $n\geq 1$, $\mu^{(n)}$ is prob measure on $(\mathbb{R}^n, \mathscr{B}(\mathbb{R}^n))$. For $1\leq m\leq n$, define $\pi_{m,n}$ as extension mapping, $\forall B\in \mathscr{B}(\mathbb{R}^m)$, $\pi_{m, n}(B):=\{(x_1, x_2, ..., x_n)\in \mathbb{R}^n, (x_1, x_2, ..., x_m)\in B\}$, i.e.
	$$\pi_{m,n}(B)=B\times \mathbb{R}^{n-m}$$
	Assume $\mu^{(n)}$ satisfies consistency condition:\newline
	$\forall n\geq 1$, $\forall 1\leq m\leq n $, $\forall B\in \mathscr{B}(\mathbb{R}^m)$, 
	$$\mu^{(n)}(\pi_{m,n}(B))=\mu^{(n)}(B)$$
	Then, exists prob space $(\Omega, \mathcal{F}, \mathbb{P})$, sequence of RV (\textit{Not necessarily indep}) $\{X_n:n\geq 1\}$, such that $\mu^{(n)}=\mathcal{L}_{(X_1, X_2, ..., X_n)}$.

	\item[\textit{Rm.}] Thm prelude is a particular case of (\textbf{Kolmogorov}).
\end{itemize}


%------------------------------------------------------------------------
%////////////////////////////////////////////////////////////////////////
%////////////////////////////////////////////////////////////////////////
%------------------------------------------------------------------------
\chapter{Conditioning and Martingale}

%////////////////////////////////////////////////////////////////////////
\section{Conditional Expectation}
\begin{itemize}
	\item[\textit{Def.}] \textbf{Conditional Expectation}: Define $(\Omega, \mathcal{F}, \mathbb{P})$ be probability space and $X: \Omega \to \mathbb{R}$ RV, $X\in \mathcal{L}^1 (\Omega, \mathcal{F}, \mathbb{P})$. $\mathcal{G}\subseteq \mathcal{F}$ is a sub $\sigma$ algebra Then, $Y\in \mathcal{L}^1$ is the conditional expectation of $X$ given $\mathcal{G}$ (actually an RV), denoted by $Y:=\mathbb{E}\left[X|\mathcal{G} \right]$ if
	\begin{itemize}
		\item[$\cdot$] $Y \in m \mathcal{G}$.
		\item[$\cdot$] For all $A \in \mathcal{G}$,
		$$\int_{A} X d \mathbb{P} = \int_A Y d \mathbb{P}$$
	\end{itemize}

	\item[\textit{Rm.}] The intuition of $\mathbb{E}\left[X| \mathcal{G} \right]$ is, given the partial information contained in $\mathcal{G}$, the best prediction of $X$ on whole space.

	\item[\textit{Rm.}] Conditional expectation can be defined for $X, Y$ not necessarily in $\mathcal{L}^1$. It is ok as long as for all $A \in \mathcal{G}$, integral of $X, Y$ on $A$ are defined.

	\item[\textit{Rm.}] The defining condition can be replaced by if $\mathcal{G}=\sigma(\mathcal{I})$, where $\mathcal{I}$ is a $\pi$ system, then $\forall A \in \mathcal{I}$, the integrals are equal. Because $A\in \mathcal{G} \mapsto \int_A X d \mathbb{P}$ can be viewed as a signed measure on $\mathcal{G}$, we can apply $\pi$ system lemma.

	\item[\textit{Prop.}] (\textbf{Monotonicity}) If $X_1 \leq X_2~~a.s.$ then $Y_1:=\mathbb{E}\left[X_1|\mathcal{G} \right]\leq \mathbb{E}\left[X_2|\mathcal{G} \right]=:Y_2~~a.s.$
	\begin{itemize}
		\item[\textit{Proof}.] Let $A:=\{Y_2>Y_1\}$, clearly $A\in \mathcal{G}$, by definition
		\begin{equation}
			\int_A Y_1 d\mathbb{P}=\int_A X_1 d\mathbb{P}\leq \int_A X_2 d\mathbb{P} = \int_A Y_2 d\mathbb{P}
		\end{equation}
		\begin{equation}
			\int_A (Y_1-Y_2) d\mathbb{P} \leq0
		\end{equation}
		But $(Y_1 - Y_2)>0$ on $A$, so $\mathbb{P}\left(A\right)=0$. $\blacksquare$
	\end{itemize}

	\item[\textit{Thm.}] (\textbf{Existence and Uniqueness}) Given $(\Omega, \mathcal{F}, \mathbb{P})$, $\mathcal{G}\subseteq \mathcal{F}$, $X \in \mathcal{L}^1$, then $\mathbb{E}\left[X|\mathcal{G} \right]$ exists and is unique a.s.
	\begin{itemize}
		\item[\textit{Proof}.] Prove uniqueness first, assume $Y_1, Y_2$ both satisfies definition. Since $X=X$, by monotonicity, $Y_1 \leq Y_2$; $Y_2 \leq Y_1$. $\blacksquare$ \newline
		Then existence. We have two approaches. \newline

		\textbf{Version 1.} (\textit{Radon-Nikodyn thm}) the idea is that we view conditional expectation as a signed measure. \newline
		Define $\mu_{\mathcal{G}}^X$ on $\mathcal{G}$, such that $\forall A \in \mathcal{G}$,
		\begin{equation}
			\mu_{\mathcal{G}}^X (A):=\int_A X d\mathbb{P}
		\end{equation}
		One can check this is a measure. Besides, when $\mathbb{P}\left(A\right)=0$, $\mu_{\mathcal{G}}^X (A)=0$. Moreover, $\mu_{\mathcal{G}}^X (A)$ is absolutely continous wrt $\mathbb{P}\lceil_{\mathcal{G}}$ (probability measure restricted on $\mathcal{G}$). Apply \textbf{Radon-Nikodyn}, $\exists Y \in m \mathcal{G}$,\footnote{Note: here we get correct measurability of $Y$ for free.} s.t.
		\begin{equation}
			Y = \frac{d\mu_{\mathcal{G}}^X}{d\mathbb{P}\lceil_{\mathcal{G}}}~~\text{i.e. the R-N derivative}
		\end{equation}
		So, for all $A \in \mathcal{G}$, (view $Y$ as the density)
		\begin{equation}
			\int_A X d\mathbb{P}=:\mu_{\mathcal{G}}^X (A)=\int_{A}Y d\mathbb{P}\lceil_{\mathcal{G}}=\int_{A}Y d\mathbb{P}
		\end{equation}

		\textbf{Version 2.} (\textit{$\mathcal{L}^2$ projection}) We first assume $X \in \mathcal{L}^2(\Omega, \mathcal{F}, \mathbb{P})=:\mathcal{L}^2(\mathcal{F})$. Then for $\mathcal{G}\subseteq \mathcal{F}$, $\mathcal{L}^2(\mathcal{G})=\{Y\in \mathcal{L}^2 (\Omega, \mathcal{F}, \mathbb{P}): Y\in m \mathcal{G}\}$ is a Hilbert space, and a subspace of $\mathcal{L}^2(\mathcal{F})$. Because
		\begin{itemize}
			\item[$\cdot$] $\mathcal{L}^2(\mathcal{G})$ is complete. Given any Cauchy $\{Y_n\}$ in it, $\{Y_n\}$ admits a limit in $\mathcal{L}^2(\mathcal{G})$, itself. Because
			\item[$\cdot$] $\{Y_n\}$ is also a Cauchy in $\mathcal{L}^2(\mathcal{F})$ $\Rightarrow$ $\exists Y_{\infty}\in \mathcal{F}$, s.t. $Y_n \xrightarrow{\mathcal{L}^2} Y_{\infty}$ $\Rightarrow$ $Y_n \xrightarrow{i.p} Y_{\infty}$.
			\item[$\cdot$] Exists subsequence $\{Y_{n_k}\}$, $Y_{n_k} \xrightarrow{a.s.} Y_{\infty}$. Since $Y_{n_k} \in m \mathcal{G}$, a.s. convergence preserves measurability, so $Y_{\infty} \in m \mathcal{G}$, i.e. $Y_{\infty} \in \mathcal{L}^2(\mathcal{G})$.
 		\end{itemize}
 		For any $X \in \mathcal{L}^2(\mathcal{F})$, consider projection of $X$ onto $\mathcal{L}^2(\mathcal{G})$, denoted by $P_{\mathcal{G}}X$, by projection, we mean
 		\begin{itemize}
 			\item[$\cdot$] $P_{\mathcal{G}}X\in m \mathcal{G}$.
 			\item[$\cdot$] $(X-P_{\mathcal{G}}X)$ is orthogonal to $P_{\mathcal{G}}X$, i.e. for all $Y \in \mathcal{L}^2(\mathcal{G})$:
 			\begin{equation}
 				\int_{\Omega} Y(X-P_{\mathcal{G}}X) d\mathbb{P} =0
 			\end{equation}
 		\end{itemize}
 		For any $A\in \mathcal{G}$, take $Y=\mathbbm{1}_{A}$, we have
 		\begin{equation}
 			\int_{\Omega} \mathbbm{1}_{A}(X-P_{\mathcal{G}}X) d\mathbb{P} =0
 		\end{equation}
 		The conditional expection is exactly $P_{\mathcal{G}}X$. \newline
 		Now for general $X\in \mathcal{L}^1$, take $X_n^{\pm}\in SF^+$, such that $X_n^{\pm}\nearrow X^{\pm}$. By simple function we have $X_n^{\pm}\in \mathcal{L}^2$ for free. By previous arguments we define $\mathbb{E}\left[X_n^{\pm}|\mathcal{G}\right]:= P_{\mathcal{G}}X_n^{\pm}$. \newline
 		$P_{\mathcal{G}}X_n^{\pm} \nearrow Y^{\pm}$ for some $Y^{\pm}\in m \mathcal{G}$ (since limit transfers measurability). We can verify by (\textbf{MON}) that $Y^{\pm}$ has defining property of $\mathbb{E}\left[X^{\pm}|\mathcal{G}\right]$. \newline
 		Finally since everything are finite, by linearity, $Y=Y^{+}-Y^-=:\mathbb{E}\left[X|\mathcal{G}\right]$. $\blacksquare$

 		\item[\textit{Ex}] Examples of conditional expection. 
 		\begin{itemize}
 			\item[1.] $\mathcal{G}=\sigma(A)=\sigma(\mathbbm{1}_{A})=\{\emptyset, \Omega, A, A^{c}\}$, $A \in \mathcal{F}$. Then for every $X \in \mathcal{L}^1$, 
 			\begin{equation}
 				\mathbb{E}\left[X|\mathcal{G}\right]=\mathbbm{1}_{A}\frac{\mathbb{E}\left[X; A\right]}{\mathbb{P}\left(A\right)}+\mathbbm{1}_{A^c} \frac{\mathbb{E}\left[X;A^c\right]}{\mathbb{P}\left(A^c\right)}
 			\end{equation}
 			\item[\textit{Rm.}] Since $\mathbb{E}\left[X|\mathcal{G}\right]\in m\sigma(A)=m\sigma(\mathbbm{1}_{A}$, think about it, $\mathbb{E}\left[X|\mathcal{G}\right]$ must be somehow a function \textit{composed} with $\mathbbm{1}_{A}$.

 			\item[2.] (\textit{Conditioning of events}) If $X=\mathbbm{1}_{B}$, $B\in \mathcal{F}$, ($\mathbb{E}\left[B|A\right]$)
 			\begin{equation}
 				\begin{split}
 					\mathbb{E}\left[\mathbbm{1}_{B}|\sigma(A)\right]&=\mathbbm{1}_{A}\frac{\mathbb{P}\left(A \cap B\right)}{\mathbb{P}\left(A\right)}+\mathbbm{1}_{A^c}\frac{\mathbb{P}\left(B\cap A^c\right)}{\mathbb{P}\left(A^c\right)}\\
 					&=\mathbbm{1}_{A}\mathbb{P}\left(B|A\right)+\mathbbm{1}_{A^c}\mathbb{P}\left(B|A^c\right)
 				\end{split}
 			\end{equation}

 			\item[3.] (\textit{Conditioning of RVs}) $\mathcal{G}=\sigma(Y)$, $X,Y$ have joint pdf $f_{(X,Y)}$, $\mathbb{E}\left[X|\mathcal{G}\right]=:\mathbb{E}\left[X|Y\right]$, define conditional pdf of $X$ given $Y$ as
 			\begin{equation}
 				f_{X|Y}(x|y):=\frac{f_{(X,Y)}(x,y)}{f_Y(y)}~~\text{if $f_Y(y)\ne 0$, else $0$}
 			\end{equation}
 			Assume $h: \mathbb{R}\to \mathbb{R}$ is Borel function s.t. $h(X) \in \mathcal{L}^1$, define
 			\begin{equation}
 				g(y):=\int_{\mathbb{R}}h(x)f_{X|Y}(x,y)dx
 			\end{equation}
 			Then conditional expection of $X$ given $Y$ is $g$ composed with $Y$. (again, c.f. remark in example 1, since $\mathbb{E}\left[X|Y\right]\in m\sigma(Y)$, it must be a function composed with $Y$.)
 			\begin{equation}
 				\mathbb{E}\left[h(X)|Y\right]=g(Y)=\int_{\mathbb{R}}h(x)f_{X|Y}(x,Y)dx
 			\end{equation}
 		\end{itemize}
	\end{itemize}
\end{itemize}

%////////////////////////////////////////////////////////////////////////
\section{Properties}

%------------------------------------------------------------------------
\subsection{Simple properties}
\begin{itemize}
	\item[\textit{Prop.}] (\textbf{Expectation}) a special case of tower property:
	\begin{equation}
		\mathbb{E}\left[\mathbb{E}\left[X|\mathcal{G}\right]\right] = \mathbb{E}\left[X\right]
	\end{equation}

	\item[\textit{Prop.}] If $X\in m \mathcal{G}$, then
	\begin{equation}
		\mathbb{E}\left[X|\mathcal{G}\right] = X
	\end{equation}

	\item[\textit{Prop.}] (\textbf{Linearlity})
	\begin{equation}
		\mathbb{E}\left[aX+bY|\mathcal{G}\right]=a \mathbb{E}\left[X|\mathcal{G}\right] + b \mathbb{E}\left[Y|\mathcal{G}\right]
	\end{equation}

	\item[\textit{Prop.}] (\textbf{Monotonicity}) If $X_1\leq X_2$, then
	\begin{equation}
		\mathbb{E}\left[X_1|\mathcal{G}\right]\leq \mathbb{E}\left[X_2|\mathcal{G}\right]
	\end{equation}
\end{itemize}

%------------------------------------------------------------------------
\subsection{Conditional Convergence Thms}
\begin{itemize}
	\item[\textit{Prop.}] (\textbf{cMON}) If $X_n \nearrow X$, $X_n, X \in \mathcal{L}^1$; then $\mathbb{E}\left[X_n|\mathcal{G}\right]\nearrow \mathbb{E}\left[X|\mathcal{G}\right]$.
	\begin{itemize}
		\item[\textit{Proof}.] Take $X_n - X_1 \nearrow X-X_1$, clearly $X_n-X_1 \in (m \mathcal{F})^+$. Define $Y_n:=\mathbb{E}\left[X_n|\mathcal{G}\right]$, for all $A\in \mathcal{G}$, 
		\begin{equation}
			\begin{split}
				\int_A (X-X_1) d\mathbb{P} &= \lim\limits_{n\rightarrow\infty}\int_A (X_n - X_1) d\mathbb{P}~~\text{ (\textbf{MON})}\\
				&= \lim\limits_{n\rightarrow\infty} \int_A (\mathbb{E}\left[X_n|\mathcal{G}\right] - \mathbb{E}\left[X_1|\mathcal{G}\right]) d\mathbb{P}~~\text{ (definition)}\\
				&= \int_A \lim\limits_{n\rightarrow\infty} \mathbb{E}\left[X_n|\mathcal{G}\right] - \mathbb{E}\left[X_1|\mathcal{G}\right] d\mathbb{P}~~\text{ (\textbf{MON}) again}\\
			\end{split}
		\end{equation}
		Cancel out $X_1$, we have
		\begin{equation}
			\int_A X d\mathbb{P} = \int_A \lim\limits_{n\rightarrow\infty}\mathbb{E}\left[X_n|\mathcal{G}\right] d\mathbb{P} 
		\end{equation}
		So by definition, $\mathbb{E}\left[X|\mathcal{G}\right]:= \lim\limits_{n\rightarrow\infty}\mathbb{E}\left[X_n|\mathcal{G}\right]$. $\blacksquare$
	\end{itemize}

	\item[\textit{Prop.}] (\textbf{cFatou}) If $X_n\geq 0$, then
	\begin{equation}
		\mathbb{E}\left[\liminf\limits_{n\rightarrow\infty}X_n|\mathcal{G}\right]\leq \liminf\limits_{n\rightarrow\infty} \mathbb{E}\left[X_n|\mathcal{G}\right]
	\end{equation}

	\item[\textit{Prop.}] (\textbf{cDOM}) If $|X_n|\leq Y\in \mathcal{L}^1$, $X_n \xrightarrow{a.s.} X$, then $\mathbb{E}\left[X_n|\mathcal{G}\right] \xrightarrow{a.s.} \mathbb{E}\left[X|\mathcal{G}\right]$.

	\item[\textit{Prop.}] (\textbf{cJensen}) $\phi: \mathbb{R}\to \mathbb{R}$, \textit{convex}. $\phi(x)\in \mathcal{L}^1$. Then $\mathbb{E}\left[\phi(X)|\mathcal{G}\right]\geq \phi(\mathbb{E}\left[X|\mathcal{G}\right])$.

	\item[\textit{Cor.}] If $X\in \mathcal{L}^p$, then $|\mathbb{E}\left[X|\mathcal{G}\right]|^p\leq \mathbb{E}\left[|X|^p|\mathcal{G}\right]$. Moreover we take $p$ norm of both sides,
	\begin{equation}
		(\mathbb{E}\left[|\mathbb{E}\left[X|\mathcal{G}\right]\right|^p])^{\frac{1}{p}}\leq (\mathbb{E}\left[\mathbb{E}\left[|X|^p|\mathcal{G}\right]\right])^{\frac{1}{p}}= \mathbb{E}\left[|X|^p\right]^{\frac{1}{p}}
	\end{equation}
	i.e., $X\in \mathcal{L}^p$ automatically guarantees that $\mathbb{E}\left[X|\mathcal{G}\right]\in \mathcal{L}^p$, and
	\begin{equation}
		\|\mathbb{E}\left[X|\mathcal{G}\right]\|_{\mathcal{L}^p}\leq \|X\|_{\mathcal{L}^p}
	\end{equation}

\end{itemize}

%------------------------------------------------------------------------
\subsection{Tower Property}
\begin{itemize}
	\item[\textit{Prop.}] (\textbf{Tower property}) Suppose $\mathcal{H} \subseteq \mathcal{G}$ is a sub $\sigma$ algebra, then
	\begin{equation}
		\mathbb{E}\left[\mathbb{E}\left[X|\mathcal{H}\right]|\mathcal{G}\right]= \mathbb{E}\left[X|\mathcal{H}\right] = \mathbb{E}\left[\mathbb{E}\left[X|\mathcal{G}\right] |\mathcal{H}\right]
	\end{equation}
	\item[\textit{Proof}.] The first equal sign is trivial, because $\mathbb{E}\left[X|\mathcal{H}\right] \in m \mathcal{H}\subseteq m\mathcal{G}$. By property 2, $\mathbb{E}\left[\mathbb{E}\left[X|\mathcal{H}\right] |\mathcal{G}\right]= \mathbb{E}\left[X|\mathcal{H}\right]$ can be taken out from outer expectation. \newline
	The second one. For all $A\in \mathcal{H} \subseteq \mathcal{G}$,
	\begin{equation}
		\int_A X d\mathbb{P}= \int_A \mathbb{E}\left[X|\mathcal{G}\right] d\mathbb{P}=\int_A \mathbb{E}\left[\mathbb{E}\left[X|\mathcal{G}\right] |\mathcal{H}\right] d\mathbb{P}
	\end{equation}
	The first equal sign follows that $A\in \mathcal{G}$, second follows that $A\in \mathcal{H}$. By definition, $\mathbb{E}\left[X|\mathcal{H}\right]=\mathbb{E}\left[\mathbb{E}\left[X|\mathcal{G}\right] |\mathcal{H}\right]$. $\blacksquare$
\end{itemize}

%------------------------------------------------------------------------
\subsection{Taking out what is known}
\begin{itemize}
	\item[\textit{Prop.}] Suppose $Z\in m \mathcal{G}$ and $XZ \in \mathcal{L}^1$, then $\mathbb{E}\left[XZ|\mathcal{G}\right]=Z\mathbb{E}\left[X|\mathcal{G}\right]$.
	\item[$Proof.$] Follow the definition, it suffices to show that for all $A\in \mathcal{G}$, 
	\begin{equation}
		\int_A XZ d\mathbb{P} = \int_A Z \mathbb{E}\left[X|\mathcal{G}\right] d\mathbb{P}~~(\dagger)
	\end{equation}
	Where $Z \in m \mathcal{G}$. First we assume $Z= \mathbbm{1}_{B}$ for $B\in \mathcal{G}$, then
	\begin{equation}
		LHS = \int_{A\cap B} X d\mathbb{P} = \int_{A\cap B} \mathbb{E}\left[X|\mathcal{G}\right] d\mathbb{P} = RHS
	\end{equation}
	Equal sign in the middle follows the definition, where $A\cap B\in \mathcal{G}$.\newline
	By linearity, dagger holds for all $Z\in S \mathcal{G}^+$ (simple function measurable on $\mathcal{G}$). \newline
	By (\textbf{MON}), holds for all $Z \in (m \mathcal{G})^+$ with $X^{\pm}$.\footnote{we don't know the sign of $X$, so we pose constraint to $X^{\pm}$ such that $XZ$ is positive.}
	\begin{equation}
		|XZ| = (X^++X^-)(Z^++Z^-) <\infty
	\end{equation}
	So $X^{\pm}Z^{\pm} \in \mathcal{L}^1$ for any combinations of plus minus, thus all integrals involved in dagger are finite, by linearity, dagger holds for general $X,Z$. $\blacksquare$
\end{itemize}

%------------------------------------------------------------------------
\subsection{Independence condition}
\begin{itemize}
	\item[\textit{Prop.}] (\textit{Drop the independent sigma algebra}) If $\mathcal{H} \subseteq \mathcal{F}$ is another sub sigma algebra; $\mathcal{H}$ is indep. of $\sigma(\mathcal{G}, \sigma(X))$, then
	\begin{equation}
		\mathbb{E}\left[X|\sigma(\mathcal{G}, \mathcal{H})\right]= \mathbb{E}\left[X|\mathcal{G}\right]~~(\triangle)
	\end{equation}
	In particular, if $\mathcal{H}$ is indep of $\sigma(X)$,
	\begin{equation}
		\mathbb{E}\left[X|\mathcal{H}\right]=\mathbb{E}\left[X\right]
	\end{equation}

	\item[\textit{Proof}.] Define
	\begin{equation}
		\mathcal{I}:=\{G\cap H: G\in \mathcal{G}, H\in \mathcal{H}\}
	\end{equation}
	One can verify that $\mathcal{I}$ is a pi system. Moreover $\sigma(I)=\sigma(\mathcal{G},\mathcal{H})$. Examine eq triangle, we can see that LHS is the conditional expectation of $X$ given $\sigma(\mathcal{G}, \mathcal{H})$. So it suffices to estabilish: for all $A\in \sigma(\mathcal{G},\mathcal{H})$
	\begin{equation}
		\int_A X d\mathbb{P} = \int_A \mathbb{E}\left[X|\mathcal{G}\right] d\mathbb{P}
	\end{equation}
	It futher suffices to show this only on pi system. For $A \in \mathcal{I}$, say $A=G\cap H$ for $G\in \mathcal{G}, H\in \mathcal{H}$. We have
	\begin{equation}
		\begin{split}
			\int_{G \cap H} X d\mathbb{P} &= \int_{\Omega} \mathbbm{1}_{G} \mathbbm{1}_{H} X d\mathbb{P} = \mathbb{E}\left[\mathbbm{1}_{G}\mathbbm{1}_{H}X\right]\\
			&= \mathbb{E}\left[\mathbbm{1}_{H}\right]\cdot \mathbb{E}\left[\mathbbm{1}_{G}X\right]=\mathbb{P}\left(H\right)\int_G X d\mathbb{P} \\
			&= \mathbb{P}\left(H\right) \int_G \mathbb{E}\left[X|\mathcal{G}\right] d\mathbb{P}~~\text{(By definition for $G\in \mathcal{G})$}\\
			&= \int_{G \cap H} \mathbb{E}\left[X|\mathcal{G}\right] d\mathbb{P}
		\end{split}
	\end{equation}
	Apply extension theorem, for $A \in \sigma(\mathcal{I})$, this also holds. $\blacksquare$


	\item[\textit{Prop.}] (\textit{Two coordinates}) Assume $X,Y$ indep RVs, law $\mathcal{L}_X, \mathcal{L}_Y$. $h: \mathbb{R}^2 \to \mathbb{R}$ is Borel function s.t. $h(X,Y)\in \mathcal{L}^1$. Define function $\gamma^h$,
	\begin{equation}
		\gamma^h(x) := \mathbb{E}\left[h(x, Y)\right] 
	\end{equation}
	(taking expectation wrt second coordinate; integrate second coordinate out). Then, $\mathbb{E}\left[h(X,Y)|\sigma(X)\right]=\gamma^h(X)$. ($\gamma^h(X)\in m\sigma(X)$ follows this.)

	\item[\textit{Rm.}] This proposition is saying, for borel function $h(X,Y)$, the best predition of $h$ given $\sigma(X)$ is just integrate $Y$ out.

	\item[\textit{Proof}.] It is sufficient to show that for all $A \in \sigma(X)$ (the preimage set, for $B\in \mathscr{B}(\mathbb{R}), A=\{w: X(w)\in B\}$)
	\begin{equation}
		\int_A h(X,Y) d\mathbb{P} = \int_A \gamma^h(X) d\mathbb{P}
	\end{equation}
	We start from LHS, $A:= \{w\in \Omega: X(w)\in B\}$
	\begin{equation}
		\begin{split}
			\int_{\{w:X(w)\in B\}} h(X,Y) d\mathbb{P} &= \iint_{B\times \mathbb{R}}h(x,y) d \mathcal{L}_{(X,Y)}\\
			&= \iint_{B\times \mathbb{R}}h(x,y) d (\mathcal{L}_X \times  \mathcal{L}_Y)~~\text{(using indep.)}\\
			&= \int_B \left(\int_{\mathbb{R}}h(x,y)d \mathcal{L}_Y  \right) d \mathcal{L}_X\\
			&=\int_B \gamma^h(x) d \mathcal{L}_X = \int_A \gamma^h(X) d\mathbb{P}~~\blacksquare
		\end{split}
	\end{equation}
\end{itemize}

%////////////////////////////////////////////////////////////////////////
\section{Martingale}
\begin{itemize}
	\item[\textit{Def.}] \textbf{Stochastic Process}: A sequence of RVs from initial state $X_0$, $\{X_n: n\geq 0\}$ is called a stochastic process.
\end{itemize}
%------------------------------------------------------------------------
\subsection{Filtration, Adaptedness}
\begin{itemize}
	\item[\textit{Def.}] \textbf{Filtration}: given $(\Omega, \mathcal{F}, \mathbb{P})$. $\{\mathcal{F}_n:n\geq 0\}$ is a filtration if
	\begin{itemize}
		\item[$\cdot$] $\mathcal{F}_n \in \mathcal{F}$ is sub sigma algebra.
		\item[$\cdot$] $\mathcal{F}_n \subseteq \mathcal{F}_{n+1}$ for all $n\geq 0$ (\textit{nested}).
	\end{itemize}

	\item[\textit{Def.}] \textbf{Filtered Space}: The probability space equipped with a filtration structure, i.e. $(\Omega, \mathcal{F}, \{\mathcal{F}_n: n\geq 0\},\mathbb{P})$ is a filtered space.

	\item[\textit{Def.}] \textbf{Adaptedness}: A stochastic process $\{X_n: n\geq 0\}$ on filtered space is adapted if $X_n \in m \mathcal{F}_n$. \newline
	In particular, process $\{X_n\}$ is always adapted wrt the \textit{Natural Filtration} $\{\mathcal{F}_n: n\geq 0\}$, where $\mathcal{F}_n := \sigma(X_0, X_1, ..., X_n)$.
\end{itemize}

%------------------------------------------------------------------------
\subsection{Martingale, Sub/Sup Martingale}
\begin{itemize}
	\item[\textit{Def.}] \textbf{Martingale}: Given $(\Omega, \mathcal{F}, \mathbb{P})$. $\{\mathcal{F}_n:n\geq 0\}$, a adapted process $\{X_n:n\geq 0\}$ is a martingale if
	\begin{itemize}
		\item[$\cdot$] $X_n \in \mathcal{L}^1$.
		\item[$\cdot$] $\mathbb{E}\left[X_{n+1}|\mathcal{F}_n\right] = X_n$, for all $n\geq 0$.
	\end{itemize}

	\item[\textit{Ex}.] Popular examples.
	\begin{itemize}
		\item[$\cdot$] \textit{Partial sum of a indep, $0$-mean sequence of RVs forms a martingale.} Rigorously, $\{Y_n:n\geq 1\}$ is indep, $\mathbb{E}\left[Y_n\right]=0$. $X_0:=0$, $X_n:=\sum_{j=1}^n Y_j$, $\mathcal{F}_0 = \{\Omega, \emptyset\}$, $\mathcal{F}_n:=\sigma(Y_1, Y_2, ..., Y_n)$, then $\{X_n\}$ is martingale wrt $\{\mathcal{F}_n\}$, we can check:
		\begin{equation}
			\mathbb{E}\left[X_{n+1}|\mathcal{F}_n\right]=\mathbb{E}\left[X_n+Y_{n+1}|\mathcal{F}_n\right]=X_n + \mathbb{E}\left[Y_{n+1}\right]=X_n
		\end{equation}

		\item[$\cdot$] In addition to the first example, if $\mathrm{Var}\left[Y_n\right]=1$, then $\{X_n^2 -n : n\geq 0\}$ is martingale. ($X_n^2$ is square of partial sum). On top of this one, if $Y_n$ are i.i.d standard normal, then $\forall \lambda \in \mathbb{R}$, $\{e^{\lambda X_n - \frac{\lambda^2 n}{2}}: n\geq 0\}$ is martingale.

		\item[$\cdot$] If $X \in \mathcal{L}^1$, define $X_n:= \mathbb{E}\left[X|\mathcal{F}_n\right]$, $\{X_n:n\geq 0\}$ is martingale. Check it inserting $X_n$, use tower property:
		\begin{equation}
			\mathbb{E}\left[X_{n+1}|\mathcal{F}_n\right] = \mathbb{E}\left[ \mathbb{E}\left[X|\mathcal{F}_{n+1}\right] |\mathcal{F}_n\right]=\mathbb{E}\left[X|\mathcal{F}_n\right]=:X_n
		\end{equation}
	\end{itemize}

	\item[\textit{Def.}] \textbf{Sub-Martingale}: $\{X_n:n\geq 0\}$ is a sub-martingale if $X_n \in \mathcal{L}^1$ and $\mathbb{E}\left[X_{n+1}|\mathcal{F}_n\right]\geq X_n$. Similarly we define \textbf{Sup-Martingale}: $\mathbb{E}\left[X_{n+1}|\mathcal{F}_n\right]$.

	\item[\textit{Rm.}] Martingale is the model of \textit{fair game}, sub-martingale says the future is better than present, the game is biased for us. Sup martingale says game is biased against us. Given $\{X_n\}$ a Sup-Martingale, than $\{-X_n\}$ is a sub-martingale.

	\item[\textit{Rm.}] (\textit{Any future is same as one step forward}): For (sub) martingale, $\forall m\geq n+1$ (any future), by tower property and definition,
	\begin{equation}
			\mathbb{E}\left[X_m|\mathcal{F}_n\right] = \mathbb{E}\left[\mathbb{E}\left[X_m|\mathcal{F}_{m-1}\right] | \mathcal{F}_n\right]=\mathbb{E}\left[X_{m-1}|\mathcal{F}_n\right]
	\end{equation}
	Repeat this until $\mathcal{F}_{n+1}$, we get $\mathbb{E}\left[X_m|\mathcal{F}_n\right]= X_n$.

	\item[\textit{Thm.}] (\textbf{Composition with Convex Function}) Given $\{X_n:n\geq 0\}$ is adapted, let $\phi: \mathbb{R}\to \mathbb{R}$ convex, such that $\phi(X_n)\in \mathcal{L}^1$ $\forall n\geq 0$. If either
	\begin{itemize}
		\item[$\cdot$] $\{X_n:n\geq 0\}$ is a martingale.
		\item[$\cdot$] $\{X_n:n\geq 0\}$ is a submartingale, $\phi$ is non-decreasing
	\end{itemize}
	Then $\{\phi(X_n):n\geq 0\}$ is a submartingale.

	\item[$Proof.$] By (\textbf{cJensen}), $\forall n\geq 0$:
	\begin{equation}
		\mathbb{E}\left[\phi(X_{n+1})|\mathcal{F}_n\right] \geq \phi(\mathbb{E}\left[X_{n+1}|\mathcal{F}_n\right])=\phi(X_n)~~~\text{For the first condition}.
	\end{equation}
	For the second condition, $\mathbb{E}\left[X_{n+1}|\mathcal{F}_n\right]\geq X_n$, since $\phi$ is non-decreasing, we have $\phi(\mathbb{E}\left[X_{n+1}|\mathcal{F}_n\right])\geq \phi(X_n)$. $\blacksquare$

	\item[\textit{Cor.}] Use thm above:
	\begin{itemize}
		\item[$\cdot$] If $\{X_n: n\geq 0\}$ is martingale, then $\{|X_n|^p: n\geq 0\}$ is a submartingale for all $p\geq 1$.
		\item[$\cdot$] If $\{X_n: n\geq 0\}$ is submartingale, then $\{X_n^+: n\geq 0\}$ is submartingale.
		\item[$\cdot$] If $\{X_n: n\geq 0\}$ is \textit{non-negative} submartingale, then $\{X_n^p: n\geq 0\}$ is submartingale.
	\end{itemize}

	\item[\textit{Proof}.] First one is clear. For the second one, view $\phi$ as $X_n^+ = \mathbbm{1}_{(0,+\infty)}X_n$. non-decreasing. Same argument for third.
\end{itemize}

%------------------------------------------------------------------------
\subsection{Doob's Decomposition Thm}
\begin{itemize}
	\item[\textit{Thm.}] (\textbf{Doob Decomposition}) $\{X_n: n\geq 0\}$ is a submartingale, then there exists a process $\{Y_n: n\geq 0\}$ such that 
	\begin{itemize}
		\item[$\cdot$] $Y_0=0$, $Y_n \in \mathcal{L}^1$, $Y_{n+1} \in m \mathcal{F}_n$ forall $n\geq0$, i.e. $\{Y_n: n\geq 0\}$ is a \textbf{previsable} process. ($Y_{n+1}$ is known at $n$).
		\item[$\cdot$] $Y_n$ is non-decreasing, i.e. $Y_n\leq Y_{n+1}$ a.s.
		\item[$\cdot$] $M_n:=X_n-Y_n$ is a martingale.
		\item[$\cdot$] If $Y_n$ exists, it's unique.
	\end{itemize}

	\item[\textit{Proof}.] First show the uniqueness. Assume $Y_n$ exists, assume not unique, i.e. exists another $\{W_n: n\geq 0\}$ also satisfies 1,2,3. Define $\Delta:=Y_n-W_n$, clearly $\Delta_0=0$. Manipulate $\Delta_n$:
	\begin{equation}
		\Delta_n = Y_n - W_n = (X_n-W_n)-(X_n-Y_n)
	\end{equation}
	By linearity, and by (3), $\Delta_n$ is martingale. Hence $\Delta_n = \mathbb{E}\left[\Delta_{n+1}|\mathcal{F}_n\right]$. However since $Y_{n+1},W_{n+1}\in m \mathcal{F}_n$, $\Delta_n$ is also previsible, $\Delta_{n+1}\in m \mathcal{F}_n$.
	\begin{equation}
		\Delta_n = \mathbb{E}\left[\Delta_{n+1}|\mathcal{F}_n\right] = \Delta_{n+1} = ... = \Delta_0 \equiv 0~~\blacksquare
	\end{equation}
	(We can come up with a remark: if a process is a martingale and also previsible, then it is a constant.)\newline
	Now show the existance of $Y_n$. $Y_0=0$, for $n\geq 0$, define
	\begin{equation}
		Y_{n+1}:=\sum_{j=0}^n \left(\mathbb{E}\left[X_{j+1}|\mathcal{F}_j\right]-X_j \right)
	\end{equation}
	The increment part of submartingale. Since $\mathbb{E}\left[X_{j+1}|\mathcal{F}_j\right]\in m \mathcal{F}_j$, clearly $Y_{n+1}\in m \mathcal{F}_n$. By property of submartingale, every term in the summation is positive, so $Y_n \leq Y_{n+1}$. Now only need to prove $X_n - Y_n$ is martingale.
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[X_{n+1}-Y_{n+1}|\mathcal{F}_n\right]&=\mathbb{E}\left[X_{n+1}-\sum_{j=0}^n \left(\mathbb{E}\left[X_{j+1}|\mathcal{F}_j\right]-X_j \right)|\mathcal{F}_n\right]\\
			&= \mathbb{E}\left[X_{n+1}|\mathcal{F}_n\right]-\sum_{j=0}^n \left(\mathbb{E}\left[X_{j+1}|\mathcal{F}_j\right]-X_j \right)\\
			&=\mathbb{E}\left[X_{n+1}|\mathcal{F}_n\right]-(\mathbb{E}\left[X_{n+1}|\mathcal{F}_n\right]-X_n)-\sum_{j=0}^{n-1} \left(\mathbb{E}\left[X_{j+1}|\mathcal{F}_j\right]-X_j \right)\\
			&= X_n - \sum_{j=0}^{n-1} \left(\mathbb{E}\left[X_{j+1}|\mathcal{F}_j\right]-X_j \right)=X_n-Y_n~~~\blacksquare
		\end{split}
	\end{equation}
\end{itemize}

%////////////////////////////////////////////////////////////////////////
\section{Stopping Time}
\begin{itemize}
	\item[\textit{Def.}] \textbf{Stopping Time}: Given $(\Omega, \mathcal{F}, \{\mathcal{F}_n\}, \mathbb{P})$. A Random variable $\tau: \Omega \to \{0,1,2,..., \infty\}$ is a stopping time if $\{\tau\leq n\}\in \mathcal{F}_n$. Using $\{\tau>n\},\{\tau<n\},\{\tau\geq n\},\{\tau=n\}$ are equivalent, if $\tau$ is a stopping time, all these sets $\in \mathcal{F}_n$.

	\item[\textit{Def.}] (\textit{Sigma algebra with stopping time subscript}): $\tau$ is a stopping time, $\mathcal{F}_{\tau}:=\{A\in \mathcal{F}, A\cap \{\tau \leq n\}\in \mathcal{F}_n~~\forall n\geq 0\}$. One can verify that $\mathcal{F}_{\tau}$ is a sigma algebra, but note that $\mathcal{F}_{\tau}\ne \sigma(\tau)$.

	\item[\textit{Def.}] (\textit{RV with stopping time subscript}): $\{X_n: n\geq 0\}$ is adapted, for $w\in \Omega$ define
	\begin{equation*}
		X_{\tau}(w):=X_{\tau(w)}(w) := \begin{cases}
		X_n(w) &\text{if $\tau(w)=n< +\infty$}\\
		\begin{cases}
		\lim\limits_{n\rightarrow\infty}X_n(w) &\text{if $X_n$ admits limit}\\
		\text{undefined} &\text{if limit does not exist}
		\end{cases} &\text{if $\tau(w)=+\infty$}
		\end{cases}
	\end{equation*}
\end{itemize}

%------------------------------------------------------------------------
\subsection{Simple Properties of Stopping Time}
\begin{itemize}
	\item[\textit{Prop.}]
	\begin{itemize}
		\item[1$\cdot$] If $\tau$ is stopping time, $n$ is any fixed positive integer, then $\tau \wedge n:=\min\{\tau, n\}$ is a stopping time.
		\item[2$\cdot$] If $\tau_1, \tau_2$ are stopping times, then $(\tau_1 \wedge \tau_2)$, $(\tau_1 + \tau_2)$, $(\tau_1 \vee \tau_2)$ are all stopping times.
		\item[3$\cdot$] If $\{X_n: n\geq 0\}$ is adapted, $\mathbb{P}\left(\tau<\infty\right)=1$ then $X_{\tau}\in m \mathcal{F}_{\tau}$.
		\begin{itemize}
			\item[$Proof.$] Since $\tau <\infty~a.s$, $X_{\tau}$ is defined a.s. Working out only a pi system is enough, for all $x\in \mathbb{R}$, we want to show that $\{X_{\tau}\leq x\}\in \mathcal{F}_{\tau}$. By definition
			\begin{equation}
				\{X_{\tau}\leq x\}\cap \{\tau\leq n\}= \bigcup_{j=0}^{n}\{\tau=j, X_j\leq x\}\in \mathcal{F}_n~~\blacksquare
			\end{equation}
		\end{itemize}
		\item[4$\cdot$] $\tau_1, \tau_2$ are stopping times, then $\mathcal{F}_{\tau_1 \wedge \tau_2}=\mathcal{F}_{\tau_1}\cap \mathcal{F}_{\tau_2}$.
		\item[5$\cdot$] $\tau_1, \tau_2$ are stopping times, deterministically $\tau_1\leq \tau_2$, then $\mathcal{F}_{\tau_1} \subseteq \mathcal{F}_{\tau_2}$.
		\begin{itemize}
			\item[\textit{Proof.}] First show (5). $\forall A \in \mathcal{F}_{\tau_1}$, It suffices to show that $A\in \mathcal{F}_{\tau_2}$,i.e. $\forall n\in \bar{\mathbb{N}}$, $A \cap \{\tau_2 \leq n\}\in \mathcal{F}_{n}$. This is true, since $\{\tau_2 \leq n\} \subseteq \{\tau_1 \leq n\}$,
			\begin{equation}
				A \cap \{\tau_2 \leq n\}=A\cap \{\tau_1 \leq n\}\cap \{\tau_2 \leq n\}\in \mathcal{F}_{n}
			\end{equation}
			Because $A \cap \{\tau_1 \leq n\}\in \mathcal{F}_n$. $\blacksquare$\newline
			For (4), $LHS \subseteq RHS$ is clear, since $LHS \subseteq \mathcal{F}_{\tau_1}, \mathcal{F}_{\tau_2}$. Only need to show ($\supseteq$) for all $A \in \mathcal{F}_{\tau_1}\cap \mathcal{F}_{\tau_2}$, 
			\begin{equation}
				\begin{split}
					A \cap \{\tau_1 \wedge \tau_2\leq n\}&=A \cap (\{\tau_1\leq n\}\cup \{\tau_2\leq n\})\\
					&= (A\cap \{\tau_1\leq n\}) \cup (A\cap\{\tau_2 \leq n\})
				\end{split}
			\end{equation}
		\end{itemize}
		\item[6$\cdot$] $\{X_n: n\geq 0\}$ is adapted, $\tau$ is stopping time, then $\{X_{\tau \wedge n}: n\geq 0\}$ is also an adapted process.
	\end{itemize}
\end{itemize}

%------------------------------------------------------------------------
\subsection{Doob's Stopping Time Thm}
\begin{itemize}
	\item[\textit{Thm.}] (\textbf{Doob}) If $\{X_n: n\geq 0\}$ is a martingale/submartingale, $\tau$ is a stopping time, then $\{X_{\tau \wedge n}: n\geq 0\}$ is still a martingale/submartingale.

	\item[\textit{Proof}.] Clearly, $X_{n \wedge \tau}\in \mathcal{L}^1$, because $X_{n \wedge \tau}=\sum_{j=0}^n \mathbbm{1}_{(\tau=j)}X_j+\mathbbm{1}_{(\tau>n)}X_n$. Now we show $\{X_{\tau \wedge n}: n\geq 0\}$ is a martingale. Concretely, we want to show $\mathbb{E}\left[X_{(n+1)\wedge \tau}|\mathcal{F}_n\right]=X_{n \wedge \tau}$. For all $A \in \mathcal{F}_n$,
	\begin{equation}
		\begin{split}
			\int_A X_{(n+1) \wedge \tau} d\mathbb{P} &= \int_{A\cap \{\tau\leq n\}} X_{\tau} d\mathbb{P} + \int_{A \cap \{\tau>n\}} X_{n+1} d\mathbb{P}\\
			&= \int_{A\cap \{\tau\leq n\}} X_{\tau} d\mathbb{P} + \int_{A \cap \{\tau>n\}} X_{n+1} d\mathbb{P}~~\text{(since $X_n$ is martingale)}\\
			&= \int_A X_{\tau \wedge n} d\mathbb{P}~~\blacksquare
		\end{split}
	\end{equation}
\end{itemize}

%------------------------------------------------------------------------
\subsection{Hunt's Thm}
\begin{itemize}
	\item[\textit{Thm.}] (\textbf{Hunt}) $\{X_n: n\geq 0\}$ is a martingale/submartingale. $\tau_1, \tau_2$ are stopping times, $\tau_1 \leq \tau_2$. If one of following conditions holds
	\begin{itemize}
		\item[$\cdot$] $\tau_1, \tau_2$ are bounded, i.e. $\exists T>0$, $\tau_1, \tau_2 \leq T$. 
		\item[$\cdot$] $\{X_n: n\geq 0\}$ is uniformly integrable. And $\tau_1, \tau_2$ are finite a.s.
		\item[$\cdot$] $\mathbb{E}\left[\tau_1\right] \leq \mathbb{E}\left[\tau_2\right]<\infty$. And exists constant $k>0$, s.t. $|X_{n+1}-X_n|\leq k$, $\forall n \geq0$.
	\end{itemize}
	Then, $\mathbb{E}\left[X_{\tau_2}|\mathcal{F}_{\tau_1}\right]=X_{\tau_1}$. ($\geq$ for submartingale case)
\end{itemize}

%------------------------------------------------------------------------
\subsection{Wald's Identity}

%////////////////////////////////////////////////////////////////////////
\section{Random Walk}

%////////////////////////////////////////////////////////////////////////
\section{Martingale Convergence}

%------------------------------------------------------------------------
\subsection{Doob's Upcrossing Inequility}

%------------------------------------------------------------------------
\subsection{Martingale Convergence Thm 1 (MCT1)}

%------------------------------------------------------------------------
\subsection{Martingale Convergence Thm 2 (MCT2)}

%------------------------------------------------------------------------
\subsection{Doob's Maximal Inequility}

%------------------------------------------------------------------------
\subsection{Martingale Convergence Thm 3 (MCT3)}

%------------------------------------------------------------------------
\subsection{Converse MCT2}

%------------------------------------------------------------------------
\subsection{Generalized 0-1 Law}





%------------------------------------------------------------------------
%////////////////////////////////////////////////////////////////////////
%////////////////////////////////////////////////////////////////////////
%------------------------------------------------------------------------
\chapter{Problems}

\begin{itemize}
	\item[]\textbf{Problem 1.} (\textbf{Equivalent Generating pi of Borel on Real Line}) Show that
	\begin{equation}
	\begin{split}
		\mathscr{B}(\mathbb{R})&=\sigma(\{[a,b): a,b\in \mathbb{R}, a<b\})\\
		&=\sigma(\{[a,b]: a,b\in \mathbb{R}, a<b\})\\
		&=\sigma(\{(-\infty,x): x\in \mathbb{Q}\})\\
		&=\sigma(\{(-\infty,x]: x\in \mathbb{Q}\})\\
	\end{split}
	\end{equation}

	\item[] \textit{Proof.} Clearly, RHS $\subseteq \mathscr{B}(\mathbb{R})$. It's sufficient to show $\supseteq$. The target is to rewrite original pi $(a,b)$ to be these 4 alternative pi. But for the first one we just show both.\newline.
	\begin{equation}
		\begin{split}
			(a,b)&=\bigcup_{n\geq 1}[a+\frac{1}{n},b]~~ \Rightarrow \mathscr{B}(\mathbb{R})\subseteq RHS1\\
			[a,b)&=\bigcap_{n\geq 1}[a, b+\frac{1}{n}]~~\Rightarrow RHS1 \subseteq \mathscr{B}(\mathbb{R})
		\end{split}
	\end{equation}
	\begin{equation}
		(a,b)=\bigcup_{n\geq 1}[a+\frac{1}{n},b-\frac{1}{n}] \Rightarrow \mathscr{B}(\mathbb{R}) \subseteq RHS2
	\end{equation}
	3 and 4; For any $a\in \mathbb{R}$, $\exists \{q_n\}, n\geq 1$ be a seq of rationals s.t. $q_n \nearrow a$ (increasingly) So,
	\begin{equation}
		(-\infty, a)=\bigcup_{n\geq 1}(-\infty, q_n)\nwarrow (-\infty, q_n)
	\end{equation}
	Therefore we also find $p_n\nearrow b$, $\{p_n\}\subseteq \mathbb{Q}$:
	\begin{equation}
		\begin{split}
			[a,b)&=(-\infty,b)\setminus(-\infty, a)\\
			&=\bigcup_{n\geq 1}(-\infty, q_n) \cap \left(\bigcup_{n\geq 1}(-\infty, q_n)\right)^c
		\end{split}
	\end{equation}
	Implies $RHS3 \subseteq RHS1 \subseteq \mathscr{B}(\mathbb{R})$. For 4:
	\begin{equation}
		(-\infty,x) = \bigcup_{n\geq 1}(-\infty, x-\frac{1}{n})~~\Rightarrow RHS4 \subseteq RHS3 \subseteq \mathscr{B}(\mathbb{R}
	\end{equation}
\end{itemize}


\begin{itemize}
	\item[]\textbf{Problem 2.} (\textbf{Singletons are not enough to generate Borel sigma}) Show $\mathscr{B}(\mathbb{R})$ is not generated by all singletons of $\mathbb{R}$. I.e show that
	\begin{equation}
		\mathscr{B}(\mathbb{R})\ne \sigma(\{x\}, x\in \mathbb{R}):= \mathcal{S}
	\end{equation}

	\item[]\textit{Proof.} Define
	\begin{equation}
		\begin{split}
			\mathcal{A}&:= \{\emptyset\}\cup \{\bigcup_{n\geq 1}\{r_n\}: r_n \in \mathbb{R}\}\\
			\mathcal{B}&:= \{B\in \mathbb{R}: B^c \in \mathcal{A}\}
		\end{split}
	\end{equation}
	i.e. $\mathcal{A}$ is collection of countable unions of singletons. $\mathcal{B}$ is collection of complements of things in $\mathcal{A}$. We claim that $\Sigma:= \mathcal{A}\cup \mathcal{B}$ is a sigma-field.
	\begin{itemize}
		\item[$\cdot$] $\emptyset\in \Sigma$.
		\item[$\cdot$] $\forall A\in \Sigma$, $A^c \in \Sigma$.
		\item[$\cdot$] Consider countablly many $A_n\in \Sigma, n\geq 1$. $A_n$ should be either in $\mathcal{A}$ or $\mathcal{B}$. Denote $\mathcal{I}:=\{i: A_i \in \mathcal{A}\}$; $\mathcal{J}:=\{j: A_j \in \mathcal{B}\}$ as indices sets marking whether collection $A_n$ belongs. Then,
		\begin{equation}
			\bigcup_{n\geq 1}A_n = \left(\bigcup_{i\in \mathcal{I}}A_i\right) \cup \left(\bigcup_{j\in \mathcal{J}}A_j\right)=: U_1\cup U_2=:U
		\end{equation}
		where $U_1 \in \mathcal{A} \subseteq \Sigma$. $U_2 = (\bigcap_{j\in \mathcal{J}}A_j^c)^c$, $\bigcap_{j\in \mathcal{J}}A_j \in \mathcal{A}$. So $U_2 \in \mathcal{B}\subseteq \Sigma$. So $U\in \mathcal{A}\cup \mathcal{B}=\Sigma$. \textbf{Check: $\Sigma$ is a sigma field}.
	\end{itemize}
	Clearly all singletions contained in $\mathcal{A}$, therefore $\Sigma$. So $\sigma(\{{x}: x\in \mathbb{R}\}) \subseteq \Sigma$. But $\mathscr{B}(\mathbb{R}) \supset (0,1) \notin \Sigma$. $\blacksquare$
\end{itemize}


\begin{itemize}
	\item[]\textbf{Problem 3.} (\textbf{Defining properties of Measure}) $S=(0,1]$, define
	\begin{equation}
		\Sigma := \left\{\bigcup_{i=1}^{k}(a_i, b_i]: k\in \mathbb{N}, 0\leq a_1 \leq b_1 \leq a_2 \leq ... \leq a_k \leq b_k \leq 1\right\} 
	\end{equation}
	(Shown) $\Sigma$ is sigma field. Define $\mu: \Sigma \mapsto [0, \infty]$, for $A\in \Sigma$,
	\begin{equation}
		\mu(A)=
		\begin{cases}
		&1~~\text{if $A\supseteq (\frac{1}{2}, \frac{1}{2}+\epsilon]$ for some $\epsilon>0$},\\
		&0~~\text{otherwise}
	\end{cases}
	\end{equation}
	Show (1) $\mu$ is finite additive. (2) $\mu$ is not countable additive.

	\item[]\textit{Proof.} For $A_n\in \Sigma, n=1,2,...,N$. $A_n$ disjoint. \newline
	Then there is at most one $A_k$ s.t. $A_k \supseteq (\frac{1}{2}, \frac{1}{2}+\epsilon]$ i.e. $\mu(A_k)=1$ and $\mu(A_j)=0$ for $j\ne k$. Clearly $\mu(\bigcup_{j=1}^{N}A_j)=\sum_{j=1}^N \mu(A_j)$. $\blacksquare$ \newline
	For the second part, it suffices to show $\mu$ is not continous (from above) at emptyset.\newline
	Pick $\{A_n\}$, $A_n:=(\frac{1}{2}, \frac{1}{2}+\frac{1}{n}]$. Clearly $A_n \searrow \emptyset$. But $\epsilon:=\frac{1}{2n}$ for any $n\geq 1$, $\mu(A_n)\equiv1$. $\blacksquare$
\end{itemize}


\begin{itemize}
	\item[]\textbf{Problem 4.} (\textbf{Indep.}) $S=(0,1]$,
\end{itemize}

%////////////////////////////////////////////////////////////////////////
\section{Prob Space}

%////////////////////////////////////////////////////////////////////////
\section{RV}

%////////////////////////////////////////////////////////////////////////
\section{Expectation}
%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 1.} On $(S, \Sigma, \mu)$ $f_n, g_n\in \mathcal{L}^1(S, \Sigma, \mu)$. $|f_n|\leq g_n$ for all $n\geq 1$. $\forall s\in S$, $f_n\to f$, $g_n\to g$. 
	\begin{itemize}
		\item[] Show that if $\mu(g_n)\to \mu(g)<\infty$, \textit{then} $\mu(f)$ is defined, and $\mu(f_n)\to \mu(f)$
	\end{itemize}
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[]\textit{Proof.} $|f_n|\leq g_n \Rightarrow g_n+f_n\geq0$ and $g_n-f_n \geq 0$. \newline
	Apply (\textbf{FATOU}), and by linearly of Fatou's LHS:
	\begin{equation}
	\begin{split}
		\mu(g)+\mu(f)&=\mu(\liminf\limits_{n\rightarrow\infty}(g_n+f_n))\leq \liminf\limits_{n\rightarrow\infty}\mu(g_n+f_n)\\
		&=\mu(g)+\liminf\limits_{n\rightarrow\infty}\mu(f_n)
	\end{split}
	\end{equation}
	\begin{equation}
	\begin{split}
		-\mu(g)+\mu(f)&=-\mu(\liminf\limits_{n\rightarrow\infty}(g_n-f_n))\geq -\liminf\limits_{n\rightarrow\infty}\mu(g_n-f_n)\\
		&=-\mu(g)+\limsup\limits_{n\rightarrow\infty}\mu(f_n)
	\end{split}
	\end{equation}
	Since $g\in \mathcal{L}^1$, $\mu(g)$ can be cancelled out from both sides:
	\begin{equation}
		\liminf\limits_{n\rightarrow\infty}\mu(f_n) \leq \limsup\limits_{n\rightarrow\infty}\mu(f_n)\leq \mu(f)\leq \liminf\limits_{n\rightarrow\infty}\mu(f_n)
	\end{equation}
	Therefore $\mu(f):=\liminf\limits_{n\rightarrow\infty}\mu(f_n)$ is defined. Moreover $\lim\limits_{n\rightarrow\infty}\mu(f_n)$ exists, and $\mu(f)=\lim\limits_{n\rightarrow\infty}\mu(f_n)$. $\blacksquare$
\end{itemize}

%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 2.} $(\Omega, \mathcal{F}, \mathbb{P})$, $X_n,X\in \mathcal{L}^1$, $X_n \xrightarrow{i.p} X$, $\mathbb{E}\left[X_n\right]\to \mathbb{E}\left[X\right]$, show that
	$$X_n \xrightarrow{\mathcal{L}^1} X$$
	(Strengthened \textbf{SCHEFFE})
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[]\textit{Proof.} Assume opposite, NOT $X_n \xrightarrow{\mathcal{L}^1}X$, i.e. $\exists \epsilon>0$ and subsequence $\{n_l\}$, such that $\mathbb{E}\left[X_{n_l}-X\right]\geq \epsilon$. (\#)\newline
	Clearly $X_{n_l} \xrightarrow{i.p} X$. By theorem, there exists a further subsequence ${X_{n_{l_m}}}$ such that ${X_{n_{l_m}}} \xrightarrow{a.s.} X$. Moreover $\mathbb{E}\left[{X_{n_{l_m}}}\right]\xrightarrow{m\to \infty} \mathbb{E}\left[X\right]$ and ${X_{n_{l_m}}} \in \mathcal{L}^1$ for any subscript, because $\{X_{n_{l_m}}\} \subseteq \{X_n\}$. \newline
	Apply original (\textbf{Scheffe}) to ${X_{n_{l_m}}}$, we have ${X_{n_{l_m}}} \xrightarrow{\mathcal{L}^1} X$, i.e. $\forall \epsilon>0$, $\exists M$, for all $n>M$, $\mathbb{E}\left[{X_{n_{l_m}}}-X\right]<\epsilon$, which contradicts (\#). $\blacksquare$
\end{itemize}

%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 3.} 
	$$\{X_n\},\{Y_n\} \text{ are uniformly integrable }\Rightarrow
	\{X_n+Y_n\}\text{ is uniformly integrable } $$
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[]\textit{Proof.} For $M>0$, consider:
	\begin{equation}
		\begin{split}
			&\sup\limits_{n}\mathbb{E}\left[|X_n+Y_n|;|X_n+Y_n|>M\right]\\
			\leq&\sup\limits_{n}\mathbb{E}\left[|X_n+Y_n|;|X_n|>\frac{M}{2}~\&~|Y_n|>\frac{M}{2}~\&~|X_n+Y_n|>M\right]+\\
			&\sup\limits_{n}\mathbb{E}\left[|X_n+Y_n|;|X_n|\leq \frac{M}{2}~\&~|Y_n|>\frac{M}{2}~\&~|X_n+Y_n|>M\right]+\\
			&\sup\limits_{n}\mathbb{E}\left[|X_n+Y_n|;|X_n|> \frac{M}{2}~\&~|Y_n|\leq\frac{M}{2}~\&~|X_n+Y_n|>M\right]
		\end{split}
	\end{equation}
	In which first term $\leq \sup\limits_{n}\mathbb{E}\left[|X_n|;|X_n|>\frac{M}{2}\right]+\sup\limits_{n}\mathbb{E}\left[|Y_n|;|Y_n|>\frac{M}{2}\right]$, \newline
	Second term $\leq 2 \sup\limits_{n}\mathbb{E}\left[|Y_n|;|Y_n|>\frac{M}{2}\right]$, \newline
	Third term $\leq 2 \sup\limits_{n}\mathbb{E}\left[|X_n|;|X_n|>\frac{M}{2}\right]$.
	\begin{equation}
	\begin{split}
		LHS &\leq 3\sup\limits_{n}\mathbb{E}\left[|X_n|;|X_n|>\frac{M}{2}\right]+3\sup\limits_{n}\mathbb{E}\left[|Y_n|;|Y_n|>\frac{M}{2}\right]\\
		&\xrightarrow{M\to \infty}3\times0+3\times0=0~~\blacksquare
	\end{split}
	\end{equation}
\end{itemize}

%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 4.} Non-trivial RV $X$ ($\mathbb{P}(X>0)>0$). Show that if $X\in \mathcal{L}^2 (\Omega, \mathcal{F}, \mathbb{P})$, then for every $\lambda \in[0,1]$,
	$$\mathbb{P}(|X|\geq \lambda \mathbb{E}\left[|X|\right])\geq \frac{(1-\lambda)^2 \mathbb{E}^2\left[|X|\right]}{\mathbb{E}\left[X^2\right]}$$
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[]\textit{Proof.} Consider
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[|X|\right] &= \mathbb{E}\left[|X|\cdot 1; |X|\geq\lambda \mathbb{E}\left[|X|\right]\right] + \mathbb{E}\left[|X|;|X|< \lambda\mathbb{E}\left[|X|\right]\right]\\
			&\leq \mathbb{E}^{\frac{1}{2}}\left[X^2;|X|\geq\lambda \mathbb{E}\left[|X|\right]\right]\cdot \mathbb{E}^{\frac{1}{2}}\left[1^2;|X|\geq\lambda \mathbb{E}\left[|X|\right]\right]+\lambda \mathbb{E}\left[|X|\right]\\
			&\leq \mathbb{E}^{\frac{1}{2}}\left[X^2\right]\cdot \mathbb{P}^{\frac{1}{2}}\left(|X|\geq \lambda \mathbb{E}\left[X\right] \right)+\lambda \mathbb{E}\left[|X|\right]\\
		\end{split}
	\end{equation}
	Where the first leq applys (Holders) ineq. Rearrange terms we have
	\begin{equation}
		(1-\lambda) \mathbb{E}\left[X\right]\leq \mathbb{E}^{\frac{1}{2}}\left[X^2\right] \mathbb{P}^{\frac{1}{2}}\left(|X|\geq\lambda \mathbb{E}\left[|X|\right]\right)
	\end{equation}
	Take square both sides,
	\begin{equation}
		\mathbb{P}\left(|X|\geq\lambda \mathbb{E}\left[|X|\right]\right)\geq \frac{(1-\lambda)^2 \mathbb{E}^2\left[|X|\right]}{\mathbb{E}\left[X^2\right]}~~~\blacksquare
	\end{equation}
\end{itemize}

%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 5.} $\{X_n\}\in \mathcal{L}^2$; suppose $\mathbb{E}\left[X_iX_j\right]=0$ for $i\ne j$, and $\sup\limits_{n}\mathbb{E}\left[X^2_n\right]<\infty$. Show that for every $\alpha>\frac{1}{2}$:
	$$\frac{\sum_{j=1}^nX_j}{n^{\alpha}}\xrightarrow{i.p}0$$
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[]\textit{Proof.} By (\textbf{Markov}):
	\begin{equation}
		\begin{split}
			\mathbb{P}\left(\left|\frac{S_n}{n^{\alpha}}-0\right|>\epsilon\right)&=\mathbb{P}\left(\left(\frac{S_n}{n^{\alpha}}\right)^2>\epsilon^2\right)\\
			&<\epsilon^{-2}\mathbb{E}\left[\frac{S_n^2}{n^{2\alpha}}\right]\\
			&=\epsilon^{-2}n^{-2\alpha}\cdot\mathbb{E}\left[\sum_{j=1}^nX_n^2+\sum_{i\ne j}X_iX_j\right]\\
			&\leq\epsilon^{-2}n^{-2\alpha}\cdot n \sup\limits_{n}\mathbb{E}\left[X_n^2\right]\\
			&=n^{1-2\alpha}\frac{\sup\limits_{n}\mathbb{E}\left[X_n^2\right]}{\epsilon^2}
		\end{split}
	\end{equation}
	Since $\sup\limits_{n}\mathbb{E}\left[X_n^2\right]<\infty$, we conclude that for all $\epsilon>0$, if $\alpha>1/2$, eq (4.11)$\xrightarrow{n\to \infty}0$; i.e.
	\begin{equation}
		\lim\limits_{n\rightarrow\infty}\mathbb{P}\left(\left|\frac{S_n}{n^{\alpha}}-0\right|>\epsilon\right)=0
	\end{equation}
	We conclude that $\frac{S_n}{n^{\alpha}}\xrightarrow{i.p}0$. $\blacksquare$
\end{itemize}

%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 6.} $\{X_n\}$: identically distributed RV. $\mathbb{E}\left[X_1^2\right]<\infty$.
	\newline
	Show: (1) for all $\epsilon>0$:
	$$\lim\limits_{n\rightarrow\infty}n\cdot \mathbb{P}\left(|X_1|\geq \epsilon\sqrt{n}\right)=0$$
	(2):
	$$\frac{1}{\sqrt{n}}\max\limits_{1\leq k\leq n}|X_k| \xrightarrow{i.p}0$$
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[(1)]\textit{Proof.} $X_1^2\in \mathcal{L}^1 \Rightarrow \lim\limits_{n\rightarrow\infty}\mathbb{E}\left[X_1^2;X_1^2>n\right]=\lim\limits_{n\rightarrow\infty}\mathbb{E}\left[X_1^2; |X_1|>\sqrt{n}\right]=0$.
	To be precise, $\forall \delta>0$, $\exists N$ large, s.t. $\forall n>N$:  $\mathbb{E}\left[X_1^2; |X_1|>\sqrt{n}\right]<\delta$. So, for \textbf{Any Fixed} $\epsilon>0$, $\exists N'=\frac{N}{\epsilon^2}$ s.t. $\forall n>N'$:
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[X_1^2; |X_1|>\epsilon\sqrt{n}\right]&<\mathbb{E}\left[X_1^2; |X_1|>\epsilon\sqrt{\frac{N}{\epsilon^2}}\right]\\
			&=\mathbb{E}\left[X_1^2; |X_1|>\sqrt{N}\right]\leq \delta
		\end{split}
	\end{equation}
	i.e. for \textbf{every fixed} $\epsilon>0$, $\lim\limits_{n\rightarrow\infty}\mathbb{E}\left[X_1^2; |X_1|>\epsilon\sqrt{n}\right]=0$.
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[X_1^2; |X_1|>\epsilon\sqrt{n}\right]
			&=\int_{|X_1|>\epsilon\sqrt{n}}X_1^2 d \mathbb{P}\\
			&>(\epsilon\sqrt{n})^2\cdot \mathbb{P}\left(|X_1|>\epsilon\sqrt{n}\right)
		\end{split}
	\end{equation}
	i.e.
	\begin{equation}
		n\cdot\mathbb{P}\left(|X_1|>\epsilon\sqrt{n}\right)<\epsilon^{-2}\cdot\mathbb{E}\left[X_1^2; |X_1|>\epsilon\sqrt{n}\right]
	\end{equation}
	Let $n\to \infty$, we get $\lim\limits_{n\rightarrow\infty}n\cdot\mathbb{P}\left(|X_1|>\epsilon\sqrt{n}\right)<\epsilon^{-2}\cdot0=0$ as desired. $\blacksquare$

	\item[(2)]\textit{Proof.} For any fixed $\epsilon$, by the fact that $\{X_n\}$ have same law: $\mathbb{P}\left(X_k>c\right)=\mathbb{P}\left(X_1>c\right)$ for all $c\in \mathbb{R}$, all $1\leq k\leq n$.
	\begin{equation}
		\begin{split}
			\mathbb{P}\left(\left|\frac{1}{\sqrt{n}}\max\limits_{1\leq k\leq n}|X_k|-0\right|>\epsilon\right)
			&=\mathbb{P}\left(\max\limits_{1\leq k\leq n}|X_k|>\epsilon\sqrt{n}\right)\\
			&=\mathbb{P}\left(\{X_k>\epsilon\sqrt{n}~\text{for some }1\leq k\leq n\}\right)\\
			&=\mathbb{P}\left(\bigcup_{k=1}^{n}\{X_k>\epsilon\sqrt{n}\}\right)\\
			&\leq \sum_{k=1}^n \mathbb{P}\left(X_k>\epsilon\sqrt{n}\right)\\
			&= n\cdot \mathbb{P}\left(X_1>\epsilon\sqrt{n}\right) \\
			&\xrightarrow{n\to \infty,\text{By (1)'s result}}0~~\blacksquare
		\end{split}
	\end{equation}
\end{itemize}

%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 7.} $\{X_n\}$ seq of indep. RVs. $\mathbb{E}\left[X_n\right]=0$, $\mathrm{Var}\left[X\right]=1$ uniformly. Show that for every $Y\in \mathcal{L}^2$, 
	$$\mathbb{E}\left[X_nY\right]\to 0$$
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[]\textit{Proof.} By $\mathbb{E}\left[X\right]=0, \mathrm{Var}\left[X\right]=1 \Rightarrow \mathbb{E}\left[X^2\right]=1$.\newline
	Define $Y_n:=\sum_{k=1}^n \mathbb{E}\left[X_kY\right]X_k$, $\forall n\geq 1$, consider second moment
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[Y_n^2\right] 
			&= \mathbb{E}\left[\sum_{k=1}^n \mathbb{E}^2\left[X_kY\right]X_k^2 + \sum_{1 \leq i\ne j \leq n } \mathbb{E}\left[X_iY\right] \mathbb{E}\left[X_jY\right]X_iX_j\right]\\
			&= \sum_{k=1}^n \mathbb{E}^2\left[X_kY\right] + \sum_{1 \leq i\ne j \leq n } \mathbb{E}\left[X_iY\right] \mathbb{E}\left[X_jY\right]\mathbb{E}\left[X_i\right] \mathbb{E}\left[X_j\right]\\
			&= \sum_{k=1}^n \mathbb{E}^2\left[X_kY\right]
		\end{split}
	\end{equation}
	Which follows that $\{X_n\}$ are independent, $\mathbb{E}\left[X_iX_j\right]=\mathbb{E}\left[X_i\right]\mathbb{E}\left[X_j\right]$ for $i \ne j$. \newline
	Now it suffices to show that $\mathbb{E}\left[Y_n^2\right]<\infty$ when $n\to \infty$, i.e. $\sup\limits_{n}\mathbb{E}\left[Y_n^2\right]<\infty$.\newline
	Consider
	\begin{equation}
		\mathbb{E}\left[YY_n\right]=\mathbb{E}\left[Y\sum_{k=1}^n \mathbb{E}\left[X_kY\right]X_k\right]=\sum_{k=1}^n \mathbb{E}^2\left[X_kY\right]=\mathbb{E}\left[Y_n^2\right]
	\end{equation}
	And
	\begin{equation}
		\begin{split}
			0&\leq \mathbb{E}\left[(Y-Y_n)^2\right]
			= \mathbb{E}\left[Y^2\right]-2 \mathbb{E}\left[YY_n\right] + \mathbb{E}\left[Y_n^2\right]\\
			&= \mathbb{E}\left[Y^2\right]-2 \mathbb{E}\left[Y_n^2\right] + \mathbb{E}\left[Y_n^2\right] = \mathbb{E}\left[Y^2\right]-\mathbb{E}\left[Y_n^2\right]
		\end{split}
	\end{equation}
	Which implies $\mathbb{E}\left[Y_n^2\right]\leq \mathbb{E}\left[Y^2\right]$, i.e. $\sup\limits_{n}\mathbb{E}\left[Y_n^2\right] \leq \mathbb{E}\left[Y^2\right]<\infty$, since $Y\in \mathcal{L}^2$ by hypothesis. Therefore
	\begin{equation}
		\sum_{k=1}^{\infty} \mathbb{E}^2\left[X_kY\right]<\infty
	\end{equation}
	So $\mathbb{E}\left[X_kY\right]\xrightarrow{n\to \infty}0$. $\blacksquare$
\end{itemize}

%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 8.} Show that following formula of the standard Gaussian rv: $X\sim N(0,1)$, then
	\begin{equation*}
		\mathbb{E}\left[X^n\right] = \begin{cases}
		0 &\text{$n$ odd,}\\
		(n-1)!! &\text{$n$ even.}
		\end{cases}
	\end{equation*}
	Further, for every $k\geq 0$, $\mathbb{E}\left[|X|^{2k+1}\right]=2^kk!\sqrt{2/\pi}$.
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[(1)]\textit{Proof.} For standard gaussian, we have density function:
	\begin{equation}
		\phi(x):=f_X(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}
	\end{equation}
	Notice that $\phi'=-x\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}=-x\phi$. \newline
	For $n\geq2$, applying integration by parts,
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[X^{n-1}\right]&=\int_{\mathbb{R}}x^{n-1}\phi(x)dx\\
			&=\left.\frac{x^n}{n-1}\right\vert_{-\infty}^{+\infty}-\int_{\mathbb{R}}\frac{x^n}{n-1}\phi'(x)dx\\
			&=\left. \frac{x^n\cdot e^{-\frac{x^2}{2}}}{\sqrt{2\pi}(n-1)}\right\vert_{-\infty}^{+\infty}+\frac{1}{n-1}\int_{\mathbb{R}}x^{n+1}\phi(x)dx\\
			&=\frac{1}{n-1}\mathbb{E}\left[X^{n+1}\right]
		\end{split}
	\end{equation}
	So $\mathbb{E}\left[X^{n+1}\right]=(n-1)\mathbb{E}\left[X^{n-1}\right]$, $n\geq 2$. \newline
	Since $\mathbb{E}\left[X\right]=0$, $\mathbb{E}\left[X^2\right]=1$ $\Rightarrow$
	\begin{equation*}
		\mathbb{E}\left[X^n\right] = \begin{cases}
		0 &\text{$n$ odd,}\\
		(n-1)!! &\text{$n$ even.}~~~\blacksquare
		\end{cases}
	\end{equation*}

	\item[(2)]\textit{Proof.} Similar as (1),
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[|X|^{n-1}\right]&=2\int_{\mathbb{R^+}}x^{n-1}\phi(x)dx\\
			&=2\left[\left.\frac{x^n}{n-1}\right\vert_{0}^{+\infty}-\int_{0}^{+\infty}\frac{x^n}{n-1}\phi'(x)dx\right]\\
			&=\frac{1}{n-1}\cdot 2\int_0^{+\infty}x^{n+1}\phi(x)dx\\
			&=\frac{1}{n-1} \mathbb{E}\left[|X|^{n+1}\right]
		\end{split}
	\end{equation}
	Since $\mathbb{E}\left[|X|\right]=\sqrt{2/\pi}$, $\mathbb{E}\left[|X|^2\right]=\mathbb{E}\left[X^2\right]=1$ $\Rightarrow$
	\begin{equation*}
		\mathbb{E}\left[|X|^n\right] = \begin{cases}
		\sqrt{\frac{2}{\pi}}\cdot(n-1)!! &\text{$n$ odd,}\\
		(n-1)!! &\text{$n$ even.}
		\end{cases}
	\end{equation*}
	Take $n=2k+1$ (odd), clearly $\mathbb{E}\left[|X|^{2k+1}\right]=2^kk!\sqrt{2/\pi}$. $\blacksquare$
\end{itemize}
 
%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 9.} $X\in m \mathcal{F}^+$, show that
	$$\mathbb{E}\left[X\right]=\int_0^{\infty} \mathbb{P}\left(X>t\right)dt=\int_0^{\infty} \mathbb{P}\left(X\geq t\right)dt$$
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[]\textit{Proof.} Firstly note that $X\in \mathbb{R}$ can be approached from below or above, i.e. $X=X^-=X^+$
	\begin{equation}
		X(w)=\int_0^{X(w)^-}1\cdot dt=\int_0^{X(w)^+}1\cdot dt
	\end{equation}
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[X\right]
			&=\int_{\Omega}\left[\int_0^{X(w)^-}1\cdot dt\right]d\mathbb{P}\\
			&=\int_{\Omega}\left[\int_0^{\infty}\mathbbm{1}_{[-\infty, X(w))}(t)\cdot dt\right]d\mathbb{P}\\
			&=\int_0^{\infty}\left[\int_{\Omega}\mathbbm{1}_{[-\infty, X(w))}(t)\cdot d \mathbb{P}\right]dt\\
			&=\int_0^{\infty}\left[\int_{\Omega}\mathbbm{1}_{\{t<X(w)\}}(w)\cdot d \mathbb{P}\right]dt\\
			&=\int_0^{\infty} \mathbb{P}\left(X>t\right)dt
		\end{split}
	\end{equation}
	The interchangeability of two integrals wrt $t$ and $\mathbb{P}$ follows (\textit{Tonelli}), since $X$ is non-negative.\newline
	To prove the second equal sign with $\mathbb{P}\left(X\geq t\right)$, we just replace upper bound of integration form of $X(w)$ with $X(w)^+$. And indicator will become $\mathbbm{1}_{[-\infty,X(w)]}$. $\blacksquare$
\end{itemize}

%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 10.} $\{X_n\}$ identically distributed. $\mathbb{E}\left[|X_n|\right]<\infty$. Show that
	$$\lim\limits_{n\rightarrow\infty}\frac{1}{n}\mathbb{E}\left[\max\limits_{1\leq j\leq n}|X_j|\right]=0$$
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[]\textit{Proof.} By result of (9),
	\begin{equation}
		\frac{1}{n} \mathbb{E}\left[\max\limits_{1\leq j\leq n}|X_j| \right]=\int_0^{\infty}\frac{1}{n}\cdot \mathbb{P}\left(\max\limits_{1\leq j\leq n}|X_j|>t\right)dt
	\end{equation}
	Denote $f_n:=n^{-1}\mathbb{P}\left(\max\limits_{1\leq j\leq n}|X_j|>t\right)$, clearly $f_n\to 0$. It suffices to show
	\begin{equation}
		\lim\limits_{n\rightarrow\infty}\frac{1}{n}\mathbb{E}\left[\max\limits_{1\leq j\leq n}|X_j|\right]=\lim\limits_{n\rightarrow\infty}\int f_n = \int \lim\limits_{n\rightarrow\infty}f_n=0
	\end{equation}
	For all $n\geq 1$, $\forall t\geq0$, consider
	\begin{equation}
		\begin{split}
			f_n:=\frac{1}{n}\mathbb{P}\left(\max\limits_{1\leq j\leq n}|X_j|>t\right)
			&\leq \frac{1}{n}\sum_{j=1}^n \mathbb{P}\left(|X_j|>t\right)\\
			&= \frac{1}{n} \cdot \sum_{j=1}^n \mathbb{P}\left(|X_1|>t\right)=\mathbb{P}\left(|X_1|>t\right)
		\end{split}
	\end{equation}
	Which follows that $\{X_n\}$ are identically distributed. Take supremum wrt n,
	\begin{equation}
		\sup\limits_{n}\frac{1}{n}\mathbb{P}\left(\max\limits_{1\leq j\leq n}|X_j|>t\right)\leq \mathbb{P}\left(|X_1|>t\right)=:g
	\end{equation}
	By result of (9), $\mathbb{E}\left[X_1\right]<\infty \Rightarrow$ LHS$\in \mathcal{L}^1$. So $f_n$ is bounded by $g\in \mathcal{L}^1$.\newline
	Apply (\textbf{DOM}), we have
	\begin{equation}
		\begin{split}
			\lim\limits_{n\rightarrow\infty}\frac{1}{n}\mathbb{E}\left[\max\limits_{1\leq j\leq n}|X_j|\right]
			&=\lim\limits_{n\rightarrow\infty}\int_0^{\infty}\frac{1}{n}\cdot \mathbb{P}\left(\max\limits_{1\leq j\leq n}|X_j|>t\right)dt\\
			&=\int_0^{\infty}\lim\limits_{n\rightarrow\infty}\frac{1}{n} \mathbb{P}\left(\max\limits_{1\leq j\leq n}|X_j|>t\right)dt=0~~~\blacksquare
		\end{split}
	\end{equation}
\end{itemize}
 
 %////////////////////////////////////////////////////////////////////////
\section{LLN}

%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 1.} (WLLN3) Let $\{X_n:n\geq 1\}$ be a sequence of pairwise indep RV on $(\Omega, \mathcal{F}, \mathbb{P})$, and $S_n$ is partial sum. Let $\{b_n:n\geq 1\}$ be seq of positive real numbers such that $b_n \nearrow \infty$, suppose
	\begin{equation}
		\lim\limits_{n\rightarrow\infty}\sum_{j=1}^n \mathbb{P}\left(|X_j|>b_n\right)=0
	\end{equation}
	\begin{equation}
		\lim\limits_{n\rightarrow\infty}\sum_{j=1}^n \mathbb{E}\left[\frac{|X_j|^2}{b_n^2}; |X_j|\leq b_n\right]=0
	\end{equation}
	If we set
	\begin{equation}
		a_n := \sum_{j=1}^n \mathbb{E}\left[X_j; |X_j|\leq b_n\right]
	\end{equation}
	Then
	\begin{equation}
		\frac{S_n - a_n}{b_n} \xrightarrow{i.p} 0
	\end{equation}

	\begin{itemize}
		\item[1.] For every $n\geq 1$ and $1\leq j\leq n$, truncate $X_n$ at $b_n$, i.e. define
		\begin{equation*}
			Y_{n,j} = \begin{cases}
			X_j &\text{if $|X_j|\leq b_n$,}\\
			0 &\text{otherwise.}
			\end{cases}
		\end{equation*}
		Let $T_n:=\sum_{j=1}^n Y_{n,j}$. Show $\lim\limits_{n\rightarrow\infty}\mathbb{P}\left(S_n \ne T_n\right)=0$

		\item[2.] Show $\mathrm{Var}\left[T_n\right]=o(b_n^2)$ as $n\to \infty$. Further show that 
		\begin{equation}
			\frac{T_n- \mathbb{E}\left[T_n\right]}{b_n}\xrightarrow{i.p} 0
		\end{equation}

		\item[3.] Show WLLN3 based on 1,2.
	\end{itemize}
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[]\textit{Proof.} (1) Since $S_n$ is partial sum of $\{X_j\}$, and $T_n$ is partial sum of $\{Y_{n,j}\}$. So $\{S_n \ne T_n\} \subseteq \{Y_{n,j}=X_j, \forall 1\leq j\leq n\}^{\complement} = \{Y_{n,j}\ne X_j, \exists 1\leq j\leq n\}$.
	\begin{equation}
		\begin{split}
			\mathbb{P}\left(S_n \ne T_n\right)&=\mathbb{P}\left(\{Y_{n,j}\ne X_j, \exists 1\leq j\leq n\}\right)= \mathbb{P}\left(\bigcup_{j=1}^n\{Y_{n,j}\ne X_j\}\right)\\
			&\leq \sum_{j=1}^n \mathbb{P}\left(Y_{n,j}\ne X_j\right)=\sum_{j=1}^n \mathbb{P}\left(|X_j|>b_n\right)
		\end{split}
	\end{equation}
	Take limit on both sides, notice that RHS is given by hypothesis (1):
	\begin{equation}
		\lim\limits_{n\rightarrow\infty}\mathbb{P}\left(S_n \ne T_n\right) \leq \lim\limits_{n\rightarrow\infty}\sum_{j=1}^n \mathbb{P}\left(|X_j|>b_n\right) = 0 ~~\blacksquare
	\end{equation}

	\item[]\textit{Proof.} (2) Since $\{X_n\}$ are pairwise indep, it is clear that for any fixed $n$, $\{Y_{n,j}\}$ are also pairwise indep. So $\mathrm{Var}\left[T_n\right]=\sum_{j=1}^n \mathrm{Var}\left[Y_{n,j}\right]$.
	\begin{equation}
		\sum_{j=1}^n\mathrm{Var}\left[Y_{n,j}\right]\leq \sum_{j=1}^n \mathbb{E}\left[Y_{n,j}^2\right]=\sum_{j=1}^n \mathbb{E}\left[X_j^2; |X_j|\leq b_n\right]
	\end{equation}
	For any fixed $n$, $b_n$ is constant with respect to summation and expectation.
	\begin{equation}
		\begin{split}
			\mathrm{Var}\left[T_n\right]&\leq \sum_{j=1}^n \mathbb{E}\left[X_j^2; |X_j|\leq b_n\right]=\sum_{j=1}^n \mathbb{E}\left[b_n^2\cdot \frac{X_j^2}{b_n^2}; |X_j|\leq b_n\right]\\
			&=b_n^2\sum_{j=1}^n \mathbb{E}\left[\frac{X_j^2}{b_n^2}; |X_j|\leq b_n\right]
		\end{split}
	\end{equation}
	i.e.
	\begin{equation}
		\frac{\mathrm{Var}\left[T_n\right]}{b_n^2}\leq \sum_{j=1}^n\mathbb{E}\left[\frac{X_j^2}{b_n^2}; |X_j|\leq b_n\right]
	\end{equation}
	Take limit on both sides, by the second hypothesis, we get exactly the definition of $\mathrm{Var}\left[T_n\right]=o(b_n^2)$.
	\begin{equation}
		\lim\limits_{n\rightarrow\infty}\frac{\mathrm{Var}\left[T_n\right]}{b_n^2}\leq \lim\limits_{n\rightarrow\infty}\sum_{j=1}^n\mathbb{E}\left[\frac{X_j^2}{b_n^2}; |X_j|\leq b_n\right]=0
	\end{equation}
	Apply \textbf{Markov}'s ineq, for all $\epsilon>0$:
	\begin{equation}
		\begin{split}
			\mathbb{P}\left(\frac{|T_n - \mathbb{E}\left[T_n\right]|}{b_n}>\epsilon\right)&=\mathbb{P}\left(\frac{|T_n - \mathbb{E}\left[T_n\right]|^2}{b^2_n}>\epsilon^2\right)\\
			&\leq \frac{\mathbb{E}\left[|T_n - \mathbb{E}\left[T_n\right]|^2\right]}{b_n^2\cdot \epsilon^2}\\
			&=\frac{\mathrm{Var}\left[T_n\right]}{b_n^2}\cdot \frac{1}{\epsilon^2} \xrightarrow{n\to \infty} 0
		\end{split}
	\end{equation}
	i.e.
	\begin{equation}
		\frac{T_n - \mathbb{E}\left[T_n\right]}{b_n} \xrightarrow{i.p} 0~~\blacksquare
	\end{equation}

	\item[]\textit{Proof.} (3) Notice that, by its definition, $a_n = \mathbb{E}\left[T_n\right]$, so
	\begin{equation}
		\begin{split}
			\frac{|S_n- a_n|}{b_n}=\frac{|S_n- \mathbb{E}\left[T_n\right]|}{b_n}&\leq \frac{|S_n-T_n|}{b_n}+\frac{|T_n- \mathbb{E}\left[T_n\right]|}{b_n}\\
			&:=Q_1 + Q_2
		\end{split}
	\end{equation}
	Since $S_n \ne T_n$ on $\mathbb{P}$-null set when $n\to \infty$, $Q_1 \xrightarrow{a.s.} 0$. And we have shown that $Q_2 \xrightarrow{i.p} 0$. So the their summation $\xrightarrow{i.p}0$. $\blacksquare$
\end{itemize}



%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 2.} Let $\{X_n: n\geq 1\}$ be a sequence of i.i.d. RV with common distribution
	\begin{equation}
		\mathbb{P}\left(X_1=k\right)=\mathbb{P}\left(X_1=-k\right)=\frac{c}{k^2\log k}, k=3,4,...
	\end{equation}
	where $c$ is a constant and $c=\frac{1}{2}(\sum_{k\geq 3}\frac{1}{k^2\log k})^{-1}$. Let $S_n$ be partial sum.
	\begin{itemize}
		\item[1.] Show $\frac{S_n}{n}\xrightarrow{i.p}0$.
		\item[2.] Show that $\mathbb{P}\left(\frac{|S_n|}{n}>\frac{1}{2}~i.o.\right)=1$. Therefore, this is an example for which WLLN holds but SLLN does not hold.
		\item[3.] Show
		\begin{equation}
			\mathbb{P}\left(\limsup\limits_{n\rightarrow\infty}\frac{S_n}{n}=\infty\right)=\mathbb{P}\left(\liminf\limits_{n\rightarrow\infty}\frac{S_n}{n}=-\infty\right)=1
		\end{equation}
		i.e. the amplitude of oscillation of $\frac{S_n}{n}$ is unbounded.
	\end{itemize}
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[]\textit{Proof.} (1) Check for WLLN3, let $b_n:=n$, firstly
	\begin{equation}
		\begin{split}
			\sum_{j=1}^n \mathbb{P}\left(|X_j|>n\right)&=n \mathbb{P}\left(|X_1|> n\right)=n\sum_{k\geq n+1}\frac{2c}{k^2\log k}\\
			&\leq \frac{n}{\log n}\sum_{k\geq n+1}\frac{2c}{k^2}\leq \frac{n}{\log n}\int_n^{\infty}\frac{2c}{x^2}dx\\
			&=\frac{2cn}{\log n}\cdot \left.(-\frac{1}{x})\right\vert_n^{\infty}=\frac{2c}{\log n} \xrightarrow{n\to \infty}0
		\end{split}
	\end{equation}
	Secondly
	\begin{equation}
		\begin{split}
			\sum_{j=1}^n \mathbb{E}\left[\frac{X_j^2}{n^2};|X_j|\leq n\right]&=n \mathbb{E}\left[\frac{X_1^2}{n^2};|X_1|\leq n\right]\\
			&=n \sum_{k=3}^n\frac{k^2}{n^2}\cdot \frac{2c}{k^2\log k}=\frac{2c}{n}\cdot \sum_{k=3}^n\frac{1}{\log k}
		\end{split}
	\end{equation}
	Now we estimate $\sum_{k=3}^n\frac{1}{\log k}$, consider
	\begin{equation}
		li(n)-li(3)=\int_{3}^n \frac{dx}{\log x}<\sum_{k=3}^n\frac{1}{\log k}<\int_4^{n+1} \frac{dx}{\log x} = li(n+1)-li(4)
	\end{equation}
	Where $li(n):=\int_0^n dx/\log(x)$. Use the estimation\footnote{From wikipedia.} of $li(n)$, we have
	\begin{equation}
		\sum_{k=3}^n\frac{1}{\log k} \sim li(n)=O\left(\frac{n}{\log n}\right)
	\end{equation}
	Therefore,
	\begin{equation}
		\frac{2c}{n}\cdot \sum_{k=3}^n\frac{1}{\log k} = O\left(\frac{1}{\log n}\right) \xrightarrow{n\to \infty} 0
	\end{equation}
	So the condtions for WLLN3 holds. Apply WLLN3, define
	\begin{equation}
		a_n := \sum_{j=1}^n \mathbb{E}\left[X_j; |X_j|\leq n\right] = 0
	\end{equation}
	\begin{equation}
		\frac{S_n - a_n}{b_n}=\frac{S_n}{n} \xrightarrow{i.p} 0~~\blacksquare
	\end{equation}

	\item[]\textit{Proof.} (2) It is clear that
	\begin{equation}
		\mathbb{E}\left[|X_1|\right] = \sum_{k\geq 3} k\cdot \frac{2c}{k^2 \log k} = \sum_{k\geq 3} \frac{2c}{k \log k} = \infty
	\end{equation}
	\begin{itemize}
		\item[$\cdot$] Fix any $A>0$, $\mathbb{E}\left[|\frac{X_1}{A}|\right]=\infty$.
		\item[$\cdot$] Follow the proof of second part of (\textbf{SLLN3}) on lecture, $\Rightarrow$ $\sum_{j\geq 1}\mathbb{P}\left(|X_1|>jA\right)=\infty$. Since $\{X_n\}$ are i.i.d, $\Rightarrow$ $\sum_{j\geq 1}\mathbb{P}\left(|X_j|>jA\right)=\infty$
		\item[$\cdot$] By (\textbf{BC2}), $\mathbb{P}\left(|X_n|>nA~~i.o.\right)=1$, i.e.
		\begin{equation}
			\mathbb{P}\left(\frac{|S_n - S_{n-1}|}{n}>A~~i.o\right)=1
		\end{equation}
		Since $\{\frac{|S_n - S_{n-1}|}{n}>A\} \subseteq\{\frac{|S_n|}{n}>\frac{A}{2}\}\cup\{\frac{|S_{n-1}|}{n-1}>\frac{A}{2}\}= \{\frac{|S_n|}{n}>\frac{A}{2}\}$. Take $A=1$, we have
		\begin{equation}
			\mathbb{P}\left(\frac{|S_n|}{n}>\frac{1}{2}~~i.o\right)=1~~\blacksquare
		\end{equation}
	\end{itemize}
	\item[]\textit{Proof.} (3) By (\textbf{SLLN3}), second part, $\mathbb{E}\left[|X_1|\right]=\infty$ $\Rightarrow$
	\begin{equation}
		\mathbb{P}\left(\limsup\limits_{n\rightarrow\infty}\frac{|S_n|}{n}=\infty\right)=1
	\end{equation}
	Define $X_n':=-X_n$, $S_n'=\sum X_n'$. Since $\{X_n\}$ is \textbf{symmetrically} distributed about $0$. $X_n$ and $X_n'$ are essentially identically distributed, so do $S_n$ and $S_n'$. Therefore, 
	\begin{equation}
		\begin{split}
			\left\{\frac{|S_n|}{n}>m~i.o\right\}&=\left\{\frac{S_n}{n}>m~i.o\right\}\cup \left\{\frac{S_n'}{n}>m~i.o\right\}\\
			&=\left\{\frac{S_n}{n}>m~i.o\right\} \subseteq \left\{\limsup\limits_{n\rightarrow\infty}\frac{S_n}{n}>m\right\}
		\end{split}
	\end{equation}
	By (2), LHS has probability 1 holds for $\forall m>1$, take intersection with respect to $m$, we have
	\begin{equation}
		\mathbb{P}\left(\limsup\limits_{n\rightarrow\infty}\frac{S_n}{n}=\infty\right)=1
	\end{equation}
	For the infimum side, note that $S_n, S_n'$ are identically distributed,
	\begin{equation}
		\begin{split}
			\mathbb{P}\left(\liminf\limits_{n\rightarrow\infty}\frac{S_n}{n}=-\infty\right)&
			=\mathbb{P}\left(\limsup\limits_{n\rightarrow\infty}\frac{-S_n}{n}=\infty\right)\\
			&=\mathbb{P}\left(\limsup\limits_{n\rightarrow\infty}\frac{S_n'}{n}=\infty\right)=\mathbb{P}\left(\limsup\limits_{n\rightarrow\infty}\frac{S_n}{n}=\infty\right)=1~~~\blacksquare
		\end{split}
	\end{equation}
\end{itemize}


%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 3.} (SLLN4) Let $\{X_n: n\geq 1\}$ be sequence of $\mathcal{L}^1$, indep RVs; $S_n$ be partial sum. Let $\phi: \mathbb{R}\to \mathbb{R}$ be positive and continuous even function such that $\frac{\phi(x)}{|x|}$ is non-decreasing in $x$ and $\frac{\phi(x)}{x^2}$ is non-increasing in $x$. Assume for some sequence $\{b_n: n\geq 1\}$ of positive real numbers with $b_n \nearrow \infty$,
		\begin{equation}
			\sum_{n\geq 1}\frac{\mathbb{E}\left[\phi(X_n)\right]}{\phi(b_n)}<\infty
		\end{equation}
		Show that $\sum_{n\geq 1}\frac{X_n - \mathbb{E}\left[X_n\right]}{b_n}$ converges a.s., hence
		\begin{equation}
			\frac{S_n - \mathbb{E}\left[S_n\right]}{b_n} \xrightarrow{a.s.} 0
		\end{equation}
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[]\textit{Proof.} We start from $\phi$. \newline
	$\bullet$ Since $\frac{\phi(x)}{|x|}$ is non-decreasing in $x$, for $|X_n|\geq b_n$, we have
	\begin{equation}
		\frac{\phi(b_n)}{b_n}\leq \frac{\phi(X_n)}{|X_n|}
	\end{equation}
	Besides since $\phi$ is positive, everything above are all positve, thus we can rearrange it without changing sign, i.e.
	\begin{equation}
		\frac{|X_n|}{b_n}\leq \frac{\phi(X_n)}{\phi(b_n)}
	\end{equation}
	Take expectation on bothsides, note that we have constrained ourselves by $|X_n|\geq b_n$,
	\begin{equation}
		\frac{\mathbb{E}\left[|X_n|; |X_n|\geq b_n\right]}{b_n}\leq \frac{\mathbb{E}\left[\phi(X_n); |X_n|\geq b_n\right]}{\phi(b_n)} \leq \frac{\mathbb{E}\left[\phi(X_n)\right]}{\phi(b_n)}~~(\triangle)
	\end{equation}
	$\bullet$ Since $\frac{\phi(x)}{x^2}$ is non-increasing in $x$, for $|X_n|\leq b_n$, we have
	\begin{equation}
		\frac{\phi(b_n)}{b_n^2}\leq \frac{\phi(|X_n|)}{|X_n|^2}=\frac{\phi(X_n)}{|X_n|^2}~~\text{ i.e. }~~\frac{|X_n|^2}{b_n^2}\leq \frac{\phi(X_n)}{\phi(b_n)}
	\end{equation}
	The equal sign from $\phi(|X_n|)$ to $\phi(X_n)$ follows that $\phi$ is a even function. \newline
	Take expectation on bothsides, note that we have constrained ourselves by $|X_n|\leq b_n$,
	\begin{equation}
		\frac{\mathbb{E}\left[|X_n|^2; |X_n|\leq b_n\right]}{b_n^2}\leq \frac{\mathbb{E}\left[\phi(X_n); |X_n|\leq b_n\right]}{\phi(b_n)} \leq \frac{\mathbb{E}\left[\phi(X_n)\right]}{\phi(b_n)}~~(\dagger)
	\end{equation}
	Now trancate $X_n$ at the level of $b_n$. Define
	\begin{equation*}
		Y_{n} = \begin{cases}
		X_n &\text{if $|X_n|\leq b_n$,}\\
		0 &\text{otherwise.}
		\end{cases}
	\end{equation*}
	And define $T_n:=\sum_1^nY_n$. By same argument as before, $X_n, Y_n$ are equivalent. Moreover $\{Y_n\}$ are also indep. \newline
	Consider sequence $\{\frac{Y_n}{b_n}\}$ (clearly also indep.),
	\begin{equation}
		\begin{split}
			\sum_{n\geq 1}\mathrm{Var}\left[\frac{Y_n}{b_n}\right]&=\sum_{n\geq 1}\frac{\mathrm{Var}\left[Y_n\right]}{b_n^2}\leq \sum_{n\geq 1}\frac{\mathbb{E}\left[Y_n^2\right]}{b_n^2}\\
			&=\sum_{n\geq 1}\frac{\mathbb{E}\left[X_n^2; |X_n|\leq b_n\right]}{b_n^2}\leq \sum_{n\geq 1}\frac{\mathbb{E}\left[\phi(X_n)\right]}{\phi(b_n)}<\infty
		\end{split}
	\end{equation}
	The last $\leq$ is due to $(\dagger)$. Apply (\textbf{SLLN2-Prelude}) to $\frac{Y_n}{b_n}$ then apply (\textbf{Kronecker}) $\Rightarrow$ 
	\begin{equation}
		\frac{1}{b_n}\sum_{n\geq 1}(Y_n - \mathbb{E}\left[Y_n\right]) \xrightarrow{a.s.}0~~\text{ i.e. }~~\frac{T_n - \mathbb{E}\left[T_n\right]}{b_n} \xrightarrow{a.s.} 0~~(\#)
	\end{equation}
	Finally consider
	\begin{equation}
		\begin{split}
			\frac{|S_n- \mathbb{E}\left[S_n\right]|}{b_n}
			&\leq\frac{|S_n - T_n|}{b_n}+\frac{|T_n - \mathbb{E}\left[T_n\right]|}{b_n}+\frac{|\mathbb{E}\left[T_n\right]-\mathbb{E}\left[S_n\right]|}{b_n}\\
			&=Q_1+Q_2+Q_3
		\end{split}
	\end{equation}
	Since $X_n, Y_n$ are equivalent, $b_n \nearrow \infty$ $\Rightarrow Q_1 \xrightarrow{a.s.} 0$. \newline
	By $(\#)$, $Q_2 \xrightarrow{a.s.} 0$.\newline
	For $Q_3$,
	\begin{equation}
		Q_3 = \frac{1}{b_n}\sum_{n\geq 1}\mathbb{E}\left[|X_n|; |X_n|\geq b_n\right]
	\end{equation}
	By $(\triangle)$,
	\begin{equation}
		\sum_{n\geq 1}\frac{\mathbb{E}\left[|X_n|; |X_n|\geq b_n\right]}{b_n}\leq \sum_{n\geq 1}\frac{\mathbb{E}\left[\phi(X_n)\right]}{\phi(b_n)}<\infty
	\end{equation}
	Apply again (\textbf{Kronecker}), $Q_3 \xrightarrow{a.s.} 0$. Therefore,
	\begin{equation}
		\frac{|S_n- \mathbb{E}\left[S_n\right]|}{b_n}=Q_1+Q_2+Q_3 \xrightarrow{a.s.} 0~~~~\blacksquare
	\end{equation}
\end{itemize}


%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 4.} (Inverting Laplace Transform) Let $f$ be bounded continuous function on $[0, \infty)$, Laplace transform of $f$ is the function $L$ on $(0, \infty)$ by
		\begin{equation}
			L(\lambda) := \int_0^{\infty}e^{-\lambda x}f(x)dx
		\end{equation}
		Let $\{X_n\}$ be indep RVs with exponential dist of rate $\lambda$, $S_n$ be partial sum. So $\mathbb{P}\left(X>x\right)=e^{-\lambda x}$, $\mathbb{E}\left[X\right]=\frac{1}{\lambda}$, $\mathrm{Var}\left[X\right]=\frac{1}{\lambda^2}$.
		\begin{itemize}
			\item[1.] Show 
			\begin{equation}
				(-1)^{n-1}\frac{\lambda^n L^{(n-1)}(\lambda)}{(n-1)!}=\mathbb{E}\left[f(S_n)\right]
			\end{equation}
			\item[2.] $f$ can be recovered from $L$ by: for $y>0$
			\begin{equation}
				f(y)=\lim\limits_{n\rightarrow\infty}(-1)^{n-1}\frac{(\frac{n}{y})^n L^{(n-1)}(\frac{n}{y})}{(n-1)!}
			\end{equation}
		\end{itemize}
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[]\textit{Proof.} (1) Denote pdf of X by $\phi_X$, we claim that for $\{X_n\}$ i.i.d. exponential($\lambda$), the pdf of partial sum evaluated at any $x>0$ is
	\begin{equation}
		\phi_{S_n}(x)=\lambda e^{-\lambda x}\frac{(\lambda x)^{n-1}}{(n-1)!}~~(\#)
	\end{equation}
	We prove by induction. Basic case $n=1$, $\phi_{S_1}=\phi_X=\lambda e^{-\lambda x}$. Assume $(\#)$ holds for $n$, then for $n+1$:
	\begin{equation}
		\begin{split}
			\phi_{S_{n+1}}(x)&=(\phi_X * \phi_{S_{n}})(x)=\int_0^{\infty}\phi_X(x-y)\phi_{S_{n}}(y)dy\\
			&=\int_0^{\infty}\lambda e^{-\lambda (x-y)}\lambda e^{-\lambda y}\frac{(\lambda y)^{n-1}}{(n-1)!}dy\\
			&=\lambda e^{-\lambda x} \int_{0}^{\infty} \lambda^n \frac{y^{n-1}}{(n-1)!} dy\\
			&=\lambda e^{-\lambda x}\frac{(\lambda x)^{n}}{n!}
		\end{split}
	\end{equation}
	Now look at LHS of equation to prove. Since $\partial_{\lambda}^{n-1}(e^{-\lambda x}f(x))$ exists and is continuous, we are allowed to take $\partial_{\lambda}^{n-1}$ inside integral.
	\begin{equation}
		\begin{split}
			(-1)^{n-1}\frac{\lambda^n L^{(n-1)}(\lambda)}{(n-1)!}&=(-1)^{n-1}\frac{\lambda^n}{(n-1)!}\int_{0}^{\infty}\partial_{\lambda}^{n-1}(e^{-\lambda x})f(x)dx\\
			=&\int_{0}^{\infty}\lambda e^{-\lambda x}\frac{(\lambda x)^{n-1}}{(n-1)!}f(x)dx=\int_{0}^{\infty}f(x)\phi_{S_n}(x)dx\\
			&=\mathbb{E}\left[f(S_n)\right]=RHS~~\blacksquare
		\end{split}
	\end{equation}

	\item[]\textit{Proof.} (2) By (\textbf{WLLN2}), since $\{X_n\}$ i.i.d. exponential, $X_n \in \mathcal{L}^1$, we have
	\begin{equation}
		\frac{S_n}{n} \xrightarrow{i.p} \mathbb{E}\left[X_1\right]=\frac{1}{\lambda}~\text{ i.e. }S_n~\xrightarrow{i.p} \frac{n}{\lambda}=:y
	\end{equation}
	Composition with continuous function $f$ preserves convergence in probability, so $f(S_n) \xrightarrow{i.p} f(y)$.\newline
	Since $f$ is bounded (by some $g\in \mathcal{L}^1[0,\infty)$?), by (\textbf{DOM}): $f(S_n) \xrightarrow{\mathcal{L}^1} f(y)$, i.e. for any fixed $y$ such that $\lambda = \frac{n}{y}$,
	\begin{equation}
		f(y)=\mathbb{E}\left[f(y)\right]=\lim\limits_{n\rightarrow\infty}\mathbb{E}\left[f(S_n)\right]=\lim\limits_{n\rightarrow\infty}(-1)^{n-1}\frac{(\frac{n}{y})^n L^{(n-1)}(\frac{n}{y})}{(n-1)!}~~\blacksquare
	\end{equation}
\end{itemize}

%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 5.} Let $\{X_n: n\geq 1\}$ be sequence of i.i.d RV with common distribution
		\begin{equation}
			\mathbb{P}\left(X_1=k\right)=p_k \text{ where $p_k\in (0,1), 1\leq k \leq L$, and } \sum_{k=1}^L p_k=1
		\end{equation}
		For every $n\geq 1$ and $1\leq k \leq L$, let $S_n$ be partial sum and $N_k^{(n)}:=\sharp\{j: 1\leq j\leq n, X_j=k\}$. (i.e. the number of $X_j$ among the first $n$ terms of sequence which take value $k$). Show that, if
		\begin{equation}
			P(n):= \prod_{k=1}^L p_k^{N_k^{(n)}}
		\end{equation}
		Then
		\begin{equation}
			\lim\limits_{n\rightarrow\infty}\frac{1}{n}\cdot \log(P(n))~~~\text{exists a.s. (find it.)}
		\end{equation}
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[]\textit{Proof.} Define 
	\begin{equation*}
		Y_{k,j} = \begin{cases}
		1 &\text{if $X_j=k$,}\\
		0 &\text{otherwise.}
		\end{cases}
	\end{equation*}
	Clearly for any fixed $1\leq k\leq L$, $\{Y_{k,j}:j\geq 1\}$ is a sequence of i.i.d RVs due to the fact that $\{X_j\}$ are i.i.d. And $N^{(n)}_k=\sum_{j=1}^n Y_{k,j}$ is a partial sum of $Y_{k,j}$.\newline
	Fix $k$, for all $j\geq 1$,
	\begin{equation}
		\mathbb{E}\left[Y_{k,j}\right]=\mathbb{E}\left[Y_{k,1}\right]=1\cdot \mathbb{P}\left(X_1=k\right)=p_k<\infty
	\end{equation}
	So by (\textbf{SLLN3}),
	\begin{equation}
		\frac{N^{(n)}_k}{n} \xrightarrow{a.s.} \mathbb{E}\left[Y_{k,1}\right]=p_k
	\end{equation}
	Therefore,
	\begin{equation}
		\begin{split}
			\frac{1}{n}\cdot \log(P(n)) &= \frac{1}{n}\sum_{k=1}^L N_k^{(n)}\log p_k = \sum_{k=1}^L \frac{N_k^{(n)}}{n}\log p_k\\
			&\xrightarrow{a.s.} \sum_{k=1}^L p_k\log p_k
		\end{split}
	\end{equation}
	i.e. $\lim\limits_{n\rightarrow\infty}\frac{1}{n}\cdot \log(P(n))$ exists almost surely. It equals to $\sum_{k=1}^L p_k\log p_k$ with $1$ probability.
\end{itemize}

%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 6.} Let $\{X_n: n\geq 1\}$ be sequence of i.i.d RVs with $\mathbb{E}\left[|X_1|\right]<\infty$, and $S_n$ be partial sum. Show that if $\mathbb{E}\left[X_1\right]\ne 0$,
	\begin{equation}
		\frac{\max\limits_{1\leq k\leq n}|X_k|}{|S_n|} \xrightarrow{a.s.} 0
	\end{equation}
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[]\textit{Proof.} For all $\epsilon>0$,
	\begin{equation}
		\begin{split}
			\infty>\mathbb{E}\left[|X_1|\right] &= \int_{0}^{\infty} \mathbb{P}\left(|X_1|>t\right)dt\\
			&=\left(\int_0^{\epsilon}+\int_{\epsilon}^{2\epsilon}+\int_{2\epsilon}^{3\epsilon}+...\right)\mathbb{P}\left(|X_1|>t\right)dt\\
			&=\sum_{n\geq 1}\int_{(n-1)\epsilon}^{n\epsilon}\mathbb{P}\left(|X_1|>t\right)dt\\
			&\geq \sum_{n\geq 1}\epsilon\cdot\mathbb{P}\left(|X_1|>n\epsilon\right)\\
			&=\epsilon\cdot \sum_{n\geq 1}\mathbb{P}\left(\frac{|X_1|}{n}>\epsilon\right)
		\end{split}
	\end{equation}
	Therefore $\sum_{n\geq 1}\mathbb{P}\left(\frac{|X_1|}{n}>\epsilon\right)<\infty$. \newline
	By (\textbf{BC1}), $\mathbb{P}\left(\frac{|X_1|}{n}>\epsilon~~i.o.\right)=0$ for all $\epsilon>0$ $\Rightarrow$ $\frac{|X_1|}{n} \xrightarrow{a.s.} 0$.\newline
	Now consider
	\begin{equation}
		\begin{split}
			\frac{\max\limits_{1\leq k\leq n}|X_k|}{|S_n|} = \frac{\max\limits_{1\leq k\leq n}|X_k|}{n}\cdot \frac{n}{|S_n|}
		\end{split}
	\end{equation}
	For the second factor, apply (\textbf{SLLN3}, since mutually indep, $\mathbb{E}\left[|X_1|\right]<\infty$),
	\begin{equation}
		\frac{S_n}{n} \xrightarrow{a.s.} \mathbb{E}\left[X_1\right] \ne 0
	\end{equation}
	So, 
	\begin{equation}
		\frac{n}{|S_n|} \xrightarrow{a.s.} \left|\frac{1}{\mathbb{E}\left[X_1\right]}\right| < \infty~~(1)
	\end{equation}
	For the first factor, we already have $\mathbb{P}\left(\frac{|X_1|}{n}>\epsilon~~i.o.\right)=0$. For any $\epsilon$,
	\begin{equation}
		\begin{split}
			\mathbb{P}\left(\frac{\max\limits_{1\leq k\leq n}|X_k|}{n}>\epsilon~~i.o.\right)&=\mathbb{P}\left(\bigcup_{k=1}^n\left\{\frac{|X_k|}{n}>\epsilon~~i.o.\right\}\right)\\
			&=\sum_{k=1}^n\mathbb{P}\left(\frac{|X_k|}{n}>\epsilon~~i.o.\right)\\
			&=n\cdot \mathbb{P}\left(\frac{|X_1|}{n}>\epsilon~~i.o.\right)=0
		\end{split}
	\end{equation}
	Therefore $\frac{\max\limits_{1\leq k\leq n}|X_k|}{n} \xrightarrow{a.s.} 0~~(2)$. By (1) and (2),
	\begin{equation}
		\frac{\max\limits_{1\leq k\leq n}|X_k|}{|S_n|} \xrightarrow{a.s.} 0 \cdot \left|\frac{1}{\mathbb{E}\left[X_1\right]}\right|=0~~\blacksquare
	\end{equation}
\end{itemize}


%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 7.} Let $\{X_n\}$ be i.i.d RVs, $\mathbb{E}\left[X_1\right]=0$, $\mathbb{E}\left[X_1^2\right]=1$. $S_n$ is partial sum. Show that for every $c\in \mathbb{R}$ and $n\geq 1$,
	\begin{equation}
		\mathbb{P}\left(\max\limits_{1\leq j\leq n}S_j \geq c\right)\leq 2 \mathbb{P}\left(S_n \geq c-\sqrt{2n}\right)
	\end{equation}
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[]\textit{Proof.} For any $c\in \mathbb{R}$,
	\begin{equation}
		\begin{split}
			\frac{1}{2}RHS&=\mathbb{P}\left(S_n \geq c-\sqrt{2n}\right)\\
			&\geq \mathbb{P}\left(S_n \geq c-\sqrt{2n}~\text{ and }\max\limits_{1\leq j\leq n}S_j \geq c\right)\\
			&=\sum_{k=1}^n \mathbb{P}\left(S_n \geq c-\sqrt{2n}~\text{ and }S_j < c, \forall j=1,2,...,k-1 \text{ and } S_k \geq c\right)\\
			&\geq \sum_{k=1}^n \mathbb{P}\left(S_k-S_n \leq \sqrt{2(n-k)}~\text{ and }S_j < c, \forall j=1,2,...,k-1 \text{ and } S_k \geq c\right)~~(\dagger)\\
		\end{split}
	\end{equation}
	The last geq sign holds, because given $\{S_k-S_n\leq \sqrt{2(n-k)}\text{ and }S_k\geq c\}$, \newline
	we have $\sqrt{2(n-k)}\geq S_k-S_n \geq c-S_n$. \newline
	$\Rightarrow$ $S_n \geq c-\sqrt{2(n-k)}\geq c-\sqrt{2n}$, i.e. this event implies the original one:
	\begin{equation}
		\{S_k-S_n\leq \sqrt{2(n-k)}\text{ and }S_k\geq c\} \subseteq \{S_n\geq c-\sqrt{2n}\text{ and }S_k\geq c\}
	\end{equation}
	Since $\{S_j < c, \forall j=1,2,...,k-1 \text{ and } S_k \geq c\}\in \sigma(X_1, X_2, ... ,X_k)$ \newline
	And $\{S_k-S_n \leq \sqrt{2(n-k)}\}\in \sigma(X_{k+1}, X_{k+2}, ..., X_n)$, these two events are independent, so,
	\begin{equation}
		\begin{split}
			(\dagger)&=\sum_{k=1}^n \mathbb{P}\left(S_k-S_n \leq \sqrt{2(n-k)}\right)\cdot \mathbb{P}\left(S_j < c, \forall j=1,2,...,k-1 \text{ and } S_k \geq c\right)\\
			&\geq \sum_{k=1}^n \mathbb{P}\left(|S_k-S_n| \leq \sqrt{2(n-k)}\right)\cdot \mathbb{P}\left(S_j < c, \forall j=1,2,...,k-1 \text{ and } S_k \geq c\right)
		\end{split}
	\end{equation}
	By (\textbf{Markov}), note that $\{X_n\}$ are indep, $\mathbb{E}\left[X_j^2\right]=1$, $\mathbb{E}\left[X_j\right]=0$,
	\begin{equation}
		\begin{split}
			\mathbb{P}\left(|S_k-S_n| > \sqrt{2(n-k)}\right)&<\frac{\mathbb{E}\left[(\sum_{j=k}^n X_j)^2\right]}{2(n-k)}\\
			&=\frac{1}{2(n-k)}\left[\sum_{j=k}^n \mathbb{E}\left[X_j^2\right]+\sum_{k\leq i\ne j\leq n} \mathbb{E}\left[X_iX_j\right]\right]\\
			&=\frac{1}{2(n-k)}\cdot[(n-k)+0]=\frac{1}{2}
		\end{split}
	\end{equation}
	Therefore,
	\begin{equation}
		\mathbb{P}\left(|S_k-S_n| \leq \sqrt{2(n-k)}\right)\geq 1-\frac{1}{2}=\frac{1}{2}
	\end{equation}
	\begin{equation}
		\begin{split}
			(\dagger)&\geq \sum_{k=1}^n \frac{1}{2}\cdot \mathbb{P}\left(S_j < c, \forall j=1,2,...,k-1 \text{ and } S_k \geq c\right)\\
			&=\frac{1}{2}\mathbb{P}\left(\max\limits_{1\leq j\leq n}S_j \geq c\right)=\frac{1}{2}LHS
		\end{split}
	\end{equation}
	So we have $LHS\leq RHS$. $\blacksquare$
\end{itemize}


%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 8.} 
	\begin{itemize}
		\item[1.] Let $X$ be non-negative RV on $(\Omega, \mathcal{F}, \mathbb{P})$, $p\in(1,\infty)$, show
		\begin{equation}
			\mathbb{E}\left[X^p\right]=p\int_0^{\infty}t^{p-1}\mathbb{P}\left(X>t\right)dt = p\int_0^{\infty}t^{p-1}\mathbb{P}\left(X\geq t\right)dt
		\end{equation}
		\item[2.] Let $\{X_n:n\geq1\}$ is sequence of square-integrable indep random variable on $(\Omega, \mathcal{F}, \mathbb{P})$ with $\mathbb{E}\left[X_n\right]=0$ for all $n\geq 1$. Set $S_n:=\sum_{j=1}^n X_j$ for each $n\geq 1$, assume that $\sum_{n\geq 1} \mathbb{E}\left[X_n^2\right]<\infty$. It is known from theorem that $S_n \xrightarrow{a.s.} S$ for some RV $S$. Moreover, due to the completeness of $\mathcal{L}^2$, we know that $S\in \mathcal{L}^2$ and $S_n \to S$ also in $\mathcal{L}^2$. Show that for every $t\geq 0$,
		\begin{equation}
			\mathbb{P}\left(\sup\limits_{n\geq 1}|S_n|^2>t\right)\leq \frac{1}{t}\mathbb{E}\left[S^2; \sup\limits_{n\geq 1}|S_n|^2>t\right]
		\end{equation}
		\item[3.] With (1) and (2), show
		\begin{equation}
			\left(\mathbb{E}\left[\sup\limits_{n\geq 1}|S_n|^{2p}>t\right]\right)^{\frac{1}{p}}\leq \frac{p}{p-1}\left(\mathbb{E}\left[|S|^{2p}\right]\right)^{\frac{1}{p}}
		\end{equation}
		\item[4.] Based on (3), conclude that if $S\in L^q$ for some $q\in(2,\infty)$, then $S_n \to S$ also in $\mathcal{L}^q$.
	\end{itemize}
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[]\textit{Proof.} (1), $X\in(m \mathcal{F})^+$, use result of HW3-9 (\textbf{Tonelli}), $\forall M>0$,
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[X^p;X<M\right]
			&=\int_{\{X<M\}}\left[\int_0^{X^p(w)^-}1\cdot dt\right]d\mathbb{P}\\
			&=\int_{\Omega}\mathbbm{1}_{\{X(w)<M\}}(w)\left[\int_0^{\infty}\mathbbm{1}_{[-\infty, X^p(w))}(t)\cdot dt\right]d\mathbb{P}\\
			&=\int_{\Omega}\left[\int_0^{\infty}\mathbbm{1}_{\{X(w)<M\}}(w)\cdot\mathbbm{1}_{[-\infty, X^p(w))}(t)\cdot dt\right]d\mathbb{P}\\
			&=\int_0^{\infty}\left[\int_{\Omega}\mathbbm{1}_{\{t^{\frac{1}{p}}<X(w)<M\}}(w)\cdot d \mathbb{P}\right]dt\\
			&=\int_0^{\infty} \mathbb{P}\left(M>X>t^{\frac{1}{p}}\right)dt\\
			&=\int_0^{M^p} \left[\mathbb{P}\left(X>t^{\frac{1}{p}}\right)-\mathbb{P}\left(X>M\right)\right]dt
		\end{split}
	\end{equation}
	$\mathbb{P}\left(X>t^{\frac{1}{p}}\right)$ is montonic function w.r.t $t$, thus integrable on finite interval $[0, M^p]$. $\mathbb{P}\left(X>M\right)$ is constant. Define
	\begin{equation}
		f_M(t):=\mathbb{P}\left(M>X>t^{\frac{1}{p}}\right)\nearrow \mathbb{P}\left(X>t^{\frac{1}{p}}\right)=:f(t)
	\end{equation}
	By our argument above, $\mu(f_M(t))=\mathbb{E}\left[X^p;X<M\right]<\infty$. By (\textbf{MON}), $\mu(f_M(t))\to \mu(f)$, i.e.
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[X^p\right]&=\lim\limits_{M\rightarrow\infty}\int_0^{\infty} \mathbb{P}\left(M>X>t^{\frac{1}{p}}\right)dt\\
			&=\int_0^{\infty} \lim\limits_{M\rightarrow\infty}\mathbb{P}\left(M>X>t^{\frac{1}{p}}\right)dt\\
			&=\int_0^{\infty}\mathbb{P}\left(X>t^{\frac{1}{p}}\right)dt~~~~\text{(let $z:=t^{\frac{1}{p}}$)}\\
			&=\int_0^{\infty}pz^{p-1}\mathbb{P}\left(X>z\right)dz
		\end{split}
	\end{equation}
	The second equal sign is the same, just replace upper bound of integral form of $X^p(w)$ with $X^p(w)^+$, and all relevant indicators will become $\mathbbm{1}_{[-\infty,X^p(w)]}$. $\blacksquare$

	\item[]\textit{Proof.} (2) The structure is similar to Kolmogorov 's inequality. Define
	\begin{equation}
		A_j:=\{|S_i|^2\leq t, \forall i=1,2,...,j-1\text{ and }|S_j|^2>t\}
	\end{equation}
	\begin{equation}
		A:=\{\max\limits_{1\leq j \leq n}|S_j|^2>t\}=\bigcup_{j=1}^{n}A_j
	\end{equation}
	Note that $A_j$'s are disjoint, then consider
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[S_n^2; A\right]&=\sum_{j=1}^n\mathbb{E}\left[S_n^2;A_j\right]=\sum_{j=1}^n \mathbb{E}\left[(S_j+(S_n-S_j))^2;A_j\right]\\
			&=\sum_{j=1}^n\mathbb{E}\left[S_j^2;A_j\right]+\sum_{j=1}^n\mathbb{E}\left[(S_n-S_j)^2;A_j\right]+2\sum_{j=1}^n\mathbb{E}\left[(S_n-S_j)S_j;A_j\right]~~(\triangle)
		\end{split}
	\end{equation}
	By same argument as the proof of Kolmogorov's ineq, RV $S_j\in m\sigma(X_1, X_2, ..., X_j)$; $(S_n-S_j)\in m\sigma(X_{j+1}, ..., X_n)$, thus independent. Therefore the cross term is $2\sum_{j=1}^n\mathbb{E}\left[(S_n-S_j);A_j\right] \mathbb{E}\left[S_j;A_j\right]=0$, by $\mathbb{E}\left[X_n\right]=0$, so
	\begin{equation}
		\begin{split}
			(\triangle)&=\sum_{j=1}^n\mathbb{E}\left[S_j^2;A_j\right]+\sum_{j=1}^n\mathbb{E}\left[(S_n-S_j)^2;A_j\right]\\
			&\geq \sum_{j=1}^n\mathbb{E}\left[S_j^2;A_j\right]>t\sum_{j=1}^n \mathbb{P}\left(A_j\right)=t\cdot \mathbb{P}\left(A\right)
		\end{split}
	\end{equation}
	i.e.
	\begin{equation}
		\mathbb{P}\left(\max\limits_{1 \leq j\leq n}|S_j|^2>t\right)\leq \frac{1}{t}\mathbb{E}\left[S_n^2; \max\limits_{1 \leq j\leq n}|S_j|^2>t\right]
	\end{equation}
	By theorem, $S_n \xrightarrow{a.s.} S$. Since $X_n$ are \textbf{non-negative}, so $S_n^2\nearrow S^2$. Take limit on both sides and apply (\textbf{MON}) on RHS,
	\begin{equation}
		\begin{split}
			\mathbb{P}\left(\sup\limits_{n\geq 1}|S_n|^2>t\right)
			&\leq \lim\limits_{n\rightarrow\infty}\frac{1}{t}\mathbb{E}\left[S_n^2; \max\limits_{1 \leq j\leq n}|S_j|^2>t\right]\\
			&=\frac{1}{t}\mathbb{E}\left[S^2; \sup\limits_{n\geq 1}|S_n|^2>t\right]~~\blacksquare
		\end{split}	
	\end{equation}

	\item[]\textit{Proof.} (3) Since $|S_n|\geq 0$, $\sup\limits_{n\geq 1}|S_n|^{2p}=(\sup\limits_{n\geq 1}|S_n|^2)^{p}$. For non-negative RV $\sup\limits_{n\geq 1}|S_n|^2$, apply (1), then apply (2),
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[(\sup\limits_{n\geq 1}|S_n|^{2})^p\right]&=p\int_0^{\infty}t^{p-1}\mathbb{P}\left(\sup\limits_{n\geq 1}|S_n|^2>t\right)dt\\
			&\leq p\int_0^{\infty}t^{p-1}\frac{1}{t}\mathbb{E}\left[S^2; \sup\limits_{n\geq 1}|S_n|^2>t\right] dt\\
			&=p \mathbb{E}\left[S^2\int_0^{\sup\limits_{n\geq 1}|S_n|^2}t^{p-2}dt\right]\\
			&= \frac{p}{p-1} \mathbb{E}\left[(\sup\limits_{n\geq 1}|S_n|^2)^{p-1}S^2\right]~~(\triangle)\\
		\end{split}
	\end{equation}
	Apply (\textbf{Holders}) to $(\triangle)$, since $\frac{1}{p}+\frac{p-1}{p}=1$,
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[\sup\limits_{n\geq 1}|S_n|^{2p}\right]\leq (\triangle) &\leq \frac{p}{p-1} \mathbb{E}\left[((\sup\limits_{n\geq 1}|S_n|^2)^{p-1})^{\frac{p}{p-1}}\right]^{\frac{p-1}{p}} \mathbb{E}\left[S^{2p}\right]^{\frac{1}{p}}\\
			&=\frac{p}{p-1} \mathbb{E}\left[\sup\limits_{n\geq 1}|S_n|^{2p}\right]^{\frac{p-1}{p}} \mathbb{E}\left[S^{2p}\right]^{\frac{1}{p}}
		\end{split}
	\end{equation}
	If $\mathbb{E}\left[\sup\limits_{n\geq 1}|S_n|^{2p}\right]<\infty$, we can divide it from both sides, which yields
	\begin{equation}
		\mathbb{E}\left[\sup\limits_{n\geq 1}|S_n|^{2p}\right]^{\frac{1}{p}}\leq \frac{p}{p-1} \mathbb{E}\left[S^{2p}\right]^{\frac{1}{p}}~~\blacksquare
	\end{equation}

	\item[]\textit{Proof.} (4) This is a direct result from (3). Suppose $S \in \mathcal{L}^q$ for some $q\in (2, \infty)$, let $p:=\frac{q}{2}\in (1, \infty)$.
	\begin{equation}
	 	\mathbb{E}\left[\sup\limits_{n\geq 1}|S_n|^{q}\right]^{\frac{2}{q}}\leq \frac{q}{q-2} \mathbb{E}\left[S^{q}\right]^{\frac{2}{q}}<\infty
	\end{equation} 
	So $\sup\limits_{n\geq 1}|S_n| \in \mathcal{L}^q$, $S_n$ is bounded, thus in $\mathcal{L}^q$ for all $n\geq 1$. $\blacksquare$

 %////////////////////////////////////////////////////////////////////////
\section{Martingale}

%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 1.} Let $\{\mu_n: n\geq 1\}$ and $\{\nu_n: n\geq 1\}$ be two sequences of probability measures on some measurable space $(S, \Sigma)$. Assume that for each $n\geq 1$, $\mu_n$ is absolutely continuous with respect to $\nu_n$ and denote the Radon-Nikodym derivative
	\begin{equation}
		Y_n := \frac{d\mu_n}{d\nu_n}
	\end{equation}
	Set $\Omega:=S\times S \times ...$, let $\mathcal{F}$ be sigma algebra generated by cylinder sets, i.e
	\begin{equation}
		\mathcal{F}:= \sigma\left(\left\{\prod_{n\geq1}F_n: F_n \subseteq S, F_n=S\text{ for all but finitely many n}\right\}\right)
	\end{equation}
	Let $\mathbb{P}$ be the prob measure on $(\Omega, \mathcal{F})$ given by $\mathbb{P}=\bigotimes_{n\geq 1}\mu_n$, $\mathbb{Q}$ is product measure corresponding to $\nu$, $\mathbb{Q}=\bigotimes_{n\geq 1}\nu_n$.
	\begin{itemize}
		\item[$1$.] Define $\mathbb{P}_n:=\bigotimes_{j= 1}^n \mu_j$, $\mathbb{Q}_n:=\bigotimes_{j= 1}^n \nu_j$, show $\mathbb{P}_n$ is absolutely continous wrt $\mathbb{Q}_n$, (i.e. $\mathbb{Q}_n(A)=0 \Rightarrow \mathbb{P}_n(A)=0$). Further show that if define $X_n(w):=\prod_1^n Y_j(w_j)$, then $X_n = \frac{d\mathbb{P}_n}{d\mathbb{Q}_n}$ is R-N derivative of $\mathbb{P}_n$ wrt $\mathbb{Q}_n$.
		\item[$2$.] Let $X_0=1$. Show $\{X_n: n\geq 0\}$ is a martingale wrt natural filtration associated with $\{X_n: n\geq 0\}$; and $\lim\limits_{n\rightarrow\infty}X_n$ exists $\mathbb{Q}-a.s$.
		\item[$3$.] Show $\mathbb{P}\left(X>0\right)$ is either 0 or 1.
		\item[$4$.] Show either $\mathbb{P}, \mathbb{Q}$ are continuous wrt to each other, or they are entirely singular wrt each other.
	\end{itemize}
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[\textit{Proof}.] (1) We first show that $\mu \ll \nu$, $\mu' \ll \nu'$ $\Rightarrow$ $\mu\times \mu' \ll \nu \times \nu'$. For any two pairs of measures.\newline
	For $A\in S\times S'$, given $(\nu \times \nu')(A)=0$, we want to show $(\mu \times \mu')(A)=0$. For $w\in S, w'\in S'$, define
	\begin{equation}
		\begin{split}
			&I^{\mathbbm{1}_{A}}(\bar{w}):=\int_{S'} \mathbbm{1}_{A}(\bar{w}, w')\mu'(dx)=\mu'(\{w'\in S': (\bar{w}, w')\in A\}):=\mu'(A'(\bar{w}))\\
			&J^{\mathbbm{1}_{A}}(\bar{w}):=\int_{S'} \mathbbm{1}_{A}(\bar{w}, w')\nu'(dx)=\nu'(\{w'\in S': (\bar{w}, w')\in A\}):=\nu'(A'(\bar{w}))
		\end{split}
	\end{equation}
	Since all $\mu,\nu$'s are probability measures (finite), then by (\textbf{Fubini}),
	\begin{equation}
		\begin{split}
			&(\nu \times \nu')(A) := \int_{S} J^{\mathbbm{1}_{A}}(w) \nu(dw)=0~~(\triangle)\\
			&(\mu \times \mu')(A) := \int_{S} I^{\mathbbm{1}_{A}}(w) \mu(dw)
		\end{split}
	\end{equation}
	By $(\triangle)$, $J^{\mathbbm{1}_{A}}(w)=0~a.s.~w$, i.e. $\nu'(A'(\bar{w}))=0~a.s.~\bar{w}$. Define $O_{\nu}:=\{w\in S: \nu'(A'(w))=0\}$, then $\nu\left(O_{\nu}\right)=1$. \newline
	By $\mu' \ll \nu'$, $O_{\nu} \subseteq O_{\mu}:=\{w\in S: \mu'(A'(w))=0\}$, hence $\mu\left(O_{\mu}\right)=1$; $\mu\left(S\setminus O_{\mu}\right)=0$; in another word $I^{\mathbbm{1}_{A}}(w)=0~a.s.~w$. Therefore
	\begin{equation}
		\begin{split}
			(\mu \times \mu')(A) &:= \int_{S} I^{\mathbbm{1}_{A}}(w) \mu(dw)\\
			&=\left(\int_{O_{\mu}}+\int_{S \setminus O_{\mu}}\right) I^{\mathbbm{1}_{A}}(w) \mu(dw)\\
			&\leq 0+1\cdot \mu(S\setminus O_{\mu})=0
		\end{split}
	\end{equation}
	Now take $\mu,\mu'=\mu_1,\mu_2$ $\Rightarrow \mu_1\times \mu_2 \ll \nu_1 \times \nu_2$. \newline
	Then take $\mu :=\mu_1\times \mu_2, \mu' = \mu_3 \Rightarrow \mu_1\times \mu_2 \times \mu_3 \ll \nu_1 \times \nu_2 \times \nu_3$. Do this recursively, finally we conclude that for finite $n$,
	\begin{equation}
		\mathbb{P}_n:=\bigotimes_{j= 1}^n \mu_j \ll \bigotimes_{j= 1}^n \nu_j=: \mathbb{Q}_n ~~\blacksquare
	\end{equation}
	By definition, for $A_n \in S$, $\mu_n(A_n)=\mathbb{E}^{\nu_n}\left[Y_n \mathbbm{1}_{A_n}\right]$. Define $X_n(w):=\prod_{j=1}^n Y_j (w_j)$ for $w\in S^n=:\Omega$, consider measurable $A:=A_1 \times ... \times A_n \subseteq \Omega$, 

	\begin{equation}
		\begin{split}
			\mathbb{E}^{\mathbb{Q}_n}\left[X_n(w) \mathbbm{1}_{A}(w)\right]&=
			\mathbb{E}^{\mathbb{Q}_n}\left[\prod_{j=1}^n Y_j (w_j) \prod_{j=1}^n \mathbbm{1}_{A_j}(w_j)\right]\\
			&=\idotsint_{S^n} \left(\prod_{j=1}^n Y_j (w_j) \mathbbm{1}_{A_j}(w_j)\right) d\mathbb{Q}_{n}\\
			&= \idotsint_{S^{n-1}} \left(\prod_{j=1}^{n-1} Y_j (w_j) \mathbbm{1}_{A_j}(w_j) \left(\int_{A_n}Y_n(w_n) d\nu_n\right) d\mathbb{Q}_{n-1} \right)\\
			&= \idotsint_{S^{n-1}} \left(\prod_{j=1}^{n-1} Y_j (w_j) \mathbbm{1}_{A_j}(w_j) \left(\int_{S}\mathbbm{1}_{A_n}(w_n) d\mu_n\right) d\mathbb{Q}_{n-1} \right)  \\
			&= ... = \idotsint_{S^n} \left(\prod_{j=1}^{n} \mathbbm{1}_{A_j}(w_j) \right) d \bigotimes_{j\geq 1}^{n} \mu_j\\
			&= \idotsint_{S^n} \mathbbm{1}_{A}(w_1, ..., w_n)~d \bigotimes_{j\geq 1}^{n} \mu_j\\
			&=\int_{\Omega}\mathbbm{1}_{A}(w) d\mathbb{P}_n = \mathbb{P}_n\left(A\right)
		\end{split}
	\end{equation}

	So by definition of R-N derivative, at every $w\in \Omega$, $\frac{d\mathbb{P}_n}{d\mathbb{Q}_n}(w):= X_n(w)$. $\blacksquare$
	\newline

	(2) In filtered space $(\Omega, \mathcal{F}, \{\mathcal{F}_n:=\sigma(X_0,X_1, ..., X_n):n\geq 0 \}, \mathbb{Q})$, clearly $Y_{n+1}$ is independent wrt $\mathcal{F}_n$ for all $n\geq 0$ and $X_n \in m \mathcal{F}_n$. Now consider
	\begin{equation}
		\begin{split}
			\mathbb{E}^{\mathbb{Q}}\left[X_{n+1}\middle|\mathcal{F}_n\right]&=\mathbb{E}^{\mathbb{Q}}\left[Y_{n+1}\prod_{k=1}^n Y_k\middle|\mathcal{F}_n\right]\\
			&= X_n \mathbb{E}^{\mathbb{Q}}\left[Y_{n+1}\cdot1\right]=X_n \mathbb{E}^{\mathbb{Q}}\left[Y_{n+1}\cdot \mathbbm{1}_{S}\right]\\
			& = X_n \mathbb{P}\left(w_{n+1}\in S\right) \\
			&= X_n \mu_{n+1}(S)=X_n
		\end{split}
	\end{equation}
	Since $\mu_n$ is a probability measure, hence positive. For any $w \in S$, $0\leq \mu_n(\{w\})=\mathbb{E}^{\nu_n}\left[Y_n;\{w\}\right]=Y_n(w)$. So $Y_n \geq 0$ everywhere for all $n\geq 1$. So $X_n=\prod_{k=1}^nY_k \geq 0$ everywhere too. \newline
	There are two cases. 
	\begin{itemize}
		\item[$\cdot$] First, if $\exists Y_m=0~a.s$ for some $m$, then clearly $X_n =0~\mathbb{Q}$-as for all $n\geq m$. We can just define $X_n \xrightarrow{a.s.} X:=0$.
		\item[$\cdot$] Second, if the first case does not happen, then $\{X_n: n\geq 0\}$ is $\mathbb{Q}$-martingale. Hence for any $n\geq1$, $\mathbb{E}^{\mathbb{Q}}\left[X_n\right]=\mathbb{E}^{\mathbb{Q}}\left[X_0\right]=1<\infty$, so $\{X_n\}$ is uniformly integrable. By (\textbf{MCT2}) $\Rightarrow$ $\exists X\in \mathcal{L}^1$, such that $X_n \xrightarrow{a.s.} X$, $X_n \xrightarrow{\mathcal{L}^1}X$.
	\end{itemize}
	In both cases, $X$ exists $\mathbb{Q}$-a.s. $\blacksquare$.

	(3) In $(\Omega, \mathcal{F}, \{\mathcal{F}_n:=\sigma(X_0,X_1, ..., X_n):n\geq 0 \}, \mathbb{P})$, by definition, 
	\begin{equation}
		X:= \lim\limits_{n\rightarrow\infty}X_n = \prod_{n\geq 1}Y_n
	\end{equation}
	Note that $Y_n=\frac{X_n}{X_{n-1}}$, for all $n\geq 1$ if everything is positive. Hence for any $m\geq 1$, $X$ can be regarded as
	\begin{equation}
		X=X_m\prod_{n\geq m+1}Y_n=\begin{cases}X_m \prod_{n\geq m+1}\frac{X_n}{X_{n-1}} &\text{If $X_n>0~\forall n\geq m$} \\ 0 &\text{If $X_n=0~\exists n\geq m$}\end{cases} \in m\sigma(X_m, X_{m+1}, ...)
	\end{equation}
	So $\forall m\geq 1$:
	\begin{equation}
		\{X>0\}\in\sigma(X_m, X_{m+1}, ...) 
	\end{equation}
	i.e. 
	\begin{equation}
		\{X>0\} \in \bigcap_{m\geq 1}\sigma(X_m, X_{m+1}, ...)=: \mathcal{T}_{{X_n}}
	\end{equation}
	$\{X>0\}$ is an event that is a member in the tail sigma algebra associated with $\{X_n\}$. By (\textbf{Kolmogorov 0-1 Law}), $\mathbb{P}(X>0)=0$ or $1.$ $\blacksquare$
	\newline

	(4) \textbf{Part-1}. Define
	\begin{equation}
		\mathcal{I}=\left\{F, F\in \bigcup_{n\geq 1}\mathcal{F}_n\right\}
	\end{equation}
	Then $\mathcal{I}$ is a pi system. Because $\forall F_i\in \mathcal{F}_i, F_j \in \mathcal{F}_j$, we have $(F_i \cap F_j) \in \mathcal{F}_{i \vee j}\subseteq \mathcal{I}$. Moreover, $\mathcal{F}$ is generated by $\mathcal{I}$, which is clear since $\sigma(\mathcal{I})=\bigvee_{n\geq 1} \mathcal{F}_n =: \mathcal{F}$.\newline
	Now assume $\mathbb{Q}(X>0)=1$, i.e. case two in (2), where $\{X_n: n\geq 0\}$ is a $\mathbb{Q}$-martingale. \newline
	For any $n\geq 1$, any $A\in \mathcal{F}_n$,
	\begin{equation}
		\begin{split}
			\mathbb{P}(A)&=\mathbb{E}^{\mathbb{Q}}\left[X_n; A\right]=\mathbb{E}^{\mathbb{Q}}\left[\mathbb{E}^{\mathbb{Q}}\left[X_{n+1}\middle|\mathcal{F}_n\right] ;A\right]\\
			&=\mathbb{E}^{\mathbb{Q}}\left[X_{n+1};A\right]=\mathbb{E}^{\mathbb{Q}}\left[\mathbb{E}^{\mathbb{Q}}\left[X_{n+2}\middle|\mathcal{F}_{n+1}\right] ;A\right]=...\\
			&=\mathbb{E}^{\mathbb{Q}}\left[X;A\right]
		\end{split}
	\end{equation}
	I.e. two measures $X \mathbb{Q}=\mathbb{P}$ on all $A \in \mathcal{F}_n$. This is true for all $n\geq 1$. Hence $X \mathbb{Q}=\mathbb{P}$ for $A\in \mathcal{I}$. By extension theorem, finally we know $X\mathbb{Q}=\mathbb{P}$ on $F\in \mathcal{F}=\sigma(\mathcal{I})$.\newline
	So, by definition, $\frac{d\mathbb{P}}{d \mathbb{Q}}:=X$, so $\mathbb{P} \ll \mathbb{Q}$.\newline

	\textbf{Part-2}. Since $\nu_n \ll \mu_n$ is also assumed to be true, we have ${d\mu_n}/{d\nu_n}=1/Y_n$.  
	We can also show $\mathbb{Q}_n \ll \mathbb{P}_n$ by exactly same arguement as in (1). Similarly, we construct the reverse R-N derivative
	\begin{equation}
		\frac{d\mathbb{Q}_n}{d\mathbb{P}_n} := \frac{1}{X_n}
	\end{equation} 
	as we done in (1). Now consider
	\begin{equation}
		\begin{split}
			\mathbb{E}^{\mathbb{P}}\left[\frac{1}{X_{n+1}}\middle|\mathcal{F}_n\right]&=\mathbb{E}^{\mathbb{P}}\left[\frac{1}{Y_{n+1}}\prod_{k=1}^n \frac{1}{Y_k}\middle|\mathcal{F}_n\right]\\
			&= \frac{1}{X_n} \mathbb{E}^{\mathbb{P}}\left[\frac{1}{Y_{n+1}}\cdot1\right]=\frac{1}{X_n} \mathbb{E}^{\mathbb{P}}\left[\frac{1}{Y_{n+1}}\cdot \mathbbm{1}_{S}\right]\\
			& = \frac{1}{X_n} \mathbb{Q}\left(w_{n+1}\in S\right) \\
			&= \frac{1}{X_n} \nu_{n+1}(S)=\frac{1}{X_n}
		\end{split}
	\end{equation}
	We have already shown that $X_n, Y_n \geq 0$ everywhere for all $n\geq 1$. By same argument in (2) there are also two cases in reverse direction. Define $Z_n:=1/X_n$:
	\begin{itemize}
		\item[$\cdot$] First, if $\exists Y_m=0~\nu_m$-as for some $m$, then $1/Y_m=\infty~\mu_m$-as. Clearly $Z_n =\infty~\mathbb{P}$-as for all $n\geq m$. We can define $Z_n \xrightarrow{a.s.} Z:=\infty$.
		\item[$\cdot$] Second, if the first case does not happen, then $\{Z_n: n\geq 0\}$ is $\mathbb{P}$-martingale. Hence for any $n\geq1$, $\mathbb{E}^{\mathbb{P}}\left[Z_n\right]=\mathbb{E}^{\mathbb{P}}\left[Z_0\right]=1<\infty$. Clearly $\{Z_n\}$ is uniformly integrable. By (\textbf{MCT2}) $\Rightarrow$ $\exists Z\in \mathcal{L}^1$, such that $Z_n \xrightarrow{a.s.} X$, $Z_n \xrightarrow{\mathcal{L}^1}Z$.
	\end{itemize}
	Now assume $\mathbb{Q}(X>0)=1$, then $Z_n=1/X_n <\infty$ for all $n\geq 1$. $\mathbb{E}^{\mathbb{P}}\left[Z_n\right]$ exists. For all $A\in \mathcal{F}_n$, by same argument as part-1:
	\begin{equation}
		\begin{split}
			\mathbb{Q}(A)&=\mathbb{E}^{\mathbb{P}}\left[Z;A\right]
		\end{split}
	\end{equation}
	By same pi-system argument, $Z\mathbb{P}=\mathbb{Q}$ on $F\in \mathcal{F}=\sigma(\mathcal{I})$.\newline
	So, by definition, $\frac{d\mathbb{Q}}{d \mathbb{P}}:=Z$, so $\mathbb{Q} \ll \mathbb{P}$.\newline

	In summary:
	\begin{itemize}
		\item[$\bullet$] If $X>0~\mathbb{Q}$-as, then $\frac{1}{X}<\infty~\mathbb{P}$-as. $X=d\mathbb{P}/d\mathbb{Q}; \frac{1}{X}=d\mathbb{Q}/d\mathbb{Q}$, which implies $\mathbb{Q} \ll \mathbb{P}$ and $\mathbb{P} \ll \mathbb{Q}$.
		\item[$\bullet$] If $X=0~\mathbb{Q}$-as, $\mathbb{Q}(X>0)=0$. But $\mathbb{P}(X>0)=\mathbb{E}^{\mathbb{Q}}\left[X\right]=1$.
	\end{itemize}
	There are only these two cases since $X\geq 0$. Dichotomous states. $\blacksquare$
\end{itemize}

%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 2.} $X, Y$ RV, $\mathcal{G}$ is sub sigma algebra, show
	\begin{itemize}
		\item[$1.$] If $X\in \mathcal{L}^2 (\Omega, \mathcal{F}, \mathbb{P})$, then $\mathrm{Var}\left[\mathbb{E}\left[X|\mathcal{G}\right]\right]\leq \mathrm{Var}\left[X\right]$.
		\item[$2.$] If $X$ is integrable, $Y$ is bounded, then 
		\begin{equation}
			\mathbb{E}\left[\mathbb{E}\left[X|\mathcal{G}\right]Y\right]=\mathbb{E}\left[X \mathbb{E}\left[Y|\mathcal{G}\right] \right]
		\end{equation}
		\item[$3.$] If $X,Y\in \mathcal{L}^2 (\Omega, \mathcal{F}, \mathbb{P})$, $\mathbb{E}\left[X^2|\mathcal{G}\right]=Y^2$, $\mathbb{E}\left[X|\mathcal{G}\right]=Y$, then $X=Y~a.s.$
	\end{itemize}
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[$Proof$.] (1)
	\begin{equation}
		\begin{split}
			\mathrm{Var}\left[\mathbb{E}\left[X|\mathcal{G}\right]\right]=\mathbb{E}\left[\mathbb{E}^2\left[X|\mathcal{G}\right] \right]-\mathbb{E}\left[\mathbb{E}\left[X|\mathcal{G}\right]\right]^2~~(\triangle)
		\end{split}
	\end{equation}
	By (\textbf{cJensen}), $x^2$ is convex, so $\mathbb{E}^2\left[X|\mathcal{G}\right]\leq \mathbb{E}\left[X^2|\mathcal{G}\right]$, by monotonicity of integral, and also note that $\mathbb{E}\left[\mathbb{E}\left[X|\mathcal{G}\right]\right]=\mathbb{E}\left[X\right]$:
	\begin{equation}
		(\triangle)\leq \mathbb{E}\left[\mathbb{E}\left[X^2|\mathcal{G}\right]\right]-\mathbb{E}\left[X\right]^2=\mathbb{E}\left[X^2\right]-\mathbb{E}\left[X\right]^2 = \mathrm{Var}\left[X\right]~~\blacksquare
	\end{equation}

	(2) For $A \in \mathcal{G}$, $X$ integrable, let $Z:= \mathbbm{1}_{A}$, then
	\begin{equation}
		\mathbb{E}\left[XZ\right]=\int_A X d\mathbb{P} = \int_A \mathbb{E}\left[X\middle|\mathcal{G}\right] d\mathbb{P} = \mathbb{E}\left[\mathbb{E}\left[X\middle|\mathcal{G}\right]Z\right]
	\end{equation}
	Denote equility $\mathbb{E}\left[XZ\right]=\mathbb{E}\left[\mathbb{E}\left[X\middle|\mathcal{G}\right]Z\right]$ as $(\dagger)$. \newline
	By linearity, $(\dagger)$ holds for $Z\in S \mathcal{G}^+$. \newline
	By (\textbf{MON}), $(\dagger)$ holds for $Z\in m \mathcal{G}^+$.\newline
	Now suppose $Z\in b \mathcal{G}$, i.e. $\exists~0<M<\infty$, $|Z|\leq M$. Write $Z=Z^+ - Z^-$, then both positive and negative parts should be bounded by $M$, i.e. $Z^{\pm}\in b \mathcal{G}^+$. Hence for $Z^{\pm}$:
	\begin{equation}
		\mathbb{E}\left[XZ^+\right]+\mathbb{E}\left[\mathbb{E}\left[X\middle|\mathcal{G}\right]Z^-\right]=\mathbb{E}\left[\mathbb{E}\left[X\middle|\mathcal{G}\right]Z^+\right]+\mathbb{E}\left[XZ^-\right]
	\end{equation}
	$\mathbb{E}\left[XZ^{\pm}\right]\leq M \mathbb{E}\left[X\right]<\infty$ since $X \in \mathcal{L}^1$, all integrals involved in the formula above are finite. By linearity:
	\begin{equation}
		\mathbb{E}\left[XZ^+\right]-\mathbb{E}\left[XZ^-\right]=\mathbb{E}\left[\mathbb{E}\left[X\middle|\mathcal{G}\right]Z^+\right]-\mathbb{E}\left[\mathbb{E}\left[X\middle|\mathcal{G}\right]Z^-\right]
	\end{equation}
	i.e. $\mathbb{E}\left[XZ\right]=\mathbb{E}\left[\mathbb{E}\left[X\middle|\mathcal{G}\right]Z\right]$ for $X\in \mathcal{L}^1$, $Z\in b \mathcal{G}$. Now for any $Y$ bounded, $Z:=\mathbb{E}\left[Y\middle|\mathcal{G}\right]\in m \mathcal{G}$ and is bounded. Hence $\mathbb{E}\left[X \mathbb{E}\left[Y\middle|\mathcal{G}\right]\right]=\mathbb{E}\left[\mathbb{E}\left[X\middle|\mathcal{G}\right] \mathbb{E}\left[Y\middle|\mathcal{G}\right]\right]$.\newline
	$\bullet$ Now consider $\mathbb{E}\left[YW\right]=\mathbb{E}\left[\mathbb{E}\left[Y\middle|\mathcal{G}\right]W\right]~(\triangle)$ for $Y$ bounded, $|Y|<M$.\newline
	Again $(\triangle)$ holds for $W:=\mathbbm{1}_{A}$, $A\in \mathcal{G}$.\newline
	By linearity, $(\triangle)$ holds for $W\in S \mathcal{G}^+$. \newline
	By (\textbf{MON}), $(\triangle)$ holds for $W \in m \mathcal{G}^+$. \newline
	For $W\in \mathcal{L}^1$, write $W=W^+ - W^-$, for $W^{\pm}$:
	\begin{equation}
		\mathbb{E}\left[YW^+\right]+\mathbb{E}\left[\mathbb{E}\left[Y\middle|\mathcal{G}\right]W^-\right]=\mathbb{E}\left[\mathbb{E}\left[Y\middle|\mathcal{G}\right]W^+\right]+\mathbb{E}\left[YW^-\right]
	\end{equation}
	$\mathbb{E}\left[YW^{\pm}\right]\leq M \mathbb{E}\left[W^{\pm}\right]<\infty$ all integrals involved in the formula above are finite. By linearity:
	\begin{equation}
		\mathbb{E}\left[YW^+\right]-\mathbb{E}\left[YW^-\right]=\mathbb{E}\left[\mathbb{E}\left[Y\middle|\mathcal{G}\right]W^+\right]-\mathbb{E}\left[\mathbb{E}\left[Y\middle|\mathcal{G}\right]W^-\right]
	\end{equation}
	i.e. $\mathbb{E}\left[YW\right]=\mathbb{E}\left[\mathbb{E}\left[Y\middle|\mathcal{G}\right]W\right]$ for $W\in m \mathcal{G}$, $W\in \mathcal{L}^1$, $Y$ bounded. Let $W:=\mathbb{E}\left[X\middle|\mathcal{G}\right]$, we have $\mathbb{E}\left[Y \mathbb{E}\left[X\middle|\mathcal{G}\right]\right]=\mathbb{E}\left[\mathbb{E}\left[Y\middle|\mathcal{G}\right] \mathbb{E}\left[X\middle|\mathcal{G}\right]\right]$. We conclude that 
	\begin{equation}
		\mathbb{E}\left[Y \mathbb{E}\left[X\middle|\mathcal{G}\right]\right]=\mathbb{E}\left[\mathbb{E}\left[Y\middle|\mathcal{G}\right] \mathbb{E}\left[X\middle|\mathcal{G}\right]\right]=\mathbb{E}\left[\mathbb{E}\left[X\middle|\mathcal{G}\right]Y\right]
	\end{equation}
	for $X\in \mathcal{L}^1$, $Y$ bounded. $\blacksquare$

	(3) by hypothesis
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[(X-Y)^2\right] &= \mathbb{E}\left[X^2\right]+\mathbb{E}\left[Y^2\right]-2 \mathbb{E}\left[XY\right]\\
			&= \mathbb{E}\left[X^2\right]+\mathbb{E}\left[Y^2\right]-2 \mathbb{E}\left[\mathbb{E}\left[XY\middle|\mathcal{G}\right]\right]\\
			&=\mathbb{E}\left[X^2\right]+\mathbb{E}\left[Y^2\right]-2 \mathbb{E}\left[Y\mathbb{E}\left[X\middle|\mathcal{G}\right]\right]\\
			&= \mathbb{E}\left[X^2\right]-\mathbb{E}\left[Y^2\right]=0
		\end{split}
	\end{equation}
	That implies $(X-Y)^2=0~a.s.$, hence $X=Y~a.s$. $\blacksquare$
\end{itemize}

%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 3.} $X, Y$ RVs with joint distribution being bivariate centered Gaussian $N(0,C)$, with mean $\left( \begin{smallmatrix} 0\\ 0 \end{smallmatrix} \right)$, covariance matrix $C = \left( \begin{smallmatrix} a&c\\ c&b \end{smallmatrix} \right)$, where $a,b>0$, $ab-c^2>0$. That is, the joint density of $(X,Y)$ is given by
	\begin{equation}
		f_{X,Y}(x,y)=\frac{1}{2\pi \sqrt{\det C}}\exp{\left(-\frac{1}{2}\mathbf{x}^T C^{-1} \mathbf{x}\right)}
	\end{equation}
	\begin{itemize}
		\item[$1.$] Determine $\mathbb{E}\left[X|Y\right]$.
		\item[$2.$] Determine $\mathbb{E}\left[\exp{\left(X-a/2\right)}|Y\right]$.
	\end{itemize}
	}
}
%------------------------------------------------------------------------

\begin{itemize}
	\item[$Proof$.] (1) Define $Z:=X-\frac{c}{b}\cdot Y$, by linearity $Z$ is still a centerred Gaussian, $\mathbb{E}\left[Z\right]=0$. We have
	\begin{equation}
			\mathrm{Cov}\left[Z, Y\right]=\mathrm{Cov}\left[X,Y\right]-\frac{c}{b}\cdot \mathrm{Var}\left[Y\right]=c-\frac{c}{b}\cdot b=0
	\end{equation}
	Moreover, since $\mathrm{Cov}\left[Z,Y\right]=\mathbb{E}\left[ZY\right]-\mathbb{E}\left[Z\right]\mathbb{E}\left[Y\right]=\mathbb{E}\left[ZY\right]=0$, by result in hint, since $Z,Y$ has joint bivariate centerred Gaussian distribution $\Rightarrow Z,Y$ independent. Hence we can write
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[X|Y\right]&=\mathbb{E}\left[Z+\frac{c}{b}\cdot Y|Y\right]=\mathbb{E}\left[Z|Y\right]+\mathbb{E}\left[\frac{c}{b}\cdot Y|Y\right]\\
			&=\mathbb{E}\left[Z\right]+\frac{c}{b}\cdot Y=\frac{c}{b}\cdot Y~~\blacksquare
		\end{split}
	\end{equation}

	(2) Still using $Z=X-\frac{c}{b}\cdot Y$, $\mathbb{E}\left[Z\right]=0$, $\mathrm{Var}\left[Z\right]=a+\frac{c^2}{b^2}\cdot b-2\cdot \frac{c}{b}\cdot c=a-\frac{c^2}{b}$. Therefore, $\exp(Z)\sim \ln \mathcal{N}(0,a-\frac{c^2}{b})$, by wikipedia,
	\begin{equation}
		\mathbb{E}\left[\exp(Z)\right]=\exp\left(\mu+\frac{1}{2}\sigma^2\right)=\exp\left(\frac{a}{2}+\frac{c^2}{2b}\right)
	\end{equation}
	We can write
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[\exp{\left(X-\frac{a}{2}\right)}|Y\right] &= \frac{1}{\exp(\frac{a}{2})}\mathbb{E}\left[\exp{\left(Z+\frac{c}{b}\cdot Y\right)}|Y\right]\\
			&=\frac{1}{\exp(\frac{a}{2})}\mathbb{E}\left[\exp{\left(Z\right)\exp\left(\frac{c}{b}\cdot Y\right)}|Y\right]\\
			&=\frac{\mathbb{E}\left[\exp(Z)\right]}{\exp(\frac{a}{2})}\exp\left(\frac{c}{b}\cdot Y\right)\\
			&=\exp\left(\frac{cY}{b}+\frac{c^2}{2b}\right)~~\blacksquare
		\end{split}
	\end{equation}
\end{itemize}

%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 4.} $T$ is a stopping time such that for some $N\in \mathbb{N}$, and some $\epsilon>0$, we have, for every $n$:
	\begin{equation}
		\mathbb{P}\left(T\leq n+N | \mathcal{F}_n\right)>\epsilon,~~a.s.
	\end{equation}
	Show by induction using $\mathbb{P}\left(T>kN\right)=\mathbb{P}\left(T>kN; T>(k-1)N\right)$ that for $k=1,2,3...$
	\begin{equation}
		\mathbb{P}\left(T>kN\right)\leq (1-\epsilon)^k
	\end{equation}
	Show that $\mathbb{E}\left[T\right]<\infty$.
	}
}
%------------------------------------------------------------------------

\begin{itemize}
	\item[$Proof$.] Since $\mathbb{P}\left(T\leq n+N | \mathcal{F}_n\right)>\epsilon$, for all $A\in \mathcal{F}_n$, we have
	\begin{equation}
		\int_A \mathbbm{1}_{\{T\leq n+N\}} d\mathbb{P} \geq \int_A \epsilon d\mathbb{P}
	\end{equation}
	Since $T$ is a stopping time, clearly $\{T>n\}\in \mathcal{F}_n$, so
	\begin{equation}
		\mathbb{P}\left(n<T\leq n+N\right)=\int_{\{T>n\}} \mathbbm{1}_{\{T\leq n+N\}} d\mathbb{P} \geq \int_{\{T>n\}} \epsilon d\mathbb{P} = \epsilon \mathbb{P}\left(T>n\right)
	\end{equation}
	Hence, for every $n$,
	\begin{equation}
		\begin{split}
			\mathbb{P}\left(T>n+N\right)&=\mathbb{P}\left(n<T\right)-\mathbb{P}\left(n<T\leq n+N\right)\\
			&\leq(1-\epsilon)\cdot\mathbb{P}\left(n<T\right)
		\end{split}
	\end{equation}
	Pick $n:=(k-1)N$, we have 
	\begin{equation}
		\mathbb{P}\left(T>kN\right)\leq(1-\epsilon)\cdot\mathbb{P}\left(T>(k-1)N\right)
	\end{equation}
	Note that $\mathbb{P}\left(T>0\right)=1$, hence for the basic case ($k=1$) we have $\mathbb{P}\left(T>N\right)\leq (1-\epsilon)\cdot 1$. Then for any $k>1$, proceed recursively for $2,3,..,k$, we have $\mathbb{P}\left(T>kN\right)\leq(1-\epsilon)^k$ as desired. Now we bound $T$ by:
	\begin{equation}
		T \leq \sum_{k=0}^{\infty} (k+1)N \cdot \mathbbm{1}_{\{(kN<T\leq (k+1)N\}}\leq N\sum_{k=0}^{\infty} (k+1) \cdot \mathbbm{1}_{\{(kN<T\}}
	\end{equation}
	Take expectation both sides
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[T\right]&\leq \mathbb{E}\left[N\sum_{k=0}^{\infty} (k+1) \cdot \mathbbm{1}_{\{(kN<T\}}\right]\\
			&=N\sum_{k=0}^{\infty} (k+1) \cdot \mathbb{P}\left(kN<T\right)=N\sum_{k=0}^{\infty} (k+1)(1-\epsilon)^k
		\end{split}
	\end{equation}
	clearly, for $0<\epsilon<1$, the summation above converges, hence $\mathbb{E}\left[T\right]<\infty~~a.s.$. $\blacksquare$
\end{itemize}

%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 5.} Let $\{X_n: n\geq 0\}$ be i.i.d RVs with common distribution $\mathbb{P}\left(X_n=1\right)=p$, $\mathbb{P}\left(X_n=-1\right)=q=1-p$, $0<p<1$. Define $S_0:=0$, $S_n$ partial sum. Then say $\{S_n: n\geq 1\}$ is a $(p-q)$ \textit{random walk} on $\mathbb{Z}$. In particular if $p=q=0.5$, $\{S_n: n\geq 1\}$ is a \textit{symmetric random walk}. Given two positive integers $a,b$, consider
	\begin{equation}
		\tau:= \inf\{n\geq 1: S_n =-a \text{ or } S_n = b\}
	\end{equation}
	\begin{itemize}
		\item[$1.$] Show that $\mathbb{E}\left[\tau\right]<\infty$.
		\item[$2.$] Assume $p\ne 1/2$, compute $\mathbb{P}\left(S_{\tau}=-a\right)$.
		\item[$3.$] Assume $p\ne 1/2$, compute $\mathbb{E}\left[\tau\right]$.
		\item[$4.$] Assume $p=1/2$, $a=b$, compute $\mathbb{E}\left[e^{t\tau}\right]$ for $t\leq 0$.
	\end{itemize}
	}
}
%------------------------------------------------------------------------

\begin{itemize}
	\item[$Proof$.] (1) Since $a,b$ finite, the walking band has finite width $a+b<\infty$.\newline
	Consider any staring time position $n\geq 0$, $S_n\in (-a,b)$, we have $\{\tau \leq a+b+n\} \supseteq \bigcap_{k=n+1}^{n+a+b}\{X_k=1\}$. That is, $S_{\tau}$ must hits $b$ before $\tau=(n+a+b)$ if it takes $(a+b)$ consecutive positive steps from $(n+1)$. Hence for all $n\geq 1$, let $\{\mathcal{F}_n:n\geq 0\}$ be natural filtration associated with $\{X_n: n\geq 0\}$,
	\begin{equation}
		\mathbb{P}\left(\tau\leq a+b+n | \mathcal{F}_n\right)\geq p^{a+b}>0
	\end{equation}
	Clearly, $\{\tau\leq a+b+n\}\in \mathcal{F}_n$. Apply problem 4's conclusion, with constant $N:=a+b, \epsilon:=p^{a+b}$, we conclude that $\mathbb{E}\left[\tau\right]<\infty~a.s.$. $\blacksquare$

	(2) Stay in the same filtered space for the rest of the proof, i.e. $(\Omega, \mathcal{F}, \{\mathcal{F}_n:n\geq 0 \}, \mathbb{P})$, where $\mathcal{F}_n:=\sigma(X_0, X_1, ..., X_n)$. Clearly $S_n \in \mathcal{F}_n$. As hint suggests, consider $(\frac{q}{p})^{S_n}$($\in m \mathcal{F}_n$). Noticing that $(\frac{q}{p})^{X_{n+1}}$ is independent wrt $\mathcal{F}_n$, we have
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[\left(\frac{q}{p}\right)^{S_{n+1}}\middle|\mathcal{F}_n\right]&=\mathbb{E}\left[\left(\frac{q}{p}\right)^{S_{n}}\left(\frac{q}{p}^{X_{n+1}}\right)\middle|\mathcal{F}_n\right]\\
			&=\left(\frac{q}{p}\right)^{S_n} \mathbb{E}\left[\left(\frac{q}{p}\right)^{X_{n+1}}\right]\\
			&=\left(\frac{q}{p}\right)^{S_n} \mathbb{E}\left[\frac{q}{p}\cdot \mathbbm{1}_{\{X_{n+1}=1\}}+\frac{p}{q}\cdot \mathbbm{1}_{\{X_{n+1}=-1\}}\right]\\
			&=\left(\frac{q}{p}\right)^{S_n}\cdot \left(p+q\right)=\left(\frac{q}{p}\right)^{S_n}
		\end{split}
	\end{equation}
	Hence, define $Z_n:=\left(\frac{q}{p}\right)^{S_n}$, $\{Z_n: n\geq 0\}$ is a martingale. Consider $|Z_{n+1}-Z_n|$:
	\begin{equation}
		\begin{split}
			|Z_{n+1}-Z_n|&=\left|\left(\frac{q}{p}\right)^{S_n}\left[\left(\frac{q}{p} \right)^{X_{n+1}}-1\right]\right|\\
			&\leq\begin{cases}
				\left(\frac{q}{p}\right)^b \left(\frac{q}{p}+1\right),~~\text{If $q\geq p$}\\
				\left(\frac{p}{q}\right)^a \left(\frac{p}{q}+1\right),~~\text{If $q<p$.}
			\end{cases}\\
			&\leq \max\left\{\left(\frac{q}{p}\right)^b \left(\frac{q}{p}+1\right), \left(\frac{p}{q}\right)^a \left(\frac{p}{q}+1\right)\right\}<\infty
		\end{split}
	\end{equation}
	Also by (1)'s result, $\mathbb{E}\left[\tau\right]<\infty$. Apply (\textbf{Hunt}'s, case-3): $\mathbb{E}\left[Z_{\tau}\right]=\mathbb{E}\left[Z_0\right]=1$. \newline
	Now since $\mathbb{P}\left(S_{\tau}=-a\right)+\mathbb{P}\left(S_{\tau}=b\right)=1$, and 
	\begin{equation}
		1=\mathbb{E}\left[Z_{\tau}\right]=\left(\frac{q}{p}\right)^{-a} \cdot \mathbb{P}\left(S_{\tau}=-a\right)+\left(\frac{q}{p}\right)^{b} \cdot \mathbb{P}\left(S_{\tau}=b\right)
	\end{equation}
	we get
	\begin{equation}
		\mathbb{P}\left(S_{\tau}=-a\right) = \frac{p^b q^a - q^a q^b}{p^ap^b - q^a q^b}~~~q\ne p~~\blacksquare
	\end{equation}

	(3) Consider
	\begin{equation}
		\mathbb{E}\left[S_{n+1}\middle|\mathcal{F}_n\right] = S_n + \mathbb{E}\left[X_{n+1}\right] = S_n + p - q
	\end{equation}
	Substract $(p-q)(n+1)$ from both sides, we get
	\begin{equation}
		\mathbb{E}\left[S_{n+1}-(p-q)(n+1)\middle|\mathcal{F}_n\right] = S_n - (p-q)n
	\end{equation}
	Hence define $\{M_n: n\geq 0\}:=\{S_n-(p-q)n: n\geq 0\}$, $M_n$ is a martingale. \newline
	Moreover, for any fixed $N\geq 0$, $(\tau \wedge N), (\tau \wedge 0)$ are bounded stopping times. Apply (\textbf{Hunt}'s, case-1): $\mathbb{E}\left[M_{\tau \wedge N}\right]=\mathbb{E}\left[M_{\tau \wedge 0}\right]=\mathbb{E}\left[M_0\right]=0$. Therefore for any fixed $N\geq 0$:
	\begin{equation}
		\begin{split}
			(p-q)\mathbb{E}\left[\tau \wedge N\right] &= \mathbb{E}\left[S_{\tau \wedge N}\right]\\
			&= \mathbb{E}\left[S_{\tau}; \tau \leq N\right] + \mathbb{E}\left[S_n; \tau>N \right]\\
		\end{split}
	\end{equation}
	In which the fisrt part $\mathbb{E}\left[S_{\tau}; \tau \leq N\right] = \mathbb{E}\left[S_{\tau}\cdot \mathbbm{1}_{\{\tau\leq N\}}\right]\nearrow$(by \textbf{MON}) $\mathbb{E}\left[S_{\tau}\right]$. \newline
	The second part $\mathbb{E}\left[S_n; \tau>N\right]\leq (a \vee b)\mathbb{P}\left(\tau>N\right)\xrightarrow{N\to \infty}0$. \newline
	So we are allowed to take $N\to \infty$ on both sides, 
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[\tau \right] &= \frac{1}{p-q}\mathbb{E}\left[S_{\tau}\right]\\
			&= \frac{1}{p-q}\left(-a\cdot  \frac{q^a(p^b-q^b)}{p^ap^b - q^a q^b}+b\cdot  \frac{p^b(p^a-q^a)}{p^ap^b - q^a q^b}\right)\\
			&= \frac{bp^b(p^a-q^a)-aq^a(p^b-q^b)}{(p-q)(p^ap^b - q^a q^b)}~~\blacksquare
		\end{split}
	\end{equation}

	(4) For any fixed $r\in \mathbb{R}$, consider $e^{rS_n}$, clearly $e^{rS_n}\in m \mathcal{F}_n$.
	\begin{equation}
		\mathbb{E}\left[e^{rS_{n+1}}\middle|\mathcal{F}_n\right]=e^{rS_n} \mathbb{E}\left[e^{rX_{n+1}}\right]=e^{rS_n}\frac{e^r+e^{-r}}{2}
	\end{equation}
	Divide both sides by $\cosh^{n+1} r$,
	\begin{equation}
		\mathbb{E}\left[e^{rS_{n+1}}\sech^{n+1}r\middle|\mathcal{F}_n\right]=e^{rS_n}\sech^n r
	\end{equation}
	Hence $\{e^{rS_n}\sech^n r:n\geq 0\}$ is a martingale. Similar as (3), for any fixed $N\geq 0$, $\tau \wedge n, \tau \wedge 0$ are bounded stopping times. Apply (\textbf{Hunt}'s, case-1), we have
	\begin{equation}
		\mathbb{E}\left[e^{rS_{\tau \wedge N}}\sech^{\tau \wedge N} r\right]=\mathbb{E}\left[e^{rS_{0}}\sech^{0} r\right]=1
	\end{equation}
	In LHS, for all $N>0, r\in \mathbb{R}, r<\infty$, note that $\sech r\leq 1$. \newline
	We have followings:
	\begin{itemize}
		\item[$\cdot$] $e^{rS_{\tau \wedge N}}\sech^{\tau \wedge N} r \xrightarrow{a.s.} e^{rS_{\tau}}\sech^{\tau} r$.
		\item[$\cdot$] $e^{rS_{\tau \wedge N}}\sech^{\tau \wedge N} r\leq e^{rS_{\tau}}\cdot 1$. Moreover, by symmetry: $\mathbb{P}\left(S_{\tau}=\pm a\right)=\frac{1}{2}$, hence we can compute $\mathbb{E}\left[e^{rS_{\tau}}\right]=\frac{e^{ra}+e^{-ra}}{2}=\cosh ra < \infty$, i.e. $e^{rS_{\tau}} \in \mathcal{L}^1$.
	\end{itemize}
	Apply (\textbf{DOM}): $e^{rS_{\tau \wedge N}}\sech^{\tau \wedge N} r \xrightarrow{\mathcal{L}^1} e^{rS_{\tau}}\sech^{\tau} r$. Hence
	\begin{equation}
		\begin{split}
			1&=\mathbb{E}\left[e^{rS_{\tau}}\sech^{\tau} r\right]\\
			&=\mathbb{E}\left[e^{rS_{\tau}}\sech^{\tau} r; S_{\tau}=a\right]+\mathbb{E}\left[e^{rS_{\tau}}\sech^{\tau} r; S_{\tau}=-a\right]\\
			&=e^{ra}\mathbb{E}\left[\sech^{\tau} r; S_{\tau}=a\right]+e^{-ra}\mathbb{E}\left[\sech^{\tau} r; S_{\tau}=-a\right]\\
			&=\frac{e^{ra}}{2}\mathbb{E}\left[\sech^{\tau} r\right]+\frac{e^{-ra}}{2}\mathbb{E}\left[\sech^{\tau} r\right]~~\text{(Since distribution of $S_{\tau}$ is symmetric)}\\
			&= \cosh (ra) \cdot \mathbb{E}\left[\sech^{\tau}r\right]
		\end{split}
	\end{equation}
	Change variable, denote $x:=\sech r$, $r=\arcsech x=\log\left(\frac{1}{x}+\sqrt{\frac{1}{x^2}-1}\right)$,
	\begin{equation}
		\sech(a \arcsech x) = \mathbb{E}\left[x^{\tau}\right]
	\end{equation}
	Hence for $t\leq 0$, $\mathbb{E}\left[e^{t\tau}\right]=\sech(a\cdot \arcsech(e^t))$. $\blacksquare$
\end{itemize}

%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 6.} Build a sequence $\{X_n: n\geq 1\}$, $X_n\in \mathcal{L}^1$, such that 
	\begin{equation}
		\mathbb{E}\left[X_{n+1}\middle|X_n\right]=X_n ~~\text{For all $n\geq 1$, but}~~\mathbb{E}\left[X_{n+1}\middle|\mathcal{F}_n\right]\ne X_n~~\text{for $n\geq 2$.}
	\end{equation}
	Where $\mathcal{F}_n:=\sigma(X_j: 1\leq j \leq n)$
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[$Proof$.] $Z_1, Z_2 \sim \mathcal{N}(0,1)$, are two independent standard gaussians, we have $\mathbb{E}\left[Z_1\right]=0, \mathbb{E}\left[Z_1^2\right]=1$. Now consider $a(Z_1 + Z_2)$ and $b(Z_1-Z_2)$ for any constant numbers $a,b<\infty$; these two are both gaussians, and has joint bavariate Gaussian distribution, moreover
	\begin{equation}
		\begin{split}
			\mathrm{Cov}\left[a(Z_1+Z_2), b(Z_1-Z_2)\right]&=\mathbb{E}\left[ab(Z_1^2-Z_2^2)\right]-ab\mathbb{E}\left[Z_1+Z_2\right]\mathbb{E}\left[Z_1-Z_2\right]\\
			&=ab(1-1)-0=0
		\end{split}
	\end{equation}
	Hence $a(Z_1+Z_2)$, $b(Z_1-Z_2)$ are independent for any $a,b<\infty$.\newline
	We construct $\{X_n: n\geq 1\}$ as follows
	\begin{equation}X_n=
		\begin{cases}
		2^{\frac{n+1}{2}}\cdot Z_1,~~\text{if n is odd}\\
		2^{\frac{n}{2}}\cdot (Z_1 - Z_2),~~\text{if n is even}
		\end{cases}
	\end{equation}
	That is, $\{X_n: n\geq 1\}:=\{2Z_1, 2(Z_1-Z_2), 4Z_1, 4(Z_1-Z_2), 8Z_1, 8(Z_1-Z_2),...\}$
	$\mathcal{F}_2=\sigma(2Z_1, 2(Z_1-Z_2))$, then for any $n\geq 2$, $X_n \in m \mathcal{F}_2 \subseteq m \mathcal{F}_3 \subseteq ... \subseteq m\mathcal{F}_{n-1}$. (actually equal signs). Now check required properties of $X$, for $n\geq 2$:
	\begin{equation}
		\mathbb{E}\left[X_{n+1}\middle|\mathcal{F}_n\right]=X_{n+1}\ne X_n
	\end{equation}
	For $n\geq 1$, $n+1$ odd:
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[X_{n+1}\middle|X_n\right]&=\mathbb{E}\left[2^{\frac{n+2}{2}}Z_1\middle|2^{\frac{n}{2}}(Z_1-Z_2)\right]\\
			&= \mathbb{E}\left[2^{\frac{n}{2}}(Z_1+Z_2)+2^{\frac{n}{2}}(Z_1-Z_2)\middle|2^{\frac{n}{2}}(Z_1-Z_2)\right]\\
			&= \mathbb{E}\left[2^{\frac{n}{2}}(Z_1+Z_2)\right]+\mathbb{E}\left[2^{\frac{n}{2}}(Z_1-Z_2)\middle|2^{\frac{n}{2}}(Z_1-Z_2)\right]\\
			&=0+2^{\frac{n}{2}}(Z_1-Z_2)=X_n
		\end{split}
	\end{equation}
	For $n\geq 1$, $n+1$ even:
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[X_{n+1}\middle|X_n\right]&=\mathbb{E}\left[2^{\frac{n+1}{2}}(Z_1-Z_2)\middle|2^{\frac{n+1}{2}}Z_1\right]\\
			&= \mathbb{E}\left[2^{\frac{n}{2}}Z_1\middle|2^{\frac{n}{2}}Z_1\right]-\mathbb{E}\left[2^{\frac{n}{2}}Z_2\right]\\
			&=2^{\frac{n}{2}}Z_1-0 = X_n~~\blacksquare
		\end{split}
	\end{equation}
\end{itemize}


%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 7.} Given filtered space $(\Omega, \mathcal{F}, \{\mathcal{F}_n:n\geq 0\}, \mathbb{P})$, let $\{Y_n: n\geq 1\}$ adapted, such that $Y_n\in \mathcal{L}^2$, $\mathbb{E}\left[Y_n\middle|\mathcal{F}_{n-1}\right]=0$. Further assume that $\sum_{n\geq 1}\mathbb{E}\left[Y^2_n\right]/n^2 < \infty$. Define $X_0:=0$, $X_n:=\sum_{j=1}^n Y_j/j$, $S_n$ be partial sum of $Y_n$.
	\begin{itemize}
		\item[$1.$] Show that $\{X_n: n\geq 0\}$ is a martingale wrt $\{\mathcal{F}_n: n\geq 0\}$.
		\item[$2.$] Based on (1) show that SLLN holds for sequence $Y_n$, i.e.
		\begin{equation}
			\frac{S_n}{n} \xrightarrow{a.s.} 0
		\end{equation}
	\end{itemize}
	}
}
%------------------------------------------------------------------------

\begin{itemize}
	\item[$Proof.$] (1) Since $\{\mathcal{F}_n: n\geq 0\}$ is filtration, $\{Y_n: n\geq 1\}$ is adapted, we have $Y_n\in m \mathcal{F}_n$. Moreover, for all $1\leq j\leq n$, $\mathcal{F}_j \subseteq \mathcal{F}_n$, hence $Y_j \in m \mathcal{F}_j \subseteq m \mathcal{F}_n$, $S_n \in m \mathcal{F}_n$, $X_n \in m \mathcal{F}_n$ are also adapted.
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[X_{n+1}\middle|\mathcal{F}_n\right]&=\mathbb{E}\left[\sum_{j=1}^{n+1}\frac{Y_j}{j}\middle|\mathcal{F}_n\right]\\
			&=\mathbb{E}\left[\sum_{j=1}^n \frac{Y_j}{j}\middle|\mathcal{F}_n\right]+\mathbb{E}\left[\frac{Y_{n+1}}{n+1}\middle|\mathcal{F}_n\right]=X_n~~\blacksquare
		\end{split}
	\end{equation}
	(2) We first calculte second moment of $X_n$,
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[X_n^2\right]&=\mathbb{E}\left[\left(\sum_{j=1}^n\frac{Y_j}{j}\right)^2\right]=\mathbb{E}\left[\left(\sum_{j=1}^n\frac{Y_j^2}{j^2}+\sum_{i=1}^n\sum_{k=1,k\ne i}^n \frac{Y_iY_k}{ik}\right)\right]\\
			&=\sum_{j=1}^n \frac{\mathbb{E}\left[Y_j^2\right]}{j^2}+\sum_{i=1}^n\sum_{k=1,k\ne i}^n \frac{\mathbb{E}\left[Y_iY_k\right]}{ik}\\
			&=\sum_{j=1}^n \frac{\mathbb{E}\left[Y_j^2\right]}{j^2}+\sum_{i=1}^n\left(\sum_{k=1,k\leq i-1}^n\frac{\mathbb{E}\left[\mathbb{E}\left[Y_iY_k\middle|\mathcal{F}_{i-1}\right]\right]}{ik}+\sum_{k=1,i \leq k-1}^n\frac{\mathbb{E}\left[\mathbb{E}\left[Y_iY_k\middle|\mathcal{F}_{k-1}\right]\right]}{ik}\right)
		\end{split}
	\end{equation}
	Now look at $\mathbb{E}\left[\mathbb{E}\left[Y_iY_k\middle|\mathcal{F}_{i-1}\right]\right]$ in the first part (where $k\leq i-1$) in the second layer of the cross terms' summation. Clearly, $Y_k \in m \mathcal{F}_k \subseteq m \mathcal{F}_{i-1}$, so it can be taken out from inner conditional expectation, i.e.
	\begin{equation}
		\mathbb{E}\left[\mathbb{E}\left[Y_iY_k\middle|\mathcal{F}_{i-1}\right]\right]=\mathbb{E}\left[Y_k\mathbb{E}\left[Y_i\middle|\mathcal{F}_{i-1}\right]\right]=\mathbb{E}\left[Y_k \cdot 0\right]=0
	\end{equation}
	Same story for the second part (where $i\leq k-1$),
	\begin{equation}
		\mathbb{E}\left[\mathbb{E}\left[Y_iY_k\middle|\mathcal{F}_{k-1}\right]\right]=\mathbb{E}\left[Y_i\mathbb{E}\left[Y_k\middle|\mathcal{F}_{k-1}\right]\right]=\mathbb{E}\left[Y_i \cdot 0\right]=0
	\end{equation}
	Hence the cross terms are actually zero. That is $\mathbb{E}\left[X_n^2\right]=\sum_{j=1}^n \frac{\mathbb{E}\left[Y_j^2\right]}{j^2}<\infty$. We conclude that $\{X_n: n\geq 0\}$ is bounded by $\mathcal{L}^2$. \newline
	By (\textbf{MCT3}), there exists $X\in \mathcal{L}^2$, such that $X_n \xrightarrow{a.s.} X$; $X_n \xrightarrow{\mathcal{L}^2} X$. Since $X\in \mathcal{L}^2$, $|X|$ must be finite, so is $X$. That is to say:
	\begin{equation}
		X_n := \sum_{j=1}^n\frac{Y_j}{j} \xrightarrow{a.s.} X < \infty
	\end{equation}
	By (\textbf{Kronecker})'s lemma,
	\begin{equation}
		\frac{1}{n}\sum_{j=1}^n Y_n = \frac{S_n}{n} \xrightarrow{a.s.} 0~~\blacksquare
	\end{equation}

\end{itemize}

%------------------------------------------------------------------------
\fbox{
	\parbox{\boxwidth}{
	\textbf{Problem 8.} A branching process $\{Z_n: n\geq 0\}$ is constructed in following way. I.e., for a family $\{X_k^{(n)}: n,k\geq 1\}$ of i.i.d $\mathbb{Z}^+$-valued RVs, define $Z_0:=1$, then define recursively for $n\geq 0$,
	\begin{equation}
		Z_{n+1} := \sum_{k=1}^{Z_n} X_k^{(n+1)}
	\end{equation}
	For any one of $X_k^{(n)}$, denoted by $X$, $\mu:=\mathbb{E}\left[X\right]<\infty$, $0<\sigma^2:=\mathrm{Var}\left[X\right]<\infty$. Show that $M_n:=Z_n/\mu^n$ is a martingale wrt filtration $\mathcal{F}_n:=\sigma(Z_0, Z_1, ..., Z_n)$. Further show that 
	\begin{equation}
		\mathbb{E}\left[Z^2_{n+1}|\mathcal{F}_n\right]=\mu^2 Z_n^2 +\sigma^2 Z_n
	\end{equation}
	And deduce that $\{M_n\}$ is bounded in $\mathcal{L}^2$ iff $\mu>1$. Show that when $\mu>1$, 
	\begin{equation}
		\mathrm{Var}\left[M_{\infty}\right]=\frac{\sigma^2}{\mu(\mu-1)}
	\end{equation}
	}
}
%------------------------------------------------------------------------
\begin{itemize}
	\item[$Proof.$] For any $n,k\geq1$, $X_k^{(n+1)}$ is independent to $\mathcal{F}_n=\sigma(Z_0, Z_1, ...,Z_n)$. Moreover $\{X_k^{(n+1)}: k\geq 1\}$ are i.i.d for all $n$. So $\mathbb{E}\left[X_k^{(n+1)}|\mathcal{F}_n\right]=\mathbb{E}\left[X_k^{(n+1)}\right]=\mu$.\newline
	Now Consider
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[Z_{n+1}|\mathcal{F}_n\right]&=\mathbb{E}\left[\sum_{k=1}^{Z_n}X_{k}^{(n+1)}\middle|\mathcal{F}_n\right]=\mathbb{E}\left[\sum_{k\geq 1}X_k^{(n+1)}\mathbbm{1}_{(Z_n\geq k)}\middle|\mathcal{F}_n\right]\\
			&=\sum_{k\geq 1} \mathbb{E}\left[X_k^{(n+1)}|\mathcal{F}_n\right]\cdot \mathbb{E}\left[\mathbbm{1}_{(Z_n\geq k)}|\mathcal{F}_n\right]\\
			&=\mu \sum_{k\geq 1}\mathbb{E}\left[\mathbbm{1}_{(Z_n\geq k)}|\mathcal{F}_n\right]~~\text{(Next: since $\mathbbm{1}_{(Z_n\geq k)}\in m \mathcal{F}_n$)}\\
			&= \mu \sum_{k\geq 1} \mathbbm{1}_{(Z_n \geq k)}=\mu \sum_{k=1}^{Z_n}1 = \mu Z_n
		\end{split}
	\end{equation}
	Hence, multiply both sides by $\mu^{-(n+1)}$, we get
	\begin{equation}
		\mathbb{E}\left[\frac{Z_{n+1}}{\mu^{n+1}}\middle|\mathcal{F}_n\right]=\frac{Z_n}{\mu^n}
	\end{equation}
	i.e. $\{M_n: n\geq 0\}:=\{Z_n\mu^{-n}: n\geq 0\}$ is a martingale.\newline
	Now calculate conditional second moment of $Z_{n+1}$. Note that $\mathrm{Var}\left[X_k^{(n)}\right]=\sigma^2$, hence $\mathbb{E}\left[(X_k^{(n)})^2\right]=\mu^2+\sigma^2$ for any $n,k\geq 1$.
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[Z^2_{n+1}|\mathcal{F}_n\right]&=\mathbb{E}\left[\left(\sum_{k=1}^{Z_n}X_{k}^{(n+1)}\right)^2\middle|\mathcal{F}_n\right]=\mathbb{E}\left[\left(\sum_{k\geq 1}X_{k}^{(n+1)}\mathbbm{1}_{(Z_n\geq k)}\right)^2\middle|\mathcal{F}_n\right]\\
			&= \mathbb{E}\left[\sum_{k\geq 1}(X_k^{(n+1)})^2 \mathbbm{1}_{(Z_n\geq k)}+\sum_{i\geq 1}\sum_{j\geq 1, j\ne i}X_i^{(n+1)}X_j^{(n+1)}\mathbbm{1}_{(Z_n\geq i \vee j)}\middle|\mathcal{F}_n\right]\\
			&=(\mu^2 +\sigma^2)\sum_{k\geq 1}\mathbbm{1}_{(Z_n\geq k)}+\mu^2\sum_{i\geq 1}\sum_{j\geq 1, j\ne i}\mathbbm{1}_{(Z_n\geq i\vee j)}\\
			&= (\mu^2+\sigma^2)\sum_{k=1}^{Z_n}1+\mu^2\sum_{i=1}^{Z_n}\sum_{j=1, j\ne i}^{Z_n}1\\
			&=(\mu^2+\sigma^2)Z_n + \mu^2 (Z^2_n-Z_n)\\
			&=\mu^2Z_n^2 +\sigma^2 Z_n
		\end{split}
	\end{equation}
	Now devide both sides by $\mu^{2n+2}$,
	\begin{equation}
		\mathbb{E}\left[M_{n+1}^2\middle|\mathcal{F}_n\right]:=\mathbb{E}\left[\frac{Z_{n+1}^2}{\mu^{2n+2}}\middle|\mathcal{F}_n\right]=\frac{Z^2_n}{\mu^{2n}}+\frac{\sigma^2 Z_n}{\mu^{2n+2}}=:M^2_n+\frac{\sigma^2Z_n}{\mu^{2n+2}}
	\end{equation}
	Take expectation both sides,
	\begin{equation}
		\mathbb{E}\left[M_{n+1}^2\right]=\mathbb{E}\left[M^2_n\right]+\frac{\sigma^2 \mathbb{E}\left[Z_n\right]}{\mu^{2(n+1)}}
	\end{equation}
	Expectation of $Z_n$ is given by $M_n$:
	\begin{equation}
		\mathbb{E}\left[\frac{Z_n}{\mu^n}\right]=\mathbb{E}\left[\frac{Z_0}{\mu^0}\right]~~\text{i.e.}~~\mathbb{E}\left[Z_n\right]=\mu^n
	\end{equation}
	Hence
	\begin{equation}
		\begin{split}
			\mathbb{E}\left[M_n^2\right]&=\mathbb{E}\left[M_0^2\right]+\sum_{k=1}^n \frac{\sigma^2 \mathbb{E}\left[Z_{k-1}\right]}{\mu^{2k}}\\&=1+\sum_{k=1}^n \frac{\sigma^2}{\mu^{k+1}}=1+\frac{\sigma^2}{\mu(\mu-1)}\left(1-\frac{1}{\mu^{n+1}}\right)
		\end{split}
	\end{equation}
	Clearly $\mathbb{E}\left[M_n^2\right]$ converges if and only if $\mu>1$. \newline
	When $\mu\geq 1$, $\mathbb{E}\left[M_n^2\right]<1+\frac{\sigma^2}{\mu(\mu-1)}<\infty$, i.e. $M_n$ is bounded by $\mathcal{L}^2$. By (\textbf{MCT3}), $\exists M\in \mathcal{L}^2$, $M_n \xrightarrow{a.s.} M$ and $M_n \xrightarrow{\mathcal{L}^2} M$, therefore
	\begin{equation}
		\mathbb{E}\left[M^2\right]=\lim\limits_{n\rightarrow\infty}\mathbb{E}\left[M_n^2\right]=1+\frac{\sigma^2}{\mu(\mu-1)}
	\end{equation}
	Note that $\mathbb{E}\left[M\right]=\mathbb{E}\left[M_0\right]=1$, So $\mathrm{Var}\left[M\right]=\frac{\sigma^2}{\mu(\mu-1)}$. $\blacksquare$
\end{itemize}


\end{itemize}
\end{document}